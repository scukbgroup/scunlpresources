# 目录

----

## 2019年



---

## 2018年

### 研究性工作

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding--ariv]() 

  **Abstract：** 本文作者推出了一套新的语言表达模型BERT，全称为Bidirectional Encoder Representations from Transformers。与近年来提出的语言模型不一样的地方在于，BERT不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个>上下文的语境信息。实验结果证明，使用预训练过的BERT模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以取得很不错的结果。具体有多不错，那就是刷爆了NLP领域的11大任务，甚至有几项任务的准确率已经超过了人类。

  **keywords:**  

> > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > 
> > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > >
> > > > > > > > > > > > > > > > > > > > >