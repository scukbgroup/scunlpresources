- BERT Pre-training of Deep Bidirectional Transformers for Language Understanding

| 项目     | 内容                                                         |
| :------- | ------------------------------------------------------------ |
| 时间     | 2018年                                                       |
| 作者单位 | Google                                                       |
| 作者信息 | Jacob<br/>Devlin,Ming-Wei Chang, Kenton Lee, Kristina Toutanova |
| 源码链接 |                                                              |
| 原文链接 | <https://arxiv.org/abs/1810.04805>                           |
| 其他解读 | https://blog.csdn.net/jilrvrtrc/article/details/83829470     |
| 关键词   | 预训练，迁移学习                                             |

  

  # 摘要

本文作者推出了一套新的语言表达模型BERT，全称为Bidirectional Encoder Representations from Transformers。与近年来提出的语言模型不一样的地方在于，BERT不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个>上下文的语境信息。实验结果证明，使用预训练过的BERT模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以取得很不错的结果。具体有多不错，那就是刷爆了NLP领域的11大任务，甚至有几项任务的准确率已经超过了人类。

# 简介

# 相关工作

# 模型介绍

# 实验及结果

# 总结





​      

