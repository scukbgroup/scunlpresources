# 目录

----

## 2019年



---

## 2018年

### 研究性工作

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding--ariv](papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html)

  **Abstract：** 本文作者推出了一套新的语言表达模型BERT，全称为Bidirectional Encoder Representations from Transformers。与近年来提出的语言模型不一样的地方在于，BERT不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的BERT模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，**并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改**。Bert是一种概念上简单，但根据经验推断其具有强大的性能。BERT在 11项自然语言处理任务中都取得了目前最好的性能，具体包括将GLUE的基准值提升到了80.4%(7.6%的绝对提升)，在MultNLI中准确率有了86.7的提升（5.6%的绝对提升），在SQuAD v1.1的问答任务中其F1值达到了93.2（1.5%）的绝对提升，比人类的表现都搞了2.0的提升。       

  **keywords:**  预训练，迁移学习

---



### 研究性工作

- [Improving Language Understanding by Generative Pre-Training](papers/Improving Language Understanding by Generative Pre-Training.html)

  **Abstract：** 本文作者推出了一套新的语言表达模型BERT，全称为Bidirectional Encoder Representations from Transformers。与近年来提出的语言模型不一样的地方在于，BERT不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的BERT模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，**并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改**。Bert是一种概念上简单，但根据经验推断其具有强大的性能。BERT在 11项自然语言处理任务中都取得了目前最好的性能，具体包括将GLUE的基准值提升到了80.4%(7.6%的绝对提升)，在MultNLI中准确率有了86.7的提升（5.6%的绝对提升），在SQuAD v1.1的问答任务中其F1值达到了93.2（1.5%）的绝对提升，比人类的表现都搞了2.0的提升。       

  **keywords:**  预训练，迁移学习

---



### 研究性工作

- [Deep contextualized word representations](papers/Deep contextualized word representations.html)

  **Abstract：** 本文作者推出了一套新的语言表达模型BERT，全称为Bidirectional Encoder Representations from Transformers。与近年来提出的语言模型不一样的地方在于，BERT不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的BERT模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，**并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改**。Bert是一种概念上简单，但根据经验推断其具有强大的性能。BERT在 11项自然语言处理任务中都取得了目前最好的性能，具体包括将GLUE的基准值提升到了80.4%(7.6%的绝对提升)，在MultNLI中准确率有了86.7的提升（5.6%的绝对提升），在SQuAD v1.1的问答任务中其F1值达到了93.2（1.5%）的绝对提升，比人类的表现都搞了2.0的提升。       

  **keywords:**  预训练，迁移学习

### 研究性工作

- [JUMPER: Learning When to Make Classification Decisions in Reading-arxiv](papers/JUMPER Learning When to Make Classification Decisions in Reading.html)

  **Abstract：** 在早期的方法中，文本分类是一般都是基于特征提取并结合机器学习方法来完成；近年来，随着深度神经网络的出现，其作为一个更强性能的机器学习模型能给是的文本处理任务能够接受原始输入并直接输出结果的端到端的过程。然而已经存在的端到端的模型缺乏对相关预测过程的解释。在这篇文章中，我们提出了一个新的框架Jumper，该框架是受到原始阅读过程的启发，该框架将文本分类任务当做一个序列决策过程。具体来将，JUMPER是一个神经网络系统，其通过扫描文本输入的序列并在其期望看到结果的时候做出决策。文本分类结果以及何时进行决策都是一个序列决策过程，其是有一个通过强化学习训练的策略网络来进行控制的。  在实验结果中表明，JUMPER有如下属性：（1）该框架在当在收集到足够的证据时就会进行决策，因此可以减少30%-40%的文本阅读时间，此外其能发现对预测结果有用的关键信息。（2）该框架在许多benchmark和工业数据集中都能取得SOTA的结果

  **keywords:**  文本分类，强化决策

> > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > 
> > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > >
> > > > > > > > > > > > > > > > > > > > >