{"index":{"version":"0.5.12","fields":[{"name":"title","boost":10},{"name":"keywords","boost":15},{"name":"body","boost":1}],"ref":"url","documentStore":{"store":{"./":["introduct","分析解读最新的自然语言处理方面的论文，供大家一起进行交流和学习。","将相关的知识进行整理后分享，方便后面的人学习","将自然语言处理方面的工具进行整理汇总","工具整理","收集整理相关成员上传的资料及相应的数据集，方便有需要的同学进行下载。","收集整理自然语言处理领域的会议及期刊，方便大家进行相关的查阅。","数据集","知识分享","自然语言处理会议及期刊","论文解读","该博客主要收集整理与自然语言处理相关工具，论文以及代码等资源信息，对目前自然语言处理最前沿的技术、论文及相关相关代码进行整理后进行分享，主要关注点包括自然语言处理基础方法，知识图谱构建，智能问答等几个方面的知识。通过不断地积累让大家在这条路上越走越远。"],"metting/summit metting.html":["aaai,全称是associ","ac(lth","acl","acl的全称是th","acl（tacl），此外还有一些期刊与自然语言处理相关。如tslp(（acm","acm","acm）主办，包括如下几个会议。","advanc","ai","ai,artifici","aistat","american","approach","artifici","asia","asian","associ","association)组织","a类","base","b类","ccf","chapter","cikm,该会议名称缩写和上一个相同，但是其全称为intern","cikmm,其全称是confer","cole","coling(intern","committe","comput","confer","conference，也是有关信息检索及数据挖掘。","conll","conll(confer","c类","data","data&corpu","emnlp","emnlp(confer","emnlp是有acl下面的比较著名的兴趣小组（speci","empir","evaluation,其由欧洲语言资源组织进行elra(european","evaluation,该会议由acl的特殊兴趣小组siglex进行组织，每年都会举办，国内也有很多研究机构及公司参与，如哈工大科大讯飞等,其最新的链接网址如下：http://alt.qcri.org/semeval2019/index.php?id=task","group","groups,sigs）之一的sigdata(speci","hearst正式宣布成立国际计算语言学学会亚太地区分会（aacl，th","http://coling2018.org","http://emnlp2018.org","http://naacl.org","http://www.conll.org/","https://blog.csdn.net/lyb3b3b/article/details/83548964","https://www.aclweb.org/port","https://www.cnblogs.com/niuxichuan/p/7602012.html","icml,全称是","ijcai,全称是intern","inform","intellig","intelligence，主要关注人工智能领域的最新研究进展，其中也有大量的人工智能相关的知识。","intelligence，也是关注人工智能领域的另外一个重要学术会议。","interest","intern","jair,journ","jmlr,","joint","journal","knowledg","languag","learn","learnin)","learning)举办的。其也是每年举办一次，由于naacl是acl在北美的分会，因此当acl在北美举办的时候，naacl就会停办一年。","linguist","linguistics)","linguistics)组织的。该会议每两年举办一次。","linguistics,其由1965年创办，是由老牌的nlp学术会议组织iccl(th","linguistics,翻译过来是计算机语言协会，自然语言处理与计算语言学领域（以下简称nlp/cl）最权威的国际专业学会，acl成立于1962年，是自然语言处理(nlp)领域影响力最大、最具活力的顶级国际学术组织，每年举办一次。这个学会主办了","linguistics,该会议是acl在北美的分会，也是有acl主办。其是有acl下面的兴趣小组signal(speci","linguistics和transact","linguistics）。此次成立acl亚太分会，将进一步促进亚太地区nlp相关技术和研究的发展。据悉，首届aacl会议预计在2020年举行，此后将每两年举行一次。","lrec","lrec全称是intern","machin","machinery,","management，其关注信息管理","method","mining,由名称可知，该会议关注搜索和数据挖掘。","naacl","naacl(th","naacl的全称是th","natur","nc,neurocompt","neural","nip","nlp/cl","nlp相关的其他国际会议","north","pacif","process","processing)","processing)),其他相关期刊及投稿链接如下：","processing)举办的。","processing,nlp）领域的一些著名会议及期刊，将按照ccf对会议的分级标准对相关会议进行整理，介绍每个会议的关注主题，召开周期、会议网站及相关的信息，方便后面查找最新的相关论文，从而对该领域进行比较深入的研究。每个会议及期刊都有相关的入口链接，方便进行查看。","processing.","processing）)以及talip((acm","research","resourc","retrieval,其主要关注信息检索。","search","semant","semev","semeval,其全称是intern","sigir,其全称是speci","speech","statist","system","transact","uai","uncertainti","web","wide","workshop","world","wsdm,该会议全称为web","人工智能领域与自然语言处理相关的会议","以下是根据ccf2015年的对自然语言处理领域的会议的分级标准来进行罗列和整理","会议名称","会议网站","全称","全称是confer","全称是intern","出版社","分类等级","参考文献","召开周期","国际会议","在国际上acl，coling，enmlp和naacl是默认的四大自然语言处理顶级学术会议，其中acl，emnlp和naacl都是由acl及相应的子组织举办的。","在自然语言处理中，一般来将，大家更关注学术会议，其主要原因是发表周期短，通过会议也可以进行深入的交流。但自然语言处理领域也有自己的学术期刊，其旗舰期刊有两个，分别是comput","在自然语言处理中，机器学习也是其主流的方法，在机器学习领域相关的学术会议包括icml,nips,uai及aistats,下面将简单介绍。","处理和人工智能是密切相关的，其是人工智能研究的重要内容，人工智能研究的两大国际顶会是aaai和ijcai","期刊","本文介绍自然语言处理（natur","机器学习领域中与自然语言处理相关的顶级会议","每两年一次","每年一次","自然语言","除了上述被ccf收录的会议外，在自然语言处理领域还有其他许多重要会议，分别是semeval,lrec等。","除了直接与自然语言处理相关外，还有其他许多与自然语言处理相关的学术会议，包括信息检索，数据挖掘及人工智能领域，这些都是属于自然语言处理的应用领域。其中信息检索和数据挖掘与自然语言处理是密切相关的，主要由美国计算机学会（associ","领域最权威的国际会议，即acl年会。1982年和1999年，acl分别成立了欧洲分会（eacl）和北美分会（naacl）两个区域性分会。近年来，亚太地区在自然语言处理方面的研究进步显著，2018年7月15日，第56届acl年会在澳大利亚墨尔本举行。开幕仪式上，acl主席marti"],"tools/readme.html":["tool"],"database/readme.html":["classif","databas","decis","https://v.youku.com/v_show/id_xnda4mdyznzgwoa==.html?spm=a2h3j.8428770.3416059.1","jumper:learn","make","read","下载地址：链接：https://pan.baidu.com/s/1nmh33yk80sznki8vhmeeea","分享者：刘露平","密码:trsadmin","时间：2019年3月27日","时间：2019年3月28日","概述：该ppt是做汇报时候的ppt，讲了知识图谱的基本知识及实体和关系抽取相关论文，有需要可以下载。","知识图谱概述ppt","该材料是一个视频，讲述的是基于强化学习的文本分类模型，其用于阅读理解中，感谢图像所王大东同学提供的资料。有兴趣的可以观看。","该页面收集整理日常工作中相关的数据集和资料，大家可以通过将相关资料放到网盘或者其他github仓库中，然后这里放上相应的链接。","（永久有效）提取码：mdan"],"paper/readme.html":["2018","ariv","bert:","bidirect","contextu","deep","gener","improv","languag","paper","pre","represent","train","transform","understand","word","该部分主要是分享目前自然语言及知识图谱处理方法的基础知识，目前包含如下三个方面：自然语言处理基础技术，知识图谱构建，智能问答,论文整理将按照年份和板块来进行整理。"],"paper/nlp/readme.html":["11项自然语言处理任务中都取得了目前最好的性能，具体包括将glue的基准值提升到了80.4%(7.6%的绝对提升)，在multnli中准确率有了86.7的提升（5.6%的绝对提升），在squad","2018年","2019年","abstract：","ariv","bert:","bidirect","contextu","deep","encod","gener","improv","keywords:","languag","nlp","pre","represent","train","transform","transformers。与近年来提出的语言模型不一样的地方在于，bert不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的bert模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改。bert是一种概念上简单，但根据经验推断其具有强大的性能。bert在","understand","v1.1的问答任务中其f1值达到了93.2（1.5%）的绝对提升，比人类的表现都搞了2.0的提升。","word","本文作者推出了一套新的语言表达模型bert，全称为bidirect","目录","研究性工作","预训练，迁移学习"],"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":["##","*a*:表示注意力header的数量。",",winograd",".","1.简介","11项自然语言处理任务中都取得了目前最好的性能，具体包括将glue的基准值提升到了80.4%(7.6%的绝对提升)，在multnli中准确率达到86.7的（5.6%的绝对提升），在squad","2","2.1","2.2","2.3","2.相关工作","2003的命名实体识别任务中进行了测试。该数据集包含200k的训练数据集，其中实体类别为person（人物）,organization（组织）,location（地点），miscellaneous（其他项）和other(非实体词)。","2018年","3","3.1","3.2","3.3","3.3.1","3.3.2","3.4","3.5","3.6","4,b1参数为0.9，b2参数为0.999，l2范数的权重为0.01.","4.1","4.1.1","4.2","4.4","4.实验及结果","43e","5","5,2e","5,3e","5,4e","5,batch_siz","5,总共迭代三个epoch.其对比的baseline为esim+glove和esim+elmo.实验结果如下：","5,而bert的学习率则是根据特定任务需求设置来进行微调的。","5等。实验结果如下图所示","a,sent","abstract：本文作者推出了一套新的语言表达模型bert，全称为bidirect","al.,2017)从另外一个重要角度来对词嵌进行处理。在该方法中，作者提出了从语言模型中提取出与上下文敏感特征的方法。通过将基于上下文敏感的词嵌和特定的任务结构相结合后，elmo在很多自然语言处理任务中都取得了soat(st","art)的实验效果，包括基于squad的问题，语义分析以及命名识别识别等众多的任务。","b","base版本的参数有110m个，而bert","batch","bert","bert是一个多层的基于双向转移编码器的模型，其根据tensor2tensor的方式来实践。bert使用的“transform","bert有两个版本：bertbase和bertlarge,其中bertbase的参数如下：l=12,h=768,a=12,而bertlarge的数据l=24,h=1024,a=16.因此bert","bert模型介绍","bert的训练语料是在一个bookcorpus(800m)和维基百科的文本语料中（2,500m个单词）。","bert的输入表示","bert的预训练过程和目前已有的方法类似，其再训练语料我们使用了两部分的语料：bookcorpus(800m","bidirect","blocks）","b）中，一个句子b有50%是句子a的下一句，而50%的可能是随机另外一个句子。","chang,","cola","deep","embedding:","embedding:使用了支持长度为512个token的词嵌表示方法。","embedding。在训练中，针对第i个单词，通过将其bert输出的词嵌表示和一个概率转移矩阵t相乘后利用softmax输出得到该但是是否是答案开始的概率。针对答案的结束，也是使用同样的方式预测某个词是否是答案的结果，而训练的目标是最大化正确开始位置和结束位置的释然概率。该过程的示意图如图所示：","embedding方法来实现词嵌表示。","embedding，而答案使用b","encod","encoder”编码器近来得到广泛的应用，因此本文中没有进行详细的介绍。","encoder模型，bert的","epochs:3,4","et","glue数据集","glue数据集是一个通用的自然语言理解评估数据集，用于评估对自然语言的理解任务。glue数据集包含多个子数据集。","glue数据集的实验结果","googl","gpt(gener","gpt,bilstm+elmo+attn等模型都有很大的提升，其中bertlarge和bertbase比起来有更多的提升。","gpt2018基于迁移学习的方法在许多句子级别的任务中取得了sota的实验结果。","gpt中则是使用的一种从左到右的编码模型，引起bert可以被看做迁移学习编码器，而openai","gpt使用的句子分隔符为[sep]，分类器的分隔符为[cls],并且这些只在微调过程中使用。而bert则是学习了[sep],[cls]以及句子分割信息（a/b）在预训练过程中。","gpt则是一种迁移学习的解码器。bert和openai","gpt在包含32,000个单词的语料库中每个epoch训练了1m","gpt在所有的微调步骤中都使用了相同的学习率5e","gpt的区别可以如图所示","gpt等结构的差别。","h","https://arxiv.org/abs/1810.04805","https://blog.csdn.net/jilrvrtrc/article/details/83829470","jacobdevlin,m","kenton","kristina","l:表示层级的数量（transform","languag","learn","lee,","level级别的任务中都取得了较好的效果。在很多有特定需要的任务中也取得了较好的效果。","levle和token","lm","lm\"(mlm)模型。在该模型中，被隐藏掉的token会被输入到softmax模型中。在所有的预训练实验中，bert采用15%的概率随机隐藏掉输入句子中的token.","lm和next","mask","masking模型。","mnli","mrpc,该数据集微软研究解释语料库由自动提取的句子对组成来自在线新闻来源，带有人工注释判断两个句子的语义是否一致相等的","next","nli是一个小型的自然语言推断数据集，在glue的网页中指出该数据集在构造上存在一定我的问题。因此在模型中排除了openaigpt。","number","optimal)，而在针对一些token级别的任务中，如squad的问答问题中，则这种方法有可能是毁灭性的，因此在这些任务中，纳入两个方面的信息是至关重要的。","posit","pre","predictioni","prediction模型。","qnli","qqp","qqp问题对是一个二进制分类任务，用于确定连个问题的语义是否是相等的。","rate(adam):5","rate)以及训练的epoch次数不同外。在预训练过程中，dropout的概率一直设置为0.1，而新增参数的相关设置是和任务先关的。作者在训练bert的过程中，发现如下参数在多个任务中具有较好的表现。","represent","rte","sentenc","size:16,32","size，学习率(learn","sqqad","squad","sst","st","token","toutanova","tps的默契上进行了训练，每个训练都花费了4天左右完成。","tpus的集群上进行了训练，而bertlarge则是在配置为16cloud","train","transform","transform)引入了最小任务的参数，在下游任务中，其通过简单的微调的方式来调整模型的参数。在以前的工作中，所有的预训练方法都是通过使用同样的目标函数并通过双向语言模型来学习更好的通用语言表示。","transformers(基于双向翻译编码的表示模型)。bert通过提出了两个新的预训练目标来解决以前的提到的单向限制问题：基于“masked”的语言模型（mlm），这种方法在1953年就曾经被提出。这种语言模型通过随机将输入句子中的token进行隐藏，然后预训练的目标就是基于上下文来预测出被覆盖掉的单词。不同于传统的从左到右的预训练语言模型，mlm训练目标使得我们能够充分利用左边和右边信息来训练一个更深的双向翻译模型。除了利用masked语言模型外，我们还在模型中引入了预测下一个句子的训练目标来同时训练句子对级别的表示模型。","transformers。与近年来提出的语言模型不一样的地方在于，bert不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的bert模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改。bert是一种概念上简单，但在实际中却有强大的性能的模型。bert在","tuning）的方法。在基于特征的方法中，elmo(2018)是一种基于特征任务的模型，其包含一个预训练模型以及一些其他的特征。在微调模型中，openai","understand","v1.1","v1.1的问答任务中其f1值达到了93.2（1.5%）的绝对提升，比人类的表现都搞了2.0的提升。","wei","wnli","words)和英语维基百科(2,500m","​","不像传统的从左到右或者从右到左的预训练语言模型任务，在bert中使用了两种原创的无监督学习的预训练任务，分别是mask","不是所有被选择的单词都会被隐藏掉，而是其中80%的可能会隐藏掉该单词，10%的时间将该单词替换成一个随机的单词。而另外还是有10的时间是该单词保持不变。","为32，训练迭代次数为3个epoch.实验结果如下：","为了能够训练一个具有双向深层的表示模型，在bert中采用了一种较为直观的模型，通过随机隐藏掉句子中的token，并让模型能对该句子进行正确的预测。这种模型在bert中被称为\"mask","为了评估bert在序列标注任务中的性能，论文还基于bert在conl","从初始概念上来将，一个更深层次的双向模型比传统的从左到有或者从右到左基于简单讲从左到右或者从右到左模型进行组合的方法有更好的效果。然后，目前传统的语言模型则往往只能通过从左到右或者从右到左的模型进行训练。在双向的环境中，句子中任意一个token都有可能被预测到在一个多层的网络环境中。","从实验结果中可以看到，bert去了的最好的结果，其中bertlarg","从实验结果中，可以看出该模型可以取得较好的实验结果。此外作者还对模型的一些相关参数进行了分析并进行了实验讨论。","从实验结果可以看出，bert也去了的sota的实验结果。","从实验结果可以，bert相比较于openai","以及分类矩阵w相结合并经过softmax分了的方式来进行微调。在每个任务中，使用的学习率为5","作者信息","作者单位","使用了包含30,000token的词库的wordpiec","关键词","其他解读","内容","则分类结果接上一个softmax层后得到相应的各个类别的概率p=softmax(cwt)。这样所有的bert的参数和新增的w参数一起经过微调后得到分类结果的输出。而在针对单词级别的任务中，训练过程和句子级别的有所区别。","区别于传统的auto","单词)。在维基百科语料中，我们目前只提取了文本消息而忽略了列表，表格以及头等信息。在实际训练中，使用文本级别的语料比句子级别的语料可以取得更好的实验结果。为了生存训练输入序列，我们从语料库中选择了两个文本句子并将其合并成一个句子。其中第一个句子使用句子a的embedding，而第二个句子使用句子b的embedding。在生成句子的时候，句子b的embedding有50%的可能是随机选择的，用于训练模型对下一个句子的预测。两个句子合并起来的token序列长度小于512个token。句子首先使用wordpiece的方法生成输入表示并在其中随机挑选15%来隐藏掉，然后在此基础上使用lm","单词可能会被隐藏掉，因此在训练过程中其会花费较多的时间。","原文链接","命名实体识别","和openaigpt的比较","在","在11项自然语言处理任务中都取得了sota的结果。表明双向语言模型在文本处理中的重要性。","在bert中输入被表示为两种形式：","在bert中，bert使用的是一个双向的自注意力机制的编码模型，而openai","在bert的输入中，对于一个输入token序列，该输入句子的序列表示可以由几部分构成：token序列表示，segment和posit","在transformer模型中，由于encoder并不知道哪些单词可能会被替换成随机的单词，因此要求编码器能够学习到一种对所有单词都有表现的能力，此外随机替换只发在在1.5%的概率中，因此不会影响模型的整体理解能力。","在实验中，作者同时发现100k+左右的数据对于模型的参数的变化是极小的。因此微调过程是十分快速的。","在微调训练过程中，大部分模型的参数和原来预训练的过程是一样的。除了batch","在构建训练集时候，在选择句子a的下一个句子时，通过随机选择下一个句子的方式来实现构建训练集。","在每层中我们设置dropout概率为0.1，激活函数选择的是gelu。训练的损失函数lm模型的平均likelihood的和以及预测下一个句子的平均likelihood.","在第三节中，详细介绍bert模型机器实现细节。该节首选叙述bert模块的整体框架及bert的输入表示。接着在3.3节中介绍预训练的任务以及核心的创新。3.4节中介绍预训练的流程，而在3.5节中介绍微调的方法。最后在3.6节中讨论bert和openai","在训练bertbase的过程中，作者在配置有4cloud","在该任务中，训练时使用的额学习率为5e","在该类型的任务中，和传统的句子分类任务有明显的不同，在对bert进行微调的任务中，将问题和答案都表示成两个独立的序列。其中问题使用a","在该项工作中，作者定义了如下变量：","在输入词序列中，序列的第一个词都是以[cls]的方式开头，针对该token的","在这篇文章中我们证明了双向预训练模型的重要性。不同于传统的双向预测模型，bert使用了一种基于masked的语言模型来训练一种更深的语言模型。此外区别于传统的浅层双向语言模型，bert是一种使用深层次的双向语言模型。","在这篇文章中，我们任务目前的技术严重限制了预训练在语言表达方面的能力，特别是针对微调的方法。最大的缺陷是目前的预训练模型都是双向的，其严格限制了模型在进行预训练时的选择能力。例如在openaigp中，作者使用了从左到右的模型，使得句子中每个token只能被以前的词所关注。这种方式对句子级别的任务是次优的(sub","在这篇论文中，我们提出了一种基于微调的方法bert:bidirect","在这节中我们介绍基于预训练的方式来生成语言表示模型的相关方方法，同时简要介绍一下在这个领域目前最流行的方法。","在进行命名实体识别的微调时时，针对每个词，使用其最后一个隐藏状态c","在针对glue数据集的微调训练中，使用了每个句子的第一个输入[cls]的最后一个隐藏层输出c","在预训练中，在每个epoch中，输入句子数量为256个句子，每个epoch迭代1,000,000次，最终没3.3亿个单词上训练了差不多40个epoch.","基于学习的词表示方法在多个领域都有广泛的应用。其中包括非神经元的方法和基于神经网络的方法。基于词嵌表示的预训练方法目前是现代语言处理系统中的一个重要集成部分，其对系统的后续处理提供了有强大的提升。除了词级别的词嵌外，这些方法也被推广到粗粒度的句子级别词嵌以及锻炼词嵌中。在传统的方法中，这些学习到的表示方法往往作为下游任务的特征输入到下游模型中。elmo(pet","基于微调的方法","基于特征的方法","基于监督数据的迁移学习方法","多类型的自然语言推理任务数据集。","如果输入是单个句子，则直接使用句子a的词嵌表示。","实验结果如图所示：","实验部分主要叙述bert在11项自然语言预处理任务中","对于句子级别的分类任务，bert的微调是很直接的。为了获取输入句子的固定维度的表示,bert使用了输入句子的第一个词的最后一个隐藏层的输入c（rh）。在进行分类时，在bert输出的基础上在增加一个分类层，分类层的矩阵表示为w。","常识推理任务（swag","微调过程","总结","我们使用饿了adam优化算法，其中学习率为1","我们展示了预训练模块可以消除在很多任务中需要依赖严重特征工程的任务。bert","摘要","数据集是一个由斯坦福收集的包含100k的问题/答案对数据集。该数据集是通过给定一个问题以及维基百科中包含该答案的段落，该任务从段落中预测出相应的答案。其简单示例如下：","数据集）","无监督学习预训练方法的优势是有大量的无标签的数据可以大量的获取，基于有标签数据的迁移学习目前也被证明在许多文本处理任务中有较好的结果，例如自然语言推理、机器翻译等任务。除了在nlp处理领域外，很多机器视觉领域的研究也说明了预训练并集合迁移学习的重要方，这些方法通过在大量的预训练数据上进行训练后再通过微调的方式可以取得较好的实验结果。","时间","是一个有斯坦福构建的二分类任务的数据集，是从一些电影评论中抽取出来的句子并且包含人工的标注信息。","是一个自然语言推理任务数据集，其中的正样本是一些问题，答案对的句子则是来自于同一个段落的句子，其并不形成问题","是第一个基于微调方法的语言表示模型，并且在sentenc","最后隐藏状态被用在分类任务的聚合表示中。而如果针对的是非分类任务，则该词对应的向量被删除掉。在句子对中，所有句子都被表示到一个序列任务中，我们通过两种不同的方式来区分。首先通过一个特殊的token[esp]来对两个句子进行区分，在第一个句子中，每个句子的token会加上句子a的embedding表示，而在句子b中，每个token都会加上句子b的embedding表示。","最近的经验改进表明，由于转移学习与语言模型已经证明，丰富，无监督的预培训是许多语言理解系统的一个组成部分。特别是，这些结果使得即使是低资源任务也能从非常深的单向架构中受益。我们的主要贡献是将这些发现进一步推广到深层双向体系结构中，使相同的预先培训的模型能够成功地处理广泛的nlp任务。虽然经验结果很强，但在某些情况下，超过了人类的表现，未来的重要工作是研究语言现象。","模块的结构","步骤，而bert是在128,000个单词中每个epoch训练了1m步。","源码链接","由于bert会通过再训练的方式来微调模型的参数，而在再训练模型中，由于不在存在[mask]信息，因此在训练中并不会隐藏掉所有的mask，而是通过以15%概率来选择那些单词被隐藏掉。此外在选择的被隐藏的单子中，采用如下措施来进行选择。","由于在bert中，有15%的","的large版本的参数为340m个。","的微调过程。","的词嵌表示。图2中表示的是bert的输入表示。","目前在预训练领域可以用于和bert进行比较的是openaigpt,openaigpt使用从左到右的方式在大量语料上进行训练。在整体结构上bert和openaigpt是相似的，因此整体结构差异较小，但是bert和openaigpt还有其他几个方面的差别。","目前有两种预训练的方法被用于下游处理任务中：基于特征的和基于微调（fine","答案对的形式。","经过和一个转移矩阵t转移后在利用一个分类网络来进行分类，微调的过程如下图所示：","结合一个单个模型的效果比传统的方法1.5的f1值的提升。","训练任务只是预测被隐藏掉的单词而不是重新构建整个句子。","许多自然语言处理任务中，如问题回答，语言推理都依赖于推断两个句子的关系，而这种关系在部分的文本预处理模型中都没有捕获。为了能够训练一个模型能够对两个句子的关系进行推理，在bert的预训练任务中，引入对对下一个句子预测的训练目标。在训练中，在一个句子对（sent","该任务是在一个有对抗生成网络生成的数据集（包含113k句子）上进行的实验任务，其任务是给定一个视频字幕中的一句话，推断最由可能的后续情节。其简单示意如下图所示：","该任务的微调过程和glue是类似的，在每个例子中，我们构建四个输入句子，其中每个都是一个数据句子a和可能后续进展b的结合。我们同样也引入一个转移矩阵t,通过将每个输入和矩阵t进行相乘后利用softmax来预测概率。在进行微调训练中，使用的学习速率为2","该数据集是一个二分类的任务，其任务目标类似于mnli,但是其训练数据较少。","该数据集是一个文本语义相似性数据集，其收集与一些新闻标题和其他来源，其预测任务是预测两个句子在语义上的多大的相似性。","该数据集是一个语言课接受性的二分类语料库，其预测的任务是一个英语句子在语言上是否是可以接受的。","语言表示模型的一个新趋势就是通过利用语言模型预训练一个模型，让后基于迁移学习的方法将其迁移到其他的下游任务中，在下游任务中在对这些模型的参数进行微调。这些方法的一个优势就是只有较少的参数需要进行重新学习。在这方面的工作中，最新的一个工作openai","语言预训练模型已经被证明能有效提升自然语言处理任务的性能。这些任务包括句子级别的任务如语言推理及解析，这些任务其目的是从句子级别推断句子间相互关系。此外还包括一些序列级别的任务，如命名实体识别，文本理解挑战任务（squad），这些任务模型中需要在序列级别产生经过微调后的更好的结果。","这篇文章的贡献如下：","项目","预训练任务","预训练流程","预训练，迁移学习","：表示隐藏层数量"],"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":["2018年","chang,","gener","googl","https://arxiv.org/abs/1810.04805","https://blog.csdn.net/jilrvrtrc/article/details/83829470","improv","jacobdevlin,m","kenton","kristina","languag","lee,","pre","toutanova","train","understand","wei","作者信息","作者单位","关键词","其他解读","内容","原文链接","时间","源码链接","项目","预训练，迁移学习"],"paper/nlp/papers/Deep contextualized word representations.html":["2018年","chang,","contextu","deep","googl","https://arxiv.org/abs/1810.04805","https://blog.csdn.net/jilrvrtrc/article/details/83829470","jacobdevlin,m","kenton","kristina","lee,","represent","toutanova","wei","word","作者信息","作者单位","关键词","其他解读","内容","原文链接","时间","源码链接","项目","预训练，迁移学习"],"paper/KnowledgeBase/readme.html":["knowledgebas"]},"length":10},"tokenStore":{"root":{"1":{"1":{"docs":{},"项":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"都":{"docs":{},"取":{"docs":{},"得":{"docs":{},"了":{"docs":{},"目":{"docs":{},"前":{"docs":{},"最":{"docs":{},"好":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{},"，":{"docs":{},"具":{"docs":{},"体":{"docs":{},"包":{"docs":{},"括":{"docs":{},"将":{"docs":{},"g":{"docs":{},"l":{"docs":{},"u":{"docs":{},"e":{"docs":{},"的":{"docs":{},"基":{"docs":{},"准":{"docs":{},"值":{"docs":{},"提":{"docs":{},"升":{"docs":{},"到":{"docs":{},"了":{"8":{"0":{"docs":{},".":{"4":{"docs":{},"%":{"docs":{},"(":{"7":{"docs":{},".":{"6":{"docs":{},"%":{"docs":{},"的":{"docs":{},"绝":{"docs":{},"对":{"docs":{},"提":{"docs":{},"升":{"docs":{},")":{"docs":{},"，":{"docs":{},"在":{"docs":{},"m":{"docs":{},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{},"n":{"docs":{},"l":{"docs":{},"i":{"docs":{},"中":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{},"有":{"docs":{},"了":{"8":{"6":{"docs":{},".":{"7":{"docs":{},"的":{"docs":{},"提":{"docs":{},"升":{"docs":{},"（":{"5":{"docs":{},".":{"6":{"docs":{},"%":{"docs":{},"的":{"docs":{},"绝":{"docs":{},"对":{"docs":{},"提":{"docs":{},"升":{"docs":{},"）":{"docs":{},"，":{"docs":{},"在":{"docs":{},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"d":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.057692307692307696}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}},"docs":{}}},"docs":{}},"docs":{}}},"达":{"docs":{},"到":{"8":{"6":{"docs":{},".":{"7":{"docs":{},"的":{"docs":{},"（":{"5":{"docs":{},".":{"6":{"docs":{},"%":{"docs":{},"的":{"docs":{},"绝":{"docs":{},"对":{"docs":{},"提":{"docs":{},"升":{"docs":{},"）":{"docs":{},"，":{"docs":{},"在":{"docs":{},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"d":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}},"docs":{}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}},"docs":{}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{},".":{"docs":{},"简":{"docs":{},"介":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}},"2":{"0":{"0":{"3":{"docs":{},"的":{"docs":{},"命":{"docs":{},"名":{"docs":{},"实":{"docs":{},"体":{"docs":{},"识":{"docs":{},"别":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"进":{"docs":{},"行":{"docs":{},"了":{"docs":{},"测":{"docs":{},"试":{"docs":{},"。":{"docs":{},"该":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"包":{"docs":{},"含":{"2":{"0":{"0":{"docs":{},"k":{"docs":{},"的":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"其":{"docs":{},"中":{"docs":{},"实":{"docs":{},"体":{"docs":{},"类":{"docs":{},"别":{"docs":{},"为":{"docs":{},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"o":{"docs":{},"n":{"docs":{},"（":{"docs":{},"人":{"docs":{},"物":{"docs":{},"）":{"docs":{},",":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{},"a":{"docs":{},"n":{"docs":{},"i":{"docs":{},"z":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"（":{"docs":{},"组":{"docs":{},"织":{"docs":{},"）":{"docs":{},",":{"docs":{},"l":{"docs":{},"o":{"docs":{},"c":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"（":{"docs":{},"地":{"docs":{},"点":{"docs":{},"）":{"docs":{},"，":{"docs":{},"m":{"docs":{},"i":{"docs":{},"s":{"docs":{},"c":{"docs":{},"e":{"docs":{},"l":{"docs":{},"l":{"docs":{},"a":{"docs":{},"n":{"docs":{},"e":{"docs":{},"o":{"docs":{},"u":{"docs":{},"s":{"docs":{},"（":{"docs":{},"其":{"docs":{},"他":{"docs":{},"项":{"docs":{},"）":{"docs":{},"和":{"docs":{},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{},"(":{"docs":{},"非":{"docs":{},"实":{"docs":{},"体":{"docs":{},"词":{"docs":{},")":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"1":{"8":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.047619047619047616}},"年":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.019230769230769232},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}},"9":{"docs":{},"年":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.019230769230769232}}}},"docs":{}},"docs":{}},"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},".":{"1":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"2":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"3":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"docs":{},"相":{"docs":{},"关":{"docs":{},"工":{"docs":{},"作":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}},"3":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},".":{"1":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"2":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"3":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},".":{"1":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"2":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"docs":{}}},"4":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"5":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"6":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"docs":{}}},"4":{"3":{"docs":{},"e":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"docs":{},",":{"docs":{},"b":{"1":{"docs":{},"参":{"docs":{},"数":{"docs":{},"为":{"0":{"docs":{},".":{"9":{"docs":{},"，":{"docs":{},"b":{"2":{"docs":{},"参":{"docs":{},"数":{"docs":{},"为":{"0":{"docs":{},".":{"9":{"9":{"9":{"docs":{},"，":{"docs":{},"l":{"2":{"docs":{},"范":{"docs":{},"数":{"docs":{},"的":{"docs":{},"权":{"docs":{},"重":{"docs":{},"为":{"0":{"docs":{},".":{"0":{"1":{"docs":{},".":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"docs":{}},"docs":{}}},"docs":{}}}}}}}},"docs":{}}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}}}},"docs":{}}}},"docs":{}}},"docs":{}}}}},"docs":{}}},".":{"1":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},".":{"1":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"docs":{}}},"2":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"4":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"docs":{},"实":{"docs":{},"验":{"docs":{},"及":{"docs":{},"结":{"docs":{},"果":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}},"5":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},",":{"2":{"docs":{},"e":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"3":{"docs":{},"e":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"4":{"docs":{},"e":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"docs":{},"b":{"docs":{},"a":{"docs":{},"t":{"docs":{},"c":{"docs":{},"h":{"docs":{},"_":{"docs":{},"s":{"docs":{},"i":{"docs":{},"z":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}},"总":{"docs":{},"共":{"docs":{},"迭":{"docs":{},"代":{"docs":{},"三":{"docs":{},"个":{"docs":{},"e":{"docs":{},"p":{"docs":{},"o":{"docs":{},"c":{"docs":{},"h":{"docs":{},".":{"docs":{},"其":{"docs":{},"对":{"docs":{},"比":{"docs":{},"的":{"docs":{},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{},"为":{"docs":{},"e":{"docs":{},"s":{"docs":{},"i":{"docs":{},"m":{"docs":{},"+":{"docs":{},"g":{"docs":{},"l":{"docs":{},"o":{"docs":{},"v":{"docs":{},"e":{"docs":{},"和":{"docs":{},"e":{"docs":{},"s":{"docs":{},"i":{"docs":{},"m":{"docs":{},"+":{"docs":{},"e":{"docs":{},"l":{"docs":{},"m":{"docs":{},"o":{"docs":{},".":{"docs":{},"实":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{},"如":{"docs":{},"下":{"docs":{},"：":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"而":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"的":{"docs":{},"学":{"docs":{},"习":{"docs":{},"率":{"docs":{},"则":{"docs":{},"是":{"docs":{},"根":{"docs":{},"据":{"docs":{},"特":{"docs":{},"定":{"docs":{},"任":{"docs":{},"务":{"docs":{},"需":{"docs":{},"求":{"docs":{},"设":{"docs":{},"置":{"docs":{},"来":{"docs":{},"进":{"docs":{},"行":{"docs":{},"微":{"docs":{},"调":{"docs":{},"的":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"等":{"docs":{},"。":{"docs":{},"实":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{},"如":{"docs":{},"下":{"docs":{},"图":{"docs":{},"所":{"docs":{},"示":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}},"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"r":{"docs":{},"o":{"docs":{},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":10}}}}}}}},"e":{"docs":{},"l":{"docs":{},"l":{"docs":{},"i":{"docs":{},"g":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"，":{"docs":{},"主":{"docs":{},"要":{"docs":{},"关":{"docs":{},"注":{"docs":{},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"领":{"docs":{},"域":{"docs":{},"的":{"docs":{},"最":{"docs":{},"新":{"docs":{},"研":{"docs":{},"究":{"docs":{},"进":{"docs":{},"展":{"docs":{},"，":{"docs":{},"其":{"docs":{},"中":{"docs":{},"也":{"docs":{},"有":{"docs":{},"大":{"docs":{},"量":{"docs":{},"的":{"docs":{},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"知":{"docs":{},"识":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"也":{"docs":{},"是":{"docs":{},"关":{"docs":{},"注":{"docs":{},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"领":{"docs":{},"域":{"docs":{},"的":{"docs":{},"另":{"docs":{},"外":{"docs":{},"一":{"docs":{},"个":{"docs":{},"重":{"docs":{},"要":{"docs":{},"学":{"docs":{},"术":{"docs":{},"会":{"docs":{},"议":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0182648401826484}}}}},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}}}}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0182648401826484}}}}}}},"c":{"docs":{},"m":{"docs":{},"l":{"docs":{},",":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}},"j":{"docs":{},"c":{"docs":{},"a":{"docs":{},"i":{"docs":{},",":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}},"m":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"v":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.047619047619047616},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.019230769230769232},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":1.7037037037037035}}}}}}}},"分":{"docs":{},"析":{"docs":{},"解":{"docs":{},"读":{"docs":{},"最":{"docs":{},"新":{"docs":{},"的":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"方":{"docs":{},"面":{"docs":{},"的":{"docs":{},"论":{"docs":{},"文":{"docs":{},"，":{"docs":{},"供":{"docs":{},"大":{"docs":{},"家":{"docs":{},"一":{"docs":{},"起":{"docs":{},"进":{"docs":{},"行":{"docs":{},"交":{"docs":{},"流":{"docs":{},"和":{"docs":{},"学":{"docs":{},"习":{"docs":{},"。":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"类":{"docs":{},"等":{"docs":{},"级":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"享":{"docs":{},"者":{"docs":{},"：":{"docs":{},"刘":{"docs":{},"露":{"docs":{},"平":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.11764705882352941}}}}}}}}},"将":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"知":{"docs":{},"识":{"docs":{},"进":{"docs":{},"行":{"docs":{},"整":{"docs":{},"理":{"docs":{},"后":{"docs":{},"分":{"docs":{},"享":{"docs":{},"，":{"docs":{},"方":{"docs":{},"便":{"docs":{},"后":{"docs":{},"面":{"docs":{},"的":{"docs":{},"人":{"docs":{},"学":{"docs":{},"习":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}}}}},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"方":{"docs":{},"面":{"docs":{},"的":{"docs":{},"工":{"docs":{},"具":{"docs":{},"进":{"docs":{},"行":{"docs":{},"整":{"docs":{},"理":{"docs":{},"汇":{"docs":{},"总":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}},"工":{"docs":{},"具":{"docs":{},"整":{"docs":{},"理":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}},"收":{"docs":{},"集":{"docs":{},"整":{"docs":{},"理":{"docs":{},"相":{"docs":{},"关":{"docs":{},"成":{"docs":{},"员":{"docs":{},"上":{"docs":{},"传":{"docs":{},"的":{"docs":{},"资":{"docs":{},"料":{"docs":{},"及":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"方":{"docs":{},"便":{"docs":{},"有":{"docs":{},"需":{"docs":{},"要":{"docs":{},"的":{"docs":{},"同":{"docs":{},"学":{"docs":{},"进":{"docs":{},"行":{"docs":{},"下":{"docs":{},"载":{"docs":{},"。":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"领":{"docs":{},"域":{"docs":{},"的":{"docs":{},"会":{"docs":{},"议":{"docs":{},"及":{"docs":{},"期":{"docs":{},"刊":{"docs":{},"，":{"docs":{},"方":{"docs":{},"便":{"docs":{},"大":{"docs":{},"家":{"docs":{},"进":{"docs":{},"行":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"查":{"docs":{},"阅":{"docs":{},"。":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"由":{"docs":{},"斯":{"docs":{},"坦":{"docs":{},"福":{"docs":{},"收":{"docs":{},"集":{"docs":{},"的":{"docs":{},"包":{"docs":{},"含":{"1":{"0":{"0":{"docs":{},"k":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{},"/":{"docs":{},"答":{"docs":{},"案":{"docs":{},"对":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"。":{"docs":{},"该":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"是":{"docs":{},"通":{"docs":{},"过":{"docs":{},"给":{"docs":{},"定":{"docs":{},"一":{"docs":{},"个":{"docs":{},"问":{"docs":{},"题":{"docs":{},"以":{"docs":{},"及":{"docs":{},"维":{"docs":{},"基":{"docs":{},"百":{"docs":{},"科":{"docs":{},"中":{"docs":{},"包":{"docs":{},"含":{"docs":{},"该":{"docs":{},"答":{"docs":{},"案":{"docs":{},"的":{"docs":{},"段":{"docs":{},"落":{"docs":{},"，":{"docs":{},"该":{"docs":{},"任":{"docs":{},"务":{"docs":{},"从":{"docs":{},"段":{"docs":{},"落":{"docs":{},"中":{"docs":{},"预":{"docs":{},"测":{"docs":{},"出":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"答":{"docs":{},"案":{"docs":{},"。":{"docs":{},"其":{"docs":{},"简":{"docs":{},"单":{"docs":{},"示":{"docs":{},"例":{"docs":{},"如":{"docs":{},"下":{"docs":{},"：":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}},"）":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}},"知":{"docs":{},"识":{"docs":{},"分":{"docs":{},"享":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}},"图":{"docs":{},"谱":{"docs":{},"概":{"docs":{},"述":{"docs":{},"p":{"docs":{},"p":{"docs":{},"t":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}}},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"处":{"docs":{},"理":{"docs":{},"会":{"docs":{},"议":{"docs":{},"及":{"docs":{},"期":{"docs":{},"刊":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}}}}}}}}},"论":{"docs":{},"文":{"docs":{},"解":{"docs":{},"读":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}},"该":{"docs":{},"博":{"docs":{},"客":{"docs":{},"主":{"docs":{},"要":{"docs":{},"收":{"docs":{},"集":{"docs":{},"整":{"docs":{},"理":{"docs":{},"与":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"相":{"docs":{},"关":{"docs":{},"工":{"docs":{},"具":{"docs":{},"，":{"docs":{},"论":{"docs":{},"文":{"docs":{},"以":{"docs":{},"及":{"docs":{},"代":{"docs":{},"码":{"docs":{},"等":{"docs":{},"资":{"docs":{},"源":{"docs":{},"信":{"docs":{},"息":{"docs":{},"，":{"docs":{},"对":{"docs":{},"目":{"docs":{},"前":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"最":{"docs":{},"前":{"docs":{},"沿":{"docs":{},"的":{"docs":{},"技":{"docs":{},"术":{"docs":{},"、":{"docs":{},"论":{"docs":{},"文":{"docs":{},"及":{"docs":{},"相":{"docs":{},"关":{"docs":{},"相":{"docs":{},"关":{"docs":{},"代":{"docs":{},"码":{"docs":{},"进":{"docs":{},"行":{"docs":{},"整":{"docs":{},"理":{"docs":{},"后":{"docs":{},"进":{"docs":{},"行":{"docs":{},"分":{"docs":{},"享":{"docs":{},"，":{"docs":{},"主":{"docs":{},"要":{"docs":{},"关":{"docs":{},"注":{"docs":{},"点":{"docs":{},"包":{"docs":{},"括":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"基":{"docs":{},"础":{"docs":{},"方":{"docs":{},"法":{"docs":{},"，":{"docs":{},"知":{"docs":{},"识":{"docs":{},"图":{"docs":{},"谱":{"docs":{},"构":{"docs":{},"建":{"docs":{},"，":{"docs":{},"智":{"docs":{},"能":{"docs":{},"问":{"docs":{},"答":{"docs":{},"等":{"docs":{},"几":{"docs":{},"个":{"docs":{},"方":{"docs":{},"面":{"docs":{},"的":{"docs":{},"知":{"docs":{},"识":{"docs":{},"。":{"docs":{},"通":{"docs":{},"过":{"docs":{},"不":{"docs":{},"断":{"docs":{},"地":{"docs":{},"积":{"docs":{},"累":{"docs":{},"让":{"docs":{},"大":{"docs":{},"家":{"docs":{},"在":{"docs":{},"这":{"docs":{},"条":{"docs":{},"路":{"docs":{},"上":{"docs":{},"越":{"docs":{},"走":{"docs":{},"越":{"docs":{},"远":{"docs":{},"。":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"材":{"docs":{},"料":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"视":{"docs":{},"频":{"docs":{},"，":{"docs":{},"讲":{"docs":{},"述":{"docs":{},"的":{"docs":{},"是":{"docs":{},"基":{"docs":{},"于":{"docs":{},"强":{"docs":{},"化":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"文":{"docs":{},"本":{"docs":{},"分":{"docs":{},"类":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"其":{"docs":{},"用":{"docs":{},"于":{"docs":{},"阅":{"docs":{},"读":{"docs":{},"理":{"docs":{},"解":{"docs":{},"中":{"docs":{},"，":{"docs":{},"感":{"docs":{},"谢":{"docs":{},"图":{"docs":{},"像":{"docs":{},"所":{"docs":{},"王":{"docs":{},"大":{"docs":{},"东":{"docs":{},"同":{"docs":{},"学":{"docs":{},"提":{"docs":{},"供":{"docs":{},"的":{"docs":{},"资":{"docs":{},"料":{"docs":{},"。":{"docs":{},"有":{"docs":{},"兴":{"docs":{},"趣":{"docs":{},"的":{"docs":{},"可":{"docs":{},"以":{"docs":{},"观":{"docs":{},"看":{"docs":{},"。":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"页":{"docs":{},"面":{"docs":{},"收":{"docs":{},"集":{"docs":{},"整":{"docs":{},"理":{"docs":{},"日":{"docs":{},"常":{"docs":{},"工":{"docs":{},"作":{"docs":{},"中":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"和":{"docs":{},"资":{"docs":{},"料":{"docs":{},"，":{"docs":{},"大":{"docs":{},"家":{"docs":{},"可":{"docs":{},"以":{"docs":{},"通":{"docs":{},"过":{"docs":{},"将":{"docs":{},"相":{"docs":{},"关":{"docs":{},"资":{"docs":{},"料":{"docs":{},"放":{"docs":{},"到":{"docs":{},"网":{"docs":{},"盘":{"docs":{},"或":{"docs":{},"者":{"docs":{},"其":{"docs":{},"他":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"u":{"docs":{},"b":{"docs":{},"仓":{"docs":{},"库":{"docs":{},"中":{"docs":{},"，":{"docs":{},"然":{"docs":{},"后":{"docs":{},"这":{"docs":{},"里":{"docs":{},"放":{"docs":{},"上":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"链":{"docs":{},"接":{"docs":{},"。":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"部":{"docs":{},"分":{"docs":{},"主":{"docs":{},"要":{"docs":{},"是":{"docs":{},"分":{"docs":{},"享":{"docs":{},"目":{"docs":{},"前":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"及":{"docs":{},"知":{"docs":{},"识":{"docs":{},"图":{"docs":{},"谱":{"docs":{},"处":{"docs":{},"理":{"docs":{},"方":{"docs":{},"法":{"docs":{},"的":{"docs":{},"基":{"docs":{},"础":{"docs":{},"知":{"docs":{},"识":{"docs":{},"，":{"docs":{},"目":{"docs":{},"前":{"docs":{},"包":{"docs":{},"含":{"docs":{},"如":{"docs":{},"下":{"docs":{},"三":{"docs":{},"个":{"docs":{},"方":{"docs":{},"面":{"docs":{},"：":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"基":{"docs":{},"础":{"docs":{},"技":{"docs":{},"术":{"docs":{},"，":{"docs":{},"知":{"docs":{},"识":{"docs":{},"图":{"docs":{},"谱":{"docs":{},"构":{"docs":{},"建":{"docs":{},"，":{"docs":{},"智":{"docs":{},"能":{"docs":{},"问":{"docs":{},"答":{"docs":{},",":{"docs":{},"论":{"docs":{},"文":{"docs":{},"整":{"docs":{},"理":{"docs":{},"将":{"docs":{},"按":{"docs":{},"照":{"docs":{},"年":{"docs":{},"份":{"docs":{},"和":{"docs":{},"板":{"docs":{},"块":{"docs":{},"来":{"docs":{},"进":{"docs":{},"行":{"docs":{},"整":{"docs":{},"理":{"docs":{},"。":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.047619047619047616}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"任":{"docs":{},"务":{"docs":{},"是":{"docs":{},"在":{"docs":{},"一":{"docs":{},"个":{"docs":{},"有":{"docs":{},"对":{"docs":{},"抗":{"docs":{},"生":{"docs":{},"成":{"docs":{},"网":{"docs":{},"络":{"docs":{},"生":{"docs":{},"成":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"（":{"docs":{},"包":{"docs":{},"含":{"1":{"1":{"3":{"docs":{},"k":{"docs":{},"句":{"docs":{},"子":{"docs":{},"）":{"docs":{},"上":{"docs":{},"进":{"docs":{},"行":{"docs":{},"的":{"docs":{},"实":{"docs":{},"验":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"其":{"docs":{},"任":{"docs":{},"务":{"docs":{},"是":{"docs":{},"给":{"docs":{},"定":{"docs":{},"一":{"docs":{},"个":{"docs":{},"视":{"docs":{},"频":{"docs":{},"字":{"docs":{},"幕":{"docs":{},"中":{"docs":{},"的":{"docs":{},"一":{"docs":{},"句":{"docs":{},"话":{"docs":{},"，":{"docs":{},"推":{"docs":{},"断":{"docs":{},"最":{"docs":{},"由":{"docs":{},"可":{"docs":{},"能":{"docs":{},"的":{"docs":{},"后":{"docs":{},"续":{"docs":{},"情":{"docs":{},"节":{"docs":{},"。":{"docs":{},"其":{"docs":{},"简":{"docs":{},"单":{"docs":{},"示":{"docs":{},"意":{"docs":{},"如":{"docs":{},"下":{"docs":{},"图":{"docs":{},"所":{"docs":{},"示":{"docs":{},"：":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}},"的":{"docs":{},"微":{"docs":{},"调":{"docs":{},"过":{"docs":{},"程":{"docs":{},"和":{"docs":{},"g":{"docs":{},"l":{"docs":{},"u":{"docs":{},"e":{"docs":{},"是":{"docs":{},"类":{"docs":{},"似":{"docs":{},"的":{"docs":{},"，":{"docs":{},"在":{"docs":{},"每":{"docs":{},"个":{"docs":{},"例":{"docs":{},"子":{"docs":{},"中":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"构":{"docs":{},"建":{"docs":{},"四":{"docs":{},"个":{"docs":{},"输":{"docs":{},"入":{"docs":{},"句":{"docs":{},"子":{"docs":{},"，":{"docs":{},"其":{"docs":{},"中":{"docs":{},"每":{"docs":{},"个":{"docs":{},"都":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"数":{"docs":{},"据":{"docs":{},"句":{"docs":{},"子":{"docs":{},"a":{"docs":{},"和":{"docs":{},"可":{"docs":{},"能":{"docs":{},"后":{"docs":{},"续":{"docs":{},"进":{"docs":{},"展":{"docs":{},"b":{"docs":{},"的":{"docs":{},"结":{"docs":{},"合":{"docs":{},"。":{"docs":{},"我":{"docs":{},"们":{"docs":{},"同":{"docs":{},"样":{"docs":{},"也":{"docs":{},"引":{"docs":{},"入":{"docs":{},"一":{"docs":{},"个":{"docs":{},"转":{"docs":{},"移":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"t":{"docs":{},",":{"docs":{},"通":{"docs":{},"过":{"docs":{},"将":{"docs":{},"每":{"docs":{},"个":{"docs":{},"输":{"docs":{},"入":{"docs":{},"和":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"t":{"docs":{},"进":{"docs":{},"行":{"docs":{},"相":{"docs":{},"乘":{"docs":{},"后":{"docs":{},"利":{"docs":{},"用":{"docs":{},"s":{"docs":{},"o":{"docs":{},"f":{"docs":{},"t":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"来":{"docs":{},"预":{"docs":{},"测":{"docs":{},"概":{"docs":{},"率":{"docs":{},"。":{"docs":{},"在":{"docs":{},"进":{"docs":{},"行":{"docs":{},"微":{"docs":{},"调":{"docs":{},"训":{"docs":{},"练":{"docs":{},"中":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"的":{"docs":{},"学":{"docs":{},"习":{"docs":{},"速":{"docs":{},"率":{"docs":{},"为":{"2":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"二":{"docs":{},"分":{"docs":{},"类":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"其":{"docs":{},"任":{"docs":{},"务":{"docs":{},"目":{"docs":{},"标":{"docs":{},"类":{"docs":{},"似":{"docs":{},"于":{"docs":{},"m":{"docs":{},"n":{"docs":{},"l":{"docs":{},"i":{"docs":{},",":{"docs":{},"但":{"docs":{},"是":{"docs":{},"其":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"较":{"docs":{},"少":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"文":{"docs":{},"本":{"docs":{},"语":{"docs":{},"义":{"docs":{},"相":{"docs":{},"似":{"docs":{},"性":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"其":{"docs":{},"收":{"docs":{},"集":{"docs":{},"与":{"docs":{},"一":{"docs":{},"些":{"docs":{},"新":{"docs":{},"闻":{"docs":{},"标":{"docs":{},"题":{"docs":{},"和":{"docs":{},"其":{"docs":{},"他":{"docs":{},"来":{"docs":{},"源":{"docs":{},"，":{"docs":{},"其":{"docs":{},"预":{"docs":{},"测":{"docs":{},"任":{"docs":{},"务":{"docs":{},"是":{"docs":{},"预":{"docs":{},"测":{"docs":{},"两":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"在":{"docs":{},"语":{"docs":{},"义":{"docs":{},"上":{"docs":{},"的":{"docs":{},"多":{"docs":{},"大":{"docs":{},"的":{"docs":{},"相":{"docs":{},"似":{"docs":{},"性":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"语":{"docs":{},"言":{"docs":{},"课":{"docs":{},"接":{"docs":{},"受":{"docs":{},"性":{"docs":{},"的":{"docs":{},"二":{"docs":{},"分":{"docs":{},"类":{"docs":{},"语":{"docs":{},"料":{"docs":{},"库":{"docs":{},"，":{"docs":{},"其":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"英":{"docs":{},"语":{"docs":{},"句":{"docs":{},"子":{"docs":{},"在":{"docs":{},"语":{"docs":{},"言":{"docs":{},"上":{"docs":{},"是":{"docs":{},"否":{"docs":{},"是":{"docs":{},"可":{"docs":{},"以":{"docs":{},"接":{"docs":{},"受":{"docs":{},"的":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"a":{"docs":{},"a":{"docs":{},"i":{"docs":{},",":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}},"c":{"docs":{},"(":{"docs":{},"l":{"docs":{},"t":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"l":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}},"的":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"t":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}},"（":{"docs":{},"t":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"）":{"docs":{},"，":{"docs":{},"此":{"docs":{},"外":{"docs":{},"还":{"docs":{},"有":{"docs":{},"一":{"docs":{},"些":{"docs":{},"期":{"docs":{},"刊":{"docs":{},"与":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"相":{"docs":{},"关":{"docs":{},"。":{"docs":{},"如":{"docs":{},"t":{"docs":{},"s":{"docs":{},"l":{"docs":{},"p":{"docs":{},"(":{"docs":{},"（":{"docs":{},"a":{"docs":{},"c":{"docs":{},"m":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"m":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"）":{"docs":{},"主":{"docs":{},"办":{"docs":{},"，":{"docs":{},"包":{"docs":{},"括":{"docs":{},"如":{"docs":{},"下":{"docs":{},"几":{"docs":{},"个":{"docs":{},"会":{"docs":{},"议":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}},"d":{"docs":{},"v":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},",":{"docs":{},"a":{"docs":{},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},"m":{"docs":{},"e":{"docs":{},"r":{"docs":{},"i":{"docs":{},"c":{"docs":{},"a":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}}}}}}}},"p":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0182648401826484}}}}}}},")":{"docs":{},"的":{"docs":{},"实":{"docs":{},"验":{"docs":{},"效":{"docs":{},"果":{"docs":{},"，":{"docs":{},"包":{"docs":{},"括":{"docs":{},"基":{"docs":{},"于":{"docs":{},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"d":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{},"，":{"docs":{},"语":{"docs":{},"义":{"docs":{},"分":{"docs":{},"析":{"docs":{},"以":{"docs":{},"及":{"docs":{},"命":{"docs":{},"名":{"docs":{},"识":{"docs":{},"别":{"docs":{},"识":{"docs":{},"别":{"docs":{},"等":{"docs":{},"众":{"docs":{},"多":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"v":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.047619047619047616},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.019230769230769232}}}}},"s":{"docs":{},"i":{"docs":{},"a":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"s":{"docs":{},"o":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0228310502283105}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},")":{"docs":{},"组":{"docs":{},"织":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}},"类":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}},"b":{"docs":{},"s":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{},"：":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.057692307692307696}},"本":{"docs":{},"文":{"docs":{},"作":{"docs":{},"者":{"docs":{},"推":{"docs":{},"出":{"docs":{},"了":{"docs":{},"一":{"docs":{},"套":{"docs":{},"新":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"表":{"docs":{},"达":{"docs":{},"模":{"docs":{},"型":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"，":{"docs":{},"全":{"docs":{},"称":{"docs":{},"为":{"docs":{},"b":{"docs":{},"i":{"docs":{},"d":{"docs":{},"i":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},",":{"docs":{},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}},"l":{"docs":{},".":{"docs":{},",":{"2":{"0":{"1":{"7":{"docs":{},")":{"docs":{},"从":{"docs":{},"另":{"docs":{},"外":{"docs":{},"一":{"docs":{},"个":{"docs":{},"重":{"docs":{},"要":{"docs":{},"角":{"docs":{},"度":{"docs":{},"来":{"docs":{},"对":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"进":{"docs":{},"行":{"docs":{},"处":{"docs":{},"理":{"docs":{},"。":{"docs":{},"在":{"docs":{},"该":{"docs":{},"方":{"docs":{},"法":{"docs":{},"中":{"docs":{},"，":{"docs":{},"作":{"docs":{},"者":{"docs":{},"提":{"docs":{},"出":{"docs":{},"了":{"docs":{},"从":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"提":{"docs":{},"取":{"docs":{},"出":{"docs":{},"与":{"docs":{},"上":{"docs":{},"下":{"docs":{},"文":{"docs":{},"敏":{"docs":{},"感":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"。":{"docs":{},"通":{"docs":{},"过":{"docs":{},"将":{"docs":{},"基":{"docs":{},"于":{"docs":{},"上":{"docs":{},"下":{"docs":{},"文":{"docs":{},"敏":{"docs":{},"感":{"docs":{},"的":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"和":{"docs":{},"特":{"docs":{},"定":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"结":{"docs":{},"构":{"docs":{},"相":{"docs":{},"结":{"docs":{},"合":{"docs":{},"后":{"docs":{},"，":{"docs":{},"e":{"docs":{},"l":{"docs":{},"m":{"docs":{},"o":{"docs":{},"在":{"docs":{},"很":{"docs":{},"多":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"都":{"docs":{},"取":{"docs":{},"得":{"docs":{},"了":{"docs":{},"s":{"docs":{},"o":{"docs":{},"a":{"docs":{},"t":{"docs":{},"(":{"docs":{},"s":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}},"b":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"版":{"docs":{},"本":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"有":{"1":{"1":{"0":{"docs":{},"m":{"docs":{},"个":{"docs":{},"，":{"docs":{},"而":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}},"t":{"docs":{},"c":{"docs":{},"h":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}},"类":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.2568493150684932}},":":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.047619047619047616},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.019230769230769232}}},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"多":{"docs":{},"层":{"docs":{},"的":{"docs":{},"基":{"docs":{},"于":{"docs":{},"双":{"docs":{},"向":{"docs":{},"转":{"docs":{},"移":{"docs":{},"编":{"docs":{},"码":{"docs":{},"器":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"其":{"docs":{},"根":{"docs":{},"据":{"docs":{},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"2":{"docs":{},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"实":{"docs":{},"践":{"docs":{},"。":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"使":{"docs":{},"用":{"docs":{},"的":{"docs":{},"“":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"有":{"docs":{},"两":{"docs":{},"个":{"docs":{},"版":{"docs":{},"本":{"docs":{},"：":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"和":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},",":{"docs":{},"其":{"docs":{},"中":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"如":{"docs":{},"下":{"docs":{},"：":{"docs":{},"l":{"docs":{},"=":{"1":{"2":{"docs":{},",":{"docs":{},"h":{"docs":{},"=":{"7":{"6":{"8":{"docs":{},",":{"docs":{},"a":{"docs":{},"=":{"1":{"2":{"docs":{},",":{"docs":{},"而":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"l":{"docs":{},"=":{"2":{"4":{"docs":{},",":{"docs":{},"h":{"docs":{},"=":{"1":{"0":{"2":{"4":{"docs":{},",":{"docs":{},"a":{"docs":{},"=":{"1":{"6":{"docs":{},".":{"docs":{},"因":{"docs":{},"此":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}},"docs":{}},"docs":{}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}},"docs":{}},"docs":{}},"docs":{}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"模":{"docs":{},"型":{"docs":{},"介":{"docs":{},"绍":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}},"的":{"docs":{},"训":{"docs":{},"练":{"docs":{},"语":{"docs":{},"料":{"docs":{},"是":{"docs":{},"在":{"docs":{},"一":{"docs":{},"个":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"p":{"docs":{},"u":{"docs":{},"s":{"docs":{},"(":{"8":{"0":{"0":{"docs":{},"m":{"docs":{},")":{"docs":{},"和":{"docs":{},"维":{"docs":{},"基":{"docs":{},"百":{"docs":{},"科":{"docs":{},"的":{"docs":{},"文":{"docs":{},"本":{"docs":{},"语":{"docs":{},"料":{"docs":{},"中":{"docs":{},"（":{"2":{"docs":{},",":{"5":{"0":{"0":{"docs":{},"m":{"docs":{},"个":{"docs":{},"单":{"docs":{},"词":{"docs":{},"）":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}},"输":{"docs":{},"入":{"docs":{},"表":{"docs":{},"示":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"过":{"docs":{},"程":{"docs":{},"和":{"docs":{},"目":{"docs":{},"前":{"docs":{},"已":{"docs":{},"有":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"类":{"docs":{},"似":{"docs":{},"，":{"docs":{},"其":{"docs":{},"再":{"docs":{},"训":{"docs":{},"练":{"docs":{},"语":{"docs":{},"料":{"docs":{},"我":{"docs":{},"们":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"两":{"docs":{},"部":{"docs":{},"分":{"docs":{},"的":{"docs":{},"语":{"docs":{},"料":{"docs":{},"：":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"p":{"docs":{},"u":{"docs":{},"s":{"docs":{},"(":{"8":{"0":{"0":{"docs":{},"m":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"d":{"docs":{},"i":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.047619047619047616},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.019230769230769232},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.2534246575342465}}}}}}}}},"l":{"docs":{},"o":{"docs":{},"c":{"docs":{},"k":{"docs":{},"s":{"docs":{},"）":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}},"）":{"docs":{},"中":{"docs":{},"，":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"b":{"docs":{},"有":{"5":{"0":{"docs":{},"%":{"docs":{},"是":{"docs":{},"句":{"docs":{},"子":{"docs":{},"a":{"docs":{},"的":{"docs":{},"下":{"docs":{},"一":{"docs":{},"句":{"docs":{},"，":{"docs":{},"而":{"5":{"0":{"docs":{},"%":{"docs":{},"的":{"docs":{},"可":{"docs":{},"能":{"docs":{},"是":{"docs":{},"随":{"docs":{},"机":{"docs":{},"另":{"docs":{},"外":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}},"c":{"docs":{},"c":{"docs":{},"f":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}}}},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}}}}}},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}}}}},"i":{"docs":{},"k":{"docs":{},"m":{"docs":{},",":{"docs":{},"该":{"docs":{},"会":{"docs":{},"议":{"docs":{},"名":{"docs":{},"称":{"docs":{},"缩":{"docs":{},"写":{"docs":{},"和":{"docs":{},"上":{"docs":{},"一":{"docs":{},"个":{"docs":{},"相":{"docs":{},"同":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"其":{"docs":{},"全":{"docs":{},"称":{"docs":{},"为":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"m":{"docs":{},",":{"docs":{},"其":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}},"o":{"docs":{},"l":{"docs":{},"e":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"(":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}},"a":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"m":{"docs":{},"m":{"docs":{},"i":{"docs":{},"t":{"docs":{},"t":{"docs":{},"e":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},"p":{"docs":{},"u":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.045662100456621}}}}}},"n":{"docs":{},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":5.036529680365296}},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"，":{"docs":{},"也":{"docs":{},"是":{"docs":{},"有":{"docs":{},"关":{"docs":{},"信":{"docs":{},"息":{"docs":{},"检":{"docs":{},"索":{"docs":{},"及":{"docs":{},"数":{"docs":{},"据":{"docs":{},"挖":{"docs":{},"掘":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"l":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"(":{"docs":{},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}},"t":{"docs":{},"e":{"docs":{},"x":{"docs":{},"t":{"docs":{},"u":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.047619047619047616},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.019230769230769232},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":2.54}}}}}}}}},"类":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"&":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"p":{"docs":{},"u":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":10}}}}}}}},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}},"e":{"docs":{},"p":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.09523809523809523},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.038461538461538464},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.2534246575342465},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":2.54}}}}}},"e":{"docs":{},"m":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}},"(":{"docs":{},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}},"是":{"docs":{},"有":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"下":{"docs":{},"面":{"docs":{},"的":{"docs":{},"比":{"docs":{},"较":{"docs":{},"著":{"docs":{},"名":{"docs":{},"的":{"docs":{},"兴":{"docs":{},"趣":{"docs":{},"小":{"docs":{},"组":{"docs":{},"（":{"docs":{},"s":{"docs":{},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"i":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}}}},"b":{"docs":{},"e":{"docs":{},"d":{"docs":{},"d":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},":":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"支":{"docs":{},"持":{"docs":{},"长":{"docs":{},"度":{"docs":{},"为":{"5":{"1":{"2":{"docs":{},"个":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"的":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"表":{"docs":{},"示":{"docs":{},"方":{"docs":{},"法":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}},"。":{"docs":{},"在":{"docs":{},"训":{"docs":{},"练":{"docs":{},"中":{"docs":{},"，":{"docs":{},"针":{"docs":{},"对":{"docs":{},"第":{"docs":{},"i":{"docs":{},"个":{"docs":{},"单":{"docs":{},"词":{"docs":{},"，":{"docs":{},"通":{"docs":{},"过":{"docs":{},"将":{"docs":{},"其":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"输":{"docs":{},"出":{"docs":{},"的":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"表":{"docs":{},"示":{"docs":{},"和":{"docs":{},"一":{"docs":{},"个":{"docs":{},"概":{"docs":{},"率":{"docs":{},"转":{"docs":{},"移":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"t":{"docs":{},"相":{"docs":{},"乘":{"docs":{},"后":{"docs":{},"利":{"docs":{},"用":{"docs":{},"s":{"docs":{},"o":{"docs":{},"f":{"docs":{},"t":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"输":{"docs":{},"出":{"docs":{},"得":{"docs":{},"到":{"docs":{},"该":{"docs":{},"但":{"docs":{},"是":{"docs":{},"是":{"docs":{},"否":{"docs":{},"是":{"docs":{},"答":{"docs":{},"案":{"docs":{},"开":{"docs":{},"始":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"。":{"docs":{},"针":{"docs":{},"对":{"docs":{},"答":{"docs":{},"案":{"docs":{},"的":{"docs":{},"结":{"docs":{},"束":{"docs":{},"，":{"docs":{},"也":{"docs":{},"是":{"docs":{},"使":{"docs":{},"用":{"docs":{},"同":{"docs":{},"样":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"预":{"docs":{},"测":{"docs":{},"某":{"docs":{},"个":{"docs":{},"词":{"docs":{},"是":{"docs":{},"否":{"docs":{},"是":{"docs":{},"答":{"docs":{},"案":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"而":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"目":{"docs":{},"标":{"docs":{},"是":{"docs":{},"最":{"docs":{},"大":{"docs":{},"化":{"docs":{},"正":{"docs":{},"确":{"docs":{},"开":{"docs":{},"始":{"docs":{},"位":{"docs":{},"置":{"docs":{},"和":{"docs":{},"结":{"docs":{},"束":{"docs":{},"位":{"docs":{},"置":{"docs":{},"的":{"docs":{},"释":{"docs":{},"然":{"docs":{},"概":{"docs":{},"率":{"docs":{},"。":{"docs":{},"该":{"docs":{},"过":{"docs":{},"程":{"docs":{},"的":{"docs":{},"示":{"docs":{},"意":{"docs":{},"图":{"docs":{},"如":{"docs":{},"图":{"docs":{},"所":{"docs":{},"示":{"docs":{},"：":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"方":{"docs":{},"法":{"docs":{},"来":{"docs":{},"实":{"docs":{},"现":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"表":{"docs":{},"示":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}},"，":{"docs":{},"而":{"docs":{},"答":{"docs":{},"案":{"docs":{},"使":{"docs":{},"用":{"docs":{},"b":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}},"v":{"docs":{},"a":{"docs":{},"l":{"docs":{},"u":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{},"其":{"docs":{},"由":{"docs":{},"欧":{"docs":{},"洲":{"docs":{},"语":{"docs":{},"言":{"docs":{},"资":{"docs":{},"源":{"docs":{},"组":{"docs":{},"织":{"docs":{},"进":{"docs":{},"行":{"docs":{},"e":{"docs":{},"l":{"docs":{},"r":{"docs":{},"a":{"docs":{},"(":{"docs":{},"e":{"docs":{},"u":{"docs":{},"r":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}},"该":{"docs":{},"会":{"docs":{},"议":{"docs":{},"由":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"的":{"docs":{},"特":{"docs":{},"殊":{"docs":{},"兴":{"docs":{},"趣":{"docs":{},"小":{"docs":{},"组":{"docs":{},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"l":{"docs":{},"e":{"docs":{},"x":{"docs":{},"进":{"docs":{},"行":{"docs":{},"组":{"docs":{},"织":{"docs":{},"，":{"docs":{},"每":{"docs":{},"年":{"docs":{},"都":{"docs":{},"会":{"docs":{},"举":{"docs":{},"办":{"docs":{},"，":{"docs":{},"国":{"docs":{},"内":{"docs":{},"也":{"docs":{},"有":{"docs":{},"很":{"docs":{},"多":{"docs":{},"研":{"docs":{},"究":{"docs":{},"机":{"docs":{},"构":{"docs":{},"及":{"docs":{},"公":{"docs":{},"司":{"docs":{},"参":{"docs":{},"与":{"docs":{},"，":{"docs":{},"如":{"docs":{},"哈":{"docs":{},"工":{"docs":{},"大":{"docs":{},"科":{"docs":{},"大":{"docs":{},"讯":{"docs":{},"飞":{"docs":{},"等":{"docs":{},",":{"docs":{},"其":{"docs":{},"最":{"docs":{},"新":{"docs":{},"的":{"docs":{},"链":{"docs":{},"接":{"docs":{},"网":{"docs":{},"址":{"docs":{},"如":{"docs":{},"下":{"docs":{},"：":{"docs":{},"h":{"docs":{},"t":{"docs":{},"t":{"docs":{},"p":{"docs":{},":":{"docs":{},"/":{"docs":{},"/":{"docs":{},"a":{"docs":{},"l":{"docs":{},"t":{"docs":{},".":{"docs":{},"q":{"docs":{},"c":{"docs":{},"r":{"docs":{},"i":{"docs":{},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{},"/":{"docs":{},"s":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"v":{"docs":{},"a":{"docs":{},"l":{"2":{"0":{"1":{"9":{"docs":{},"/":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{},".":{"docs":{},"p":{"docs":{},"h":{"docs":{},"p":{"docs":{},"?":{"docs":{},"i":{"docs":{},"d":{"docs":{},"=":{"docs":{},"t":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"c":{"docs":{},"o":{"docs":{},"d":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.057692307692307696},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.00684931506849315}},"e":{"docs":{},"r":{"docs":{},"”":{"docs":{},"编":{"docs":{},"码":{"docs":{},"器":{"docs":{},"近":{"docs":{},"来":{"docs":{},"得":{"docs":{},"到":{"docs":{},"广":{"docs":{},"泛":{"docs":{},"的":{"docs":{},"应":{"docs":{},"用":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{},"本":{"docs":{},"文":{"docs":{},"中":{"docs":{},"没":{"docs":{},"有":{"docs":{},"进":{"docs":{},"行":{"docs":{},"详":{"docs":{},"细":{"docs":{},"的":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"的":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}},"p":{"docs":{},"o":{"docs":{},"c":{"docs":{},"h":{"docs":{},"s":{"docs":{},":":{"3":{"docs":{},",":{"4":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"docs":{}}},"docs":{}}}}}}},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"g":{"docs":{},"r":{"docs":{},"o":{"docs":{},"u":{"docs":{},"p":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}},"s":{"docs":{},",":{"docs":{},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"s":{"docs":{},"）":{"docs":{},"之":{"docs":{},"一":{"docs":{},"的":{"docs":{},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},"(":{"docs":{},"s":{"docs":{},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"n":{"docs":{},"e":{"docs":{},"r":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.047619047619047616},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.019230769230769232},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":1.7037037037037035}}}}}},"l":{"docs":{},"u":{"docs":{},"e":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"通":{"docs":{},"用":{"docs":{},"的":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"理":{"docs":{},"解":{"docs":{},"评":{"docs":{},"估":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"用":{"docs":{},"于":{"docs":{},"评":{"docs":{},"估":{"docs":{},"对":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"的":{"docs":{},"理":{"docs":{},"解":{"docs":{},"任":{"docs":{},"务":{"docs":{},"。":{"docs":{},"g":{"docs":{},"l":{"docs":{},"u":{"docs":{},"e":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"包":{"docs":{},"含":{"docs":{},"多":{"docs":{},"个":{"docs":{},"子":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"的":{"docs":{},"实":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}},"o":{"docs":{},"o":{"docs":{},"g":{"docs":{},"l":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}}}},"p":{"docs":{},"t":{"2":{"0":{"1":{"8":{"docs":{},"基":{"docs":{},"于":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"在":{"docs":{},"许":{"docs":{},"多":{"docs":{},"句":{"docs":{},"子":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"取":{"docs":{},"得":{"docs":{},"了":{"docs":{},"s":{"docs":{},"o":{"docs":{},"t":{"docs":{},"a":{"docs":{},"的":{"docs":{},"实":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{},"(":{"docs":{},"g":{"docs":{},"e":{"docs":{},"n":{"docs":{},"e":{"docs":{},"r":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}},",":{"docs":{},"b":{"docs":{},"i":{"docs":{},"l":{"docs":{},"s":{"docs":{},"t":{"docs":{},"m":{"docs":{},"+":{"docs":{},"e":{"docs":{},"l":{"docs":{},"m":{"docs":{},"o":{"docs":{},"+":{"docs":{},"a":{"docs":{},"t":{"docs":{},"t":{"docs":{},"n":{"docs":{},"等":{"docs":{},"模":{"docs":{},"型":{"docs":{},"都":{"docs":{},"有":{"docs":{},"很":{"docs":{},"大":{"docs":{},"的":{"docs":{},"提":{"docs":{},"升":{"docs":{},"，":{"docs":{},"其":{"docs":{},"中":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"和":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"比":{"docs":{},"起":{"docs":{},"来":{"docs":{},"有":{"docs":{},"更":{"docs":{},"多":{"docs":{},"的":{"docs":{},"提":{"docs":{},"升":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"中":{"docs":{},"则":{"docs":{},"是":{"docs":{},"使":{"docs":{},"用":{"docs":{},"的":{"docs":{},"一":{"docs":{},"种":{"docs":{},"从":{"docs":{},"左":{"docs":{},"到":{"docs":{},"右":{"docs":{},"的":{"docs":{},"编":{"docs":{},"码":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"引":{"docs":{},"起":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"可":{"docs":{},"以":{"docs":{},"被":{"docs":{},"看":{"docs":{},"做":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{},"编":{"docs":{},"码":{"docs":{},"器":{"docs":{},"，":{"docs":{},"而":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"使":{"docs":{},"用":{"docs":{},"的":{"docs":{},"句":{"docs":{},"子":{"docs":{},"分":{"docs":{},"隔":{"docs":{},"符":{"docs":{},"为":{"docs":{},"[":{"docs":{},"s":{"docs":{},"e":{"docs":{},"p":{"docs":{},"]":{"docs":{},"，":{"docs":{},"分":{"docs":{},"类":{"docs":{},"器":{"docs":{},"的":{"docs":{},"分":{"docs":{},"隔":{"docs":{},"符":{"docs":{},"为":{"docs":{},"[":{"docs":{},"c":{"docs":{},"l":{"docs":{},"s":{"docs":{},"]":{"docs":{},",":{"docs":{},"并":{"docs":{},"且":{"docs":{},"这":{"docs":{},"些":{"docs":{},"只":{"docs":{},"在":{"docs":{},"微":{"docs":{},"调":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"使":{"docs":{},"用":{"docs":{},"。":{"docs":{},"而":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"则":{"docs":{},"是":{"docs":{},"学":{"docs":{},"习":{"docs":{},"了":{"docs":{},"[":{"docs":{},"s":{"docs":{},"e":{"docs":{},"p":{"docs":{},"]":{"docs":{},",":{"docs":{},"[":{"docs":{},"c":{"docs":{},"l":{"docs":{},"s":{"docs":{},"]":{"docs":{},"以":{"docs":{},"及":{"docs":{},"句":{"docs":{},"子":{"docs":{},"分":{"docs":{},"割":{"docs":{},"信":{"docs":{},"息":{"docs":{},"（":{"docs":{},"a":{"docs":{},"/":{"docs":{},"b":{"docs":{},"）":{"docs":{},"在":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"则":{"docs":{},"是":{"docs":{},"一":{"docs":{},"种":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"解":{"docs":{},"码":{"docs":{},"器":{"docs":{},"。":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"和":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}},"在":{"docs":{},"包":{"docs":{},"含":{"3":{"2":{"docs":{},",":{"0":{"0":{"0":{"docs":{},"个":{"docs":{},"单":{"docs":{},"词":{"docs":{},"的":{"docs":{},"语":{"docs":{},"料":{"docs":{},"库":{"docs":{},"中":{"docs":{},"每":{"docs":{},"个":{"docs":{},"e":{"docs":{},"p":{"docs":{},"o":{"docs":{},"c":{"docs":{},"h":{"docs":{},"训":{"docs":{},"练":{"docs":{},"了":{"1":{"docs":{},"m":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"docs":{}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}}},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"微":{"docs":{},"调":{"docs":{},"步":{"docs":{},"骤":{"docs":{},"中":{"docs":{},"都":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"相":{"docs":{},"同":{"docs":{},"的":{"docs":{},"学":{"docs":{},"习":{"docs":{},"率":{"5":{"docs":{},"e":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"docs":{}}}}}}}}}}}}}}}}}}}},"的":{"docs":{},"区":{"docs":{},"别":{"docs":{},"可":{"docs":{},"以":{"docs":{},"如":{"docs":{},"图":{"docs":{},"所":{"docs":{},"示":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}},"等":{"docs":{},"结":{"docs":{},"构":{"docs":{},"的":{"docs":{},"差":{"docs":{},"别":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}},"h":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"s":{"docs":{},"t":{"docs":{},"正":{"docs":{},"式":{"docs":{},"宣":{"docs":{},"布":{"docs":{},"成":{"docs":{},"立":{"docs":{},"国":{"docs":{},"际":{"docs":{},"计":{"docs":{},"算":{"docs":{},"语":{"docs":{},"言":{"docs":{},"学":{"docs":{},"学":{"docs":{},"会":{"docs":{},"亚":{"docs":{},"太":{"docs":{},"地":{"docs":{},"区":{"docs":{},"分":{"docs":{},"会":{"docs":{},"（":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"，":{"docs":{},"t":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"t":{"docs":{},"p":{"docs":{},":":{"docs":{},"/":{"docs":{},"/":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"2":{"0":{"1":{"8":{"docs":{},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}},"e":{"docs":{},"m":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"2":{"0":{"1":{"8":{"docs":{},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}},"n":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}},"w":{"docs":{},"w":{"docs":{},"w":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"l":{"docs":{},"l":{"docs":{},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{},"/":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}},"s":{"docs":{},":":{"docs":{},"/":{"docs":{},"/":{"docs":{},"b":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},".":{"docs":{},"c":{"docs":{},"s":{"docs":{},"d":{"docs":{},"n":{"docs":{},".":{"docs":{},"n":{"docs":{},"e":{"docs":{},"t":{"docs":{},"/":{"docs":{},"l":{"docs":{},"y":{"docs":{},"b":{"3":{"docs":{},"b":{"3":{"docs":{},"b":{"docs":{},"/":{"docs":{},"a":{"docs":{},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"l":{"docs":{},"e":{"docs":{},"/":{"docs":{},"d":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"i":{"docs":{},"l":{"docs":{},"s":{"docs":{},"/":{"8":{"3":{"5":{"4":{"8":{"9":{"6":{"4":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}},"j":{"docs":{},"i":{"docs":{},"l":{"docs":{},"r":{"docs":{},"v":{"docs":{},"r":{"docs":{},"t":{"docs":{},"r":{"docs":{},"c":{"docs":{},"/":{"docs":{},"a":{"docs":{},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"l":{"docs":{},"e":{"docs":{},"/":{"docs":{},"d":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"i":{"docs":{},"l":{"docs":{},"s":{"docs":{},"/":{"8":{"3":{"8":{"2":{"9":{"4":{"7":{"0":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"w":{"docs":{},"w":{"docs":{},"w":{"docs":{},".":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"w":{"docs":{},"e":{"docs":{},"b":{"docs":{},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{},"/":{"docs":{},"p":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}},"c":{"docs":{},"n":{"docs":{},"b":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"s":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"/":{"docs":{},"n":{"docs":{},"i":{"docs":{},"u":{"docs":{},"x":{"docs":{},"i":{"docs":{},"c":{"docs":{},"h":{"docs":{},"u":{"docs":{},"a":{"docs":{},"n":{"docs":{},"/":{"docs":{},"p":{"docs":{},"/":{"7":{"6":{"0":{"2":{"0":{"1":{"2":{"docs":{},".":{"docs":{},"h":{"docs":{},"t":{"docs":{},"m":{"docs":{},"l":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"v":{"docs":{},".":{"docs":{},"y":{"docs":{},"o":{"docs":{},"u":{"docs":{},"k":{"docs":{},"u":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"/":{"docs":{},"v":{"docs":{},"_":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"w":{"docs":{},"/":{"docs":{},"i":{"docs":{},"d":{"docs":{},"_":{"docs":{},"x":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"4":{"docs":{},"m":{"docs":{},"d":{"docs":{},"y":{"docs":{},"z":{"docs":{},"n":{"docs":{},"z":{"docs":{},"g":{"docs":{},"w":{"docs":{},"o":{"docs":{},"a":{"docs":{},"=":{"docs":{},"=":{"docs":{},".":{"docs":{},"h":{"docs":{},"t":{"docs":{},"m":{"docs":{},"l":{"docs":{},"?":{"docs":{},"s":{"docs":{},"p":{"docs":{},"m":{"docs":{},"=":{"docs":{},"a":{"2":{"docs":{},"h":{"3":{"docs":{},"j":{"docs":{},".":{"8":{"4":{"2":{"8":{"7":{"7":{"0":{"docs":{},".":{"3":{"4":{"1":{"6":{"0":{"5":{"9":{"docs":{},".":{"1":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"r":{"docs":{},"x":{"docs":{},"i":{"docs":{},"v":{"docs":{},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{},"/":{"docs":{},"a":{"docs":{},"b":{"docs":{},"s":{"docs":{},"/":{"1":{"8":{"1":{"0":{"docs":{},".":{"0":{"4":{"8":{"0":{"5":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}},"j":{"docs":{},"a":{"docs":{},"i":{"docs":{},"r":{"docs":{},",":{"docs":{},"j":{"docs":{},"o":{"docs":{},"u":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}},"c":{"docs":{},"o":{"docs":{},"b":{"docs":{},"d":{"docs":{},"e":{"docs":{},"v":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"m":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}}}}}}}}}}}},"m":{"docs":{},"l":{"docs":{},"r":{"docs":{},",":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"o":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"u":{"docs":{},"r":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":5.004566210045662}}}}}}}},"u":{"docs":{},"m":{"docs":{},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},":":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}}}}}},"k":{"docs":{},"n":{"docs":{},"o":{"docs":{},"w":{"docs":{},"l":{"docs":{},"e":{"docs":{},"d":{"docs":{},"g":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"e":{"docs":{},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{"paper/KnowledgeBase/readme.html":{"ref":"paper/KnowledgeBase/readme.html","tf":10}}}}}}}}}}}}},"e":{"docs":{},"y":{"docs":{},"w":{"docs":{},"o":{"docs":{},"r":{"docs":{},"d":{"docs":{},"s":{"docs":{},":":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.057692307692307696}}}}}}}}},"n":{"docs":{},"t":{"docs":{},"o":{"docs":{},"n":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}}}}},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"n":{"docs":{},"a":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}}}}}}}},"l":{"docs":{},"a":{"docs":{},"n":{"docs":{},"g":{"docs":{},"u":{"docs":{},"a":{"docs":{},"g":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.045662100456621},"paper/readme.html":{"ref":"paper/readme.html","tf":0.09523809523809523},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.038461538461538464},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.2534246575342465},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":1.7037037037037035}}}}}}}},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}},"g":{"docs":{},")":{"docs":{},"举":{"docs":{},"办":{"docs":{},"的":{"docs":{},"。":{"docs":{},"其":{"docs":{},"也":{"docs":{},"是":{"docs":{},"每":{"docs":{},"年":{"docs":{},"举":{"docs":{},"办":{"docs":{},"一":{"docs":{},"次":{"docs":{},"，":{"docs":{},"由":{"docs":{},"于":{"docs":{},"n":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"是":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"在":{"docs":{},"北":{"docs":{},"美":{"docs":{},"的":{"docs":{},"分":{"docs":{},"会":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{},"当":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"在":{"docs":{},"北":{"docs":{},"美":{"docs":{},"举":{"docs":{},"办":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"n":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"就":{"docs":{},"会":{"docs":{},"停":{"docs":{},"办":{"docs":{},"一":{"docs":{},"年":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},",":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}},"v":{"docs":{},"e":{"docs":{},"l":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"都":{"docs":{},"取":{"docs":{},"得":{"docs":{},"了":{"docs":{},"较":{"docs":{},"好":{"docs":{},"的":{"docs":{},"效":{"docs":{},"果":{"docs":{},"。":{"docs":{},"在":{"docs":{},"很":{"docs":{},"多":{"docs":{},"有":{"docs":{},"特":{"docs":{},"定":{"docs":{},"需":{"docs":{},"要":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"也":{"docs":{},"取":{"docs":{},"得":{"docs":{},"了":{"docs":{},"较":{"docs":{},"好":{"docs":{},"的":{"docs":{},"效":{"docs":{},"果":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"e":{"docs":{},"和":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"u":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"i":{"docs":{},"c":{"docs":{},"s":{"docs":{},")":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}},"组":{"docs":{},"织":{"docs":{},"的":{"docs":{},"。":{"docs":{},"该":{"docs":{},"会":{"docs":{},"议":{"docs":{},"每":{"docs":{},"两":{"docs":{},"年":{"docs":{},"举":{"docs":{},"办":{"docs":{},"一":{"docs":{},"次":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}},",":{"docs":{},"其":{"docs":{},"由":{"1":{"9":{"6":{"5":{"docs":{},"年":{"docs":{},"创":{"docs":{},"办":{"docs":{},"，":{"docs":{},"是":{"docs":{},"由":{"docs":{},"老":{"docs":{},"牌":{"docs":{},"的":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},"学":{"docs":{},"术":{"docs":{},"会":{"docs":{},"议":{"docs":{},"组":{"docs":{},"织":{"docs":{},"i":{"docs":{},"c":{"docs":{},"c":{"docs":{},"l":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"翻":{"docs":{},"译":{"docs":{},"过":{"docs":{},"来":{"docs":{},"是":{"docs":{},"计":{"docs":{},"算":{"docs":{},"机":{"docs":{},"语":{"docs":{},"言":{"docs":{},"协":{"docs":{},"会":{"docs":{},"，":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"与":{"docs":{},"计":{"docs":{},"算":{"docs":{},"语":{"docs":{},"言":{"docs":{},"学":{"docs":{},"领":{"docs":{},"域":{"docs":{},"（":{"docs":{},"以":{"docs":{},"下":{"docs":{},"简":{"docs":{},"称":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},"/":{"docs":{},"c":{"docs":{},"l":{"docs":{},"）":{"docs":{},"最":{"docs":{},"权":{"docs":{},"威":{"docs":{},"的":{"docs":{},"国":{"docs":{},"际":{"docs":{},"专":{"docs":{},"业":{"docs":{},"学":{"docs":{},"会":{"docs":{},"，":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"成":{"docs":{},"立":{"docs":{},"于":{"1":{"9":{"6":{"2":{"docs":{},"年":{"docs":{},"，":{"docs":{},"是":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"(":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},")":{"docs":{},"领":{"docs":{},"域":{"docs":{},"影":{"docs":{},"响":{"docs":{},"力":{"docs":{},"最":{"docs":{},"大":{"docs":{},"、":{"docs":{},"最":{"docs":{},"具":{"docs":{},"活":{"docs":{},"力":{"docs":{},"的":{"docs":{},"顶":{"docs":{},"级":{"docs":{},"国":{"docs":{},"际":{"docs":{},"学":{"docs":{},"术":{"docs":{},"组":{"docs":{},"织":{"docs":{},"，":{"docs":{},"每":{"docs":{},"年":{"docs":{},"举":{"docs":{},"办":{"docs":{},"一":{"docs":{},"次":{"docs":{},"。":{"docs":{},"这":{"docs":{},"个":{"docs":{},"学":{"docs":{},"会":{"docs":{},"主":{"docs":{},"办":{"docs":{},"了":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"该":{"docs":{},"会":{"docs":{},"议":{"docs":{},"是":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"在":{"docs":{},"北":{"docs":{},"美":{"docs":{},"的":{"docs":{},"分":{"docs":{},"会":{"docs":{},"，":{"docs":{},"也":{"docs":{},"是":{"docs":{},"有":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"主":{"docs":{},"办":{"docs":{},"。":{"docs":{},"其":{"docs":{},"是":{"docs":{},"有":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"下":{"docs":{},"面":{"docs":{},"的":{"docs":{},"兴":{"docs":{},"趣":{"docs":{},"小":{"docs":{},"组":{"docs":{},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"(":{"docs":{},"s":{"docs":{},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"和":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}},"）":{"docs":{},"。":{"docs":{},"此":{"docs":{},"次":{"docs":{},"成":{"docs":{},"立":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"亚":{"docs":{},"太":{"docs":{},"分":{"docs":{},"会":{"docs":{},"，":{"docs":{},"将":{"docs":{},"进":{"docs":{},"一":{"docs":{},"步":{"docs":{},"促":{"docs":{},"进":{"docs":{},"亚":{"docs":{},"太":{"docs":{},"地":{"docs":{},"区":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},"相":{"docs":{},"关":{"docs":{},"技":{"docs":{},"术":{"docs":{},"和":{"docs":{},"研":{"docs":{},"究":{"docs":{},"的":{"docs":{},"发":{"docs":{},"展":{"docs":{},"。":{"docs":{},"据":{"docs":{},"悉":{"docs":{},"，":{"docs":{},"首":{"docs":{},"届":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"会":{"docs":{},"议":{"docs":{},"预":{"docs":{},"计":{"docs":{},"在":{"2":{"0":{"2":{"0":{"docs":{},"年":{"docs":{},"举":{"docs":{},"行":{"docs":{},"，":{"docs":{},"此":{"docs":{},"后":{"docs":{},"将":{"docs":{},"每":{"docs":{},"两":{"docs":{},"年":{"docs":{},"举":{"docs":{},"行":{"docs":{},"一":{"docs":{},"次":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}},":":{"docs":{},"表":{"docs":{},"示":{"docs":{},"层":{"docs":{},"级":{"docs":{},"的":{"docs":{},"数":{"docs":{},"量":{"docs":{},"（":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}},"m":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},"\"":{"docs":{},"(":{"docs":{},"m":{"docs":{},"l":{"docs":{},"m":{"docs":{},")":{"docs":{},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{},"在":{"docs":{},"该":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"，":{"docs":{},"被":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"的":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"会":{"docs":{},"被":{"docs":{},"输":{"docs":{},"入":{"docs":{},"到":{"docs":{},"s":{"docs":{},"o":{"docs":{},"f":{"docs":{},"t":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"。":{"docs":{},"在":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"实":{"docs":{},"验":{"docs":{},"中":{"docs":{},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"采":{"docs":{},"用":{"1":{"5":{"docs":{},"%":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"随":{"docs":{},"机":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"输":{"docs":{},"入":{"docs":{},"句":{"docs":{},"子":{"docs":{},"中":{"docs":{},"的":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},".":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"和":{"docs":{},"n":{"docs":{},"e":{"docs":{},"x":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}},"m":{"docs":{},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{},"i":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}},"e":{"docs":{},"r":{"docs":{},"y":{"docs":{},",":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}},"n":{"docs":{},"a":{"docs":{},"g":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"，":{"docs":{},"其":{"docs":{},"关":{"docs":{},"注":{"docs":{},"信":{"docs":{},"息":{"docs":{},"管":{"docs":{},"理":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}},"k":{"docs":{},"e":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}},"s":{"docs":{},"k":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}},"e":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"d":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}}}}}},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{},"由":{"docs":{},"名":{"docs":{},"称":{"docs":{},"可":{"docs":{},"知":{"docs":{},"，":{"docs":{},"该":{"docs":{},"会":{"docs":{},"议":{"docs":{},"关":{"docs":{},"注":{"docs":{},"搜":{"docs":{},"索":{"docs":{},"和":{"docs":{},"数":{"docs":{},"据":{"docs":{},"挖":{"docs":{},"掘":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"l":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}},"r":{"docs":{},"p":{"docs":{},"c":{"docs":{},",":{"docs":{},"该":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"微":{"docs":{},"软":{"docs":{},"研":{"docs":{},"究":{"docs":{},"解":{"docs":{},"释":{"docs":{},"语":{"docs":{},"料":{"docs":{},"库":{"docs":{},"由":{"docs":{},"自":{"docs":{},"动":{"docs":{},"提":{"docs":{},"取":{"docs":{},"的":{"docs":{},"句":{"docs":{},"子":{"docs":{},"对":{"docs":{},"组":{"docs":{},"成":{"docs":{},"来":{"docs":{},"自":{"docs":{},"在":{"docs":{},"线":{"docs":{},"新":{"docs":{},"闻":{"docs":{},"来":{"docs":{},"源":{"docs":{},"，":{"docs":{},"带":{"docs":{},"有":{"docs":{},"人":{"docs":{},"工":{"docs":{},"注":{"docs":{},"释":{"docs":{},"判":{"docs":{},"断":{"docs":{},"两":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"语":{"docs":{},"义":{"docs":{},"是":{"docs":{},"否":{"docs":{},"一":{"docs":{},"致":{"docs":{},"相":{"docs":{},"等":{"docs":{},"的":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"的":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"t":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0228310502283105}}}}}},"c":{"docs":{},",":{"docs":{},"n":{"docs":{},"e":{"docs":{},"u":{"docs":{},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}},"e":{"docs":{},"u":{"docs":{},"r":{"docs":{},"a":{"docs":{},"l":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"x":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}},"i":{"docs":{},"p":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}},"l":{"docs":{},"p":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":10}},"/":{"docs":{},"c":{"docs":{},"l":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"其":{"docs":{},"他":{"docs":{},"国":{"docs":{},"际":{"docs":{},"会":{"docs":{},"议":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}},"i":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"小":{"docs":{},"型":{"docs":{},"的":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"推":{"docs":{},"断":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"在":{"docs":{},"g":{"docs":{},"l":{"docs":{},"u":{"docs":{},"e":{"docs":{},"的":{"docs":{},"网":{"docs":{},"页":{"docs":{},"中":{"docs":{},"指":{"docs":{},"出":{"docs":{},"该":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"在":{"docs":{},"构":{"docs":{},"造":{"docs":{},"上":{"docs":{},"存":{"docs":{},"在":{"docs":{},"一":{"docs":{},"定":{"docs":{},"我":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{},"。":{"docs":{},"因":{"docs":{},"此":{"docs":{},"在":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"排":{"docs":{},"除":{"docs":{},"了":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{},"g":{"docs":{},"p":{"docs":{},"t":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}}}}},"u":{"docs":{},"m":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}},"p":{"docs":{},"a":{"docs":{},"c":{"docs":{},"i":{"docs":{},"f":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":10}}}}}},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},")":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},")":{"docs":{},",":{"docs":{},"其":{"docs":{},"他":{"docs":{},"相":{"docs":{},"关":{"docs":{},"期":{"docs":{},"刊":{"docs":{},"及":{"docs":{},"投":{"docs":{},"稿":{"docs":{},"链":{"docs":{},"接":{"docs":{},"如":{"docs":{},"下":{"docs":{},"：":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}},"举":{"docs":{},"办":{"docs":{},"的":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},",":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},"）":{"docs":{},"领":{"docs":{},"域":{"docs":{},"的":{"docs":{},"一":{"docs":{},"些":{"docs":{},"著":{"docs":{},"名":{"docs":{},"会":{"docs":{},"议":{"docs":{},"及":{"docs":{},"期":{"docs":{},"刊":{"docs":{},"，":{"docs":{},"将":{"docs":{},"按":{"docs":{},"照":{"docs":{},"c":{"docs":{},"c":{"docs":{},"f":{"docs":{},"对":{"docs":{},"会":{"docs":{},"议":{"docs":{},"的":{"docs":{},"分":{"docs":{},"级":{"docs":{},"标":{"docs":{},"准":{"docs":{},"对":{"docs":{},"相":{"docs":{},"关":{"docs":{},"会":{"docs":{},"议":{"docs":{},"进":{"docs":{},"行":{"docs":{},"整":{"docs":{},"理":{"docs":{},"，":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"每":{"docs":{},"个":{"docs":{},"会":{"docs":{},"议":{"docs":{},"的":{"docs":{},"关":{"docs":{},"注":{"docs":{},"主":{"docs":{},"题":{"docs":{},"，":{"docs":{},"召":{"docs":{},"开":{"docs":{},"周":{"docs":{},"期":{"docs":{},"、":{"docs":{},"会":{"docs":{},"议":{"docs":{},"网":{"docs":{},"站":{"docs":{},"及":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"，":{"docs":{},"方":{"docs":{},"便":{"docs":{},"后":{"docs":{},"面":{"docs":{},"查":{"docs":{},"找":{"docs":{},"最":{"docs":{},"新":{"docs":{},"的":{"docs":{},"相":{"docs":{},"关":{"docs":{},"论":{"docs":{},"文":{"docs":{},"，":{"docs":{},"从":{"docs":{},"而":{"docs":{},"对":{"docs":{},"该":{"docs":{},"领":{"docs":{},"域":{"docs":{},"进":{"docs":{},"行":{"docs":{},"比":{"docs":{},"较":{"docs":{},"深":{"docs":{},"入":{"docs":{},"的":{"docs":{},"研":{"docs":{},"究":{"docs":{},"。":{"docs":{},"每":{"docs":{},"个":{"docs":{},"会":{"docs":{},"议":{"docs":{},"及":{"docs":{},"期":{"docs":{},"刊":{"docs":{},"都":{"docs":{},"有":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"入":{"docs":{},"口":{"docs":{},"链":{"docs":{},"接":{"docs":{},"，":{"docs":{},"方":{"docs":{},"便":{"docs":{},"进":{"docs":{},"行":{"docs":{},"查":{"docs":{},"看":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},".":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}},"）":{"docs":{},")":{"docs":{},"以":{"docs":{},"及":{"docs":{},"t":{"docs":{},"a":{"docs":{},"l":{"docs":{},"i":{"docs":{},"p":{"docs":{},"(":{"docs":{},"(":{"docs":{},"a":{"docs":{},"c":{"docs":{},"m":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.09523809523809523},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.038461538461538464},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.2568493150684932},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":1.7037037037037035}},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}},"o":{"docs":{},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"c":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}}}}}},"o":{"docs":{},"u":{"docs":{},"r":{"docs":{},"c":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}}}}}},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"e":{"docs":{},"v":{"docs":{},"a":{"docs":{},"l":{"docs":{},",":{"docs":{},"其":{"docs":{},"主":{"docs":{},"要":{"docs":{},"关":{"docs":{},"注":{"docs":{},"信":{"docs":{},"息":{"docs":{},"检":{"docs":{},"索":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"d":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.047619047619047616},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.07692307692307693},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.00684931506849315},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":2.54}}}}}}}}}},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"(":{"docs":{},"a":{"docs":{},"d":{"docs":{},"a":{"docs":{},"m":{"docs":{},")":{"docs":{},":":{"5":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"docs":{}}}}}}}},")":{"docs":{},"以":{"docs":{},"及":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"e":{"docs":{},"p":{"docs":{},"o":{"docs":{},"c":{"docs":{},"h":{"docs":{},"次":{"docs":{},"数":{"docs":{},"不":{"docs":{},"同":{"docs":{},"外":{"docs":{},"。":{"docs":{},"在":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"，":{"docs":{},"d":{"docs":{},"r":{"docs":{},"o":{"docs":{},"p":{"docs":{},"o":{"docs":{},"u":{"docs":{},"t":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"一":{"docs":{},"直":{"docs":{},"设":{"docs":{},"置":{"docs":{},"为":{"0":{"docs":{},".":{"1":{"docs":{},"，":{"docs":{},"而":{"docs":{},"新":{"docs":{},"增":{"docs":{},"参":{"docs":{},"数":{"docs":{},"的":{"docs":{},"相":{"docs":{},"关":{"docs":{},"设":{"docs":{},"置":{"docs":{},"是":{"docs":{},"和":{"docs":{},"任":{"docs":{},"务":{"docs":{},"先":{"docs":{},"关":{"docs":{},"的":{"docs":{},"。":{"docs":{},"作":{"docs":{},"者":{"docs":{},"在":{"docs":{},"训":{"docs":{},"练":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"，":{"docs":{},"发":{"docs":{},"现":{"docs":{},"如":{"docs":{},"下":{"docs":{},"参":{"docs":{},"数":{"docs":{},"在":{"docs":{},"多":{"docs":{},"个":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"具":{"docs":{},"有":{"docs":{},"较":{"docs":{},"好":{"docs":{},"的":{"docs":{},"表":{"docs":{},"现":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"e":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}},"s":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"c":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"m":{"docs":{},"a":{"docs":{},"n":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"e":{"docs":{},"v":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"a":{"docs":{},"l":{"docs":{},",":{"docs":{},"其":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"r":{"docs":{},",":{"docs":{},"其":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"s":{"docs":{},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}},"z":{"docs":{},"e":{"docs":{},":":{"1":{"6":{"docs":{},",":{"3":{"2":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}},"，":{"docs":{},"学":{"docs":{},"习":{"docs":{},"率":{"docs":{},"(":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}},"p":{"docs":{},"e":{"docs":{},"e":{"docs":{},"c":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}},"y":{"docs":{},"s":{"docs":{},"t":{"docs":{},"e":{"docs":{},"m":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},"q":{"docs":{},"q":{"docs":{},"a":{"docs":{},"d":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}},"u":{"docs":{},"a":{"docs":{},"d":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}},"s":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.047619047619047616},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.019230769230769232},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.2534246575342465}},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"。":{"docs":{},"与":{"docs":{},"近":{"docs":{},"年":{"docs":{},"来":{"docs":{},"提":{"docs":{},"出":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"不":{"docs":{},"一":{"docs":{},"样":{"docs":{},"的":{"docs":{},"地":{"docs":{},"方":{"docs":{},"在":{"docs":{},"于":{"docs":{},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"不":{"docs":{},"再":{"docs":{},"仅":{"docs":{},"仅":{"docs":{},"是":{"docs":{},"只":{"docs":{},"关":{"docs":{},"注":{"docs":{},"一":{"docs":{},"个":{"docs":{},"词":{"docs":{},"前":{"docs":{},"文":{"docs":{},"或":{"docs":{},"后":{"docs":{},"文":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"，":{"docs":{},"而":{"docs":{},"是":{"docs":{},"整":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"所":{"docs":{},"有":{"docs":{},"层":{"docs":{},"都":{"docs":{},"去":{"docs":{},"关":{"docs":{},"注":{"docs":{},"其":{"docs":{},"整":{"docs":{},"个":{"docs":{},"上":{"docs":{},"下":{"docs":{},"文":{"docs":{},"的":{"docs":{},"语":{"docs":{},"境":{"docs":{},"信":{"docs":{},"息":{"docs":{},"。":{"docs":{},"实":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{},"证":{"docs":{},"明":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"过":{"docs":{},"的":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"仅":{"docs":{},"仅":{"docs":{},"在":{"docs":{},"后":{"docs":{},"面":{"docs":{},"再":{"docs":{},"包":{"docs":{},"一":{"docs":{},"层":{"docs":{},"输":{"docs":{},"出":{"docs":{},"层":{"docs":{},"，":{"docs":{},"并":{"docs":{},"对":{"docs":{},"其":{"docs":{},"进":{"docs":{},"行":{"docs":{},"微":{"docs":{},"调":{"docs":{},"训":{"docs":{},"练":{"docs":{},"，":{"docs":{},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"将":{"docs":{},"其":{"docs":{},"应":{"docs":{},"用":{"docs":{},"到":{"docs":{},"其":{"docs":{},"他":{"docs":{},"多":{"docs":{},"种":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"例":{"docs":{},"如":{"docs":{},"问":{"docs":{},"答":{"docs":{},"、":{"docs":{},"语":{"docs":{},"言":{"docs":{},"推":{"docs":{},"断":{"docs":{},"等":{"docs":{},"，":{"docs":{},"并":{"docs":{},"且":{"docs":{},"在":{"docs":{},"这":{"docs":{},"些":{"docs":{},"后":{"docs":{},"端":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"并":{"docs":{},"不":{"docs":{},"需":{"docs":{},"要":{"docs":{},"根":{"docs":{},"据":{"docs":{},"特":{"docs":{},"定":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"需":{"docs":{},"要":{"docs":{},"对":{"docs":{},"模":{"docs":{},"型":{"docs":{},"结":{"docs":{},"构":{"docs":{},"进":{"docs":{},"行":{"docs":{},"修":{"docs":{},"改":{"docs":{},"。":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"是":{"docs":{},"一":{"docs":{},"种":{"docs":{},"概":{"docs":{},"念":{"docs":{},"上":{"docs":{},"简":{"docs":{},"单":{"docs":{},"，":{"docs":{},"但":{"docs":{},"根":{"docs":{},"据":{"docs":{},"经":{"docs":{},"验":{"docs":{},"推":{"docs":{},"断":{"docs":{},"其":{"docs":{},"具":{"docs":{},"有":{"docs":{},"强":{"docs":{},"大":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{},"。":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"在":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.057692307692307696}}}}}}}}}}}}}}}}}}}}}},"在":{"docs":{},"实":{"docs":{},"际":{"docs":{},"中":{"docs":{},"却":{"docs":{},"有":{"docs":{},"强":{"docs":{},"大":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"在":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"(":{"docs":{},"基":{"docs":{},"于":{"docs":{},"双":{"docs":{},"向":{"docs":{},"翻":{"docs":{},"译":{"docs":{},"编":{"docs":{},"码":{"docs":{},"的":{"docs":{},"表":{"docs":{},"示":{"docs":{},"模":{"docs":{},"型":{"docs":{},")":{"docs":{},"。":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"通":{"docs":{},"过":{"docs":{},"提":{"docs":{},"出":{"docs":{},"了":{"docs":{},"两":{"docs":{},"个":{"docs":{},"新":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"目":{"docs":{},"标":{"docs":{},"来":{"docs":{},"解":{"docs":{},"决":{"docs":{},"以":{"docs":{},"前":{"docs":{},"的":{"docs":{},"提":{"docs":{},"到":{"docs":{},"的":{"docs":{},"单":{"docs":{},"向":{"docs":{},"限":{"docs":{},"制":{"docs":{},"问":{"docs":{},"题":{"docs":{},"：":{"docs":{},"基":{"docs":{},"于":{"docs":{},"“":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{},"e":{"docs":{},"d":{"docs":{},"”":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"（":{"docs":{},"m":{"docs":{},"l":{"docs":{},"m":{"docs":{},"）":{"docs":{},"，":{"docs":{},"这":{"docs":{},"种":{"docs":{},"方":{"docs":{},"法":{"docs":{},"在":{"1":{"9":{"5":{"3":{"docs":{},"年":{"docs":{},"就":{"docs":{},"曾":{"docs":{},"经":{"docs":{},"被":{"docs":{},"提":{"docs":{},"出":{"docs":{},"。":{"docs":{},"这":{"docs":{},"种":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"通":{"docs":{},"过":{"docs":{},"随":{"docs":{},"机":{"docs":{},"将":{"docs":{},"输":{"docs":{},"入":{"docs":{},"句":{"docs":{},"子":{"docs":{},"中":{"docs":{},"的":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"进":{"docs":{},"行":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"，":{"docs":{},"然":{"docs":{},"后":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"目":{"docs":{},"标":{"docs":{},"就":{"docs":{},"是":{"docs":{},"基":{"docs":{},"于":{"docs":{},"上":{"docs":{},"下":{"docs":{},"文":{"docs":{},"来":{"docs":{},"预":{"docs":{},"测":{"docs":{},"出":{"docs":{},"被":{"docs":{},"覆":{"docs":{},"盖":{"docs":{},"掉":{"docs":{},"的":{"docs":{},"单":{"docs":{},"词":{"docs":{},"。":{"docs":{},"不":{"docs":{},"同":{"docs":{},"于":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"从":{"docs":{},"左":{"docs":{},"到":{"docs":{},"右":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"m":{"docs":{},"l":{"docs":{},"m":{"docs":{},"训":{"docs":{},"练":{"docs":{},"目":{"docs":{},"标":{"docs":{},"使":{"docs":{},"得":{"docs":{},"我":{"docs":{},"们":{"docs":{},"能":{"docs":{},"够":{"docs":{},"充":{"docs":{},"分":{"docs":{},"利":{"docs":{},"用":{"docs":{},"左":{"docs":{},"边":{"docs":{},"和":{"docs":{},"右":{"docs":{},"边":{"docs":{},"信":{"docs":{},"息":{"docs":{},"来":{"docs":{},"训":{"docs":{},"练":{"docs":{},"一":{"docs":{},"个":{"docs":{},"更":{"docs":{},"深":{"docs":{},"的":{"docs":{},"双":{"docs":{},"向":{"docs":{},"翻":{"docs":{},"译":{"docs":{},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{},"除":{"docs":{},"了":{"docs":{},"利":{"docs":{},"用":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{},"e":{"docs":{},"d":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"外":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"还":{"docs":{},"在":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"引":{"docs":{},"入":{"docs":{},"了":{"docs":{},"预":{"docs":{},"测":{"docs":{},"下":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"训":{"docs":{},"练":{"docs":{},"目":{"docs":{},"标":{"docs":{},"来":{"docs":{},"同":{"docs":{},"时":{"docs":{},"训":{"docs":{},"练":{"docs":{},"句":{"docs":{},"子":{"docs":{},"对":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"表":{"docs":{},"示":{"docs":{},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},")":{"docs":{},"引":{"docs":{},"入":{"docs":{},"了":{"docs":{},"最":{"docs":{},"小":{"docs":{},"任":{"docs":{},"务":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"，":{"docs":{},"在":{"docs":{},"下":{"docs":{},"游":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"其":{"docs":{},"通":{"docs":{},"过":{"docs":{},"简":{"docs":{},"单":{"docs":{},"的":{"docs":{},"微":{"docs":{},"调":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"调":{"docs":{},"整":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"。":{"docs":{},"在":{"docs":{},"以":{"docs":{},"前":{"docs":{},"的":{"docs":{},"工":{"docs":{},"作":{"docs":{},"中":{"docs":{},"，":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"方":{"docs":{},"法":{"docs":{},"都":{"docs":{},"是":{"docs":{},"通":{"docs":{},"过":{"docs":{},"使":{"docs":{},"用":{"docs":{},"同":{"docs":{},"样":{"docs":{},"的":{"docs":{},"目":{"docs":{},"标":{"docs":{},"函":{"docs":{},"数":{"docs":{},"并":{"docs":{},"通":{"docs":{},"过":{"docs":{},"双":{"docs":{},"向":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"来":{"docs":{},"学":{"docs":{},"习":{"docs":{},"更":{"docs":{},"好":{"docs":{},"的":{"docs":{},"通":{"docs":{},"用":{"docs":{},"语":{"docs":{},"言":{"docs":{},"表":{"docs":{},"示":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.09523809523809523},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.038461538461538464},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.2568493150684932},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":1.7037037037037035}}}}}},"o":{"docs":{},"o":{"docs":{},"l":{"docs":{"tools/readme.html":{"ref":"tools/readme.html","tf":10}}}},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}},"u":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"o":{"docs":{},"v":{"docs":{},"a":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}}}}}}}},"p":{"docs":{},"s":{"docs":{},"的":{"docs":{},"默":{"docs":{},"契":{"docs":{},"上":{"docs":{},"进":{"docs":{},"行":{"docs":{},"了":{"docs":{},"训":{"docs":{},"练":{"docs":{},"，":{"docs":{},"每":{"docs":{},"个":{"docs":{},"训":{"docs":{},"练":{"docs":{},"都":{"docs":{},"花":{"docs":{},"费":{"docs":{},"了":{"4":{"docs":{},"天":{"docs":{},"左":{"docs":{},"右":{"docs":{},"完":{"docs":{},"成":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}},"u":{"docs":{},"s":{"docs":{},"的":{"docs":{},"集":{"docs":{},"群":{"docs":{},"上":{"docs":{},"进":{"docs":{},"行":{"docs":{},"了":{"docs":{},"训":{"docs":{},"练":{"docs":{},"，":{"docs":{},"而":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"则":{"docs":{},"是":{"docs":{},"在":{"docs":{},"配":{"docs":{},"置":{"docs":{},"为":{"1":{"6":{"docs":{},"c":{"docs":{},"l":{"docs":{},"o":{"docs":{},"u":{"docs":{},"d":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"u":{"docs":{},"n":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"）":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"。":{"docs":{},"在":{"docs":{},"基":{"docs":{},"于":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"中":{"docs":{},"，":{"docs":{},"e":{"docs":{},"l":{"docs":{},"m":{"docs":{},"o":{"docs":{},"(":{"2":{"0":{"1":{"8":{"docs":{},")":{"docs":{},"是":{"docs":{},"一":{"docs":{},"种":{"docs":{},"基":{"docs":{},"于":{"docs":{},"特":{"docs":{},"征":{"docs":{},"任":{"docs":{},"务":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"其":{"docs":{},"包":{"docs":{},"含":{"docs":{},"一":{"docs":{},"个":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"型":{"docs":{},"以":{"docs":{},"及":{"docs":{},"一":{"docs":{},"些":{"docs":{},"其":{"docs":{},"他":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"。":{"docs":{},"在":{"docs":{},"微":{"docs":{},"调":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"，":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}},"u":{"docs":{},"a":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.09523809523809523},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.038461538461538464},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.2534246575342465},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":1.7037037037037035}}}}}}}}}}}},"w":{"docs":{},"e":{"docs":{},"b":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}},"i":{"docs":{},"d":{"docs":{},"e":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"o":{"docs":{},"r":{"docs":{},"k":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"p":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},"l":{"docs":{},"d":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}},"d":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.047619047619047616},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.019230769230769232},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":2.54}},"s":{"docs":{},")":{"docs":{},"和":{"docs":{},"英":{"docs":{},"语":{"docs":{},"维":{"docs":{},"基":{"docs":{},"百":{"docs":{},"科":{"docs":{},"(":{"2":{"docs":{},",":{"5":{"0":{"0":{"docs":{},"m":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}}}}}}}}}}}}},"s":{"docs":{},"d":{"docs":{},"m":{"docs":{},",":{"docs":{},"该":{"docs":{},"会":{"docs":{},"议":{"docs":{},"全":{"docs":{},"称":{"docs":{},"为":{"docs":{},"w":{"docs":{},"e":{"docs":{},"b":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}},"n":{"docs":{},"l":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"领":{"docs":{},"域":{"docs":{},"与":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"会":{"docs":{},"议":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}},"以":{"docs":{},"下":{"docs":{},"是":{"docs":{},"根":{"docs":{},"据":{"docs":{},"c":{"docs":{},"c":{"docs":{},"f":{"2":{"0":{"1":{"5":{"docs":{},"年":{"docs":{},"的":{"docs":{},"对":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"领":{"docs":{},"域":{"docs":{},"的":{"docs":{},"会":{"docs":{},"议":{"docs":{},"的":{"docs":{},"分":{"docs":{},"级":{"docs":{},"标":{"docs":{},"准":{"docs":{},"来":{"docs":{},"进":{"docs":{},"行":{"docs":{},"罗":{"docs":{},"列":{"docs":{},"和":{"docs":{},"整":{"docs":{},"理":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}},"及":{"docs":{},"分":{"docs":{},"类":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"w":{"docs":{},"相":{"docs":{},"结":{"docs":{},"合":{"docs":{},"并":{"docs":{},"经":{"docs":{},"过":{"docs":{},"s":{"docs":{},"o":{"docs":{},"f":{"docs":{},"t":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"分":{"docs":{},"了":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"进":{"docs":{},"行":{"docs":{},"微":{"docs":{},"调":{"docs":{},"。":{"docs":{},"在":{"docs":{},"每":{"docs":{},"个":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"的":{"docs":{},"学":{"docs":{},"习":{"docs":{},"率":{"docs":{},"为":{"5":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"会":{"docs":{},"议":{"docs":{},"名":{"docs":{},"称":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}},"网":{"docs":{},"站":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"全":{"docs":{},"称":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}},"是":{"docs":{},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}},"出":{"docs":{},"版":{"docs":{},"社":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"参":{"docs":{},"考":{"docs":{},"文":{"docs":{},"献":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"召":{"docs":{},"开":{"docs":{},"周":{"docs":{},"期":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"国":{"docs":{},"际":{"docs":{},"会":{"docs":{},"议":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"在":{"1":{"1":{"docs":{},"项":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"都":{"docs":{},"取":{"docs":{},"得":{"docs":{},"了":{"docs":{},"s":{"docs":{},"o":{"docs":{},"t":{"docs":{},"a":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"。":{"docs":{},"表":{"docs":{},"明":{"docs":{},"双":{"docs":{},"向":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"在":{"docs":{},"文":{"docs":{},"本":{"docs":{},"处":{"docs":{},"理":{"docs":{},"中":{"docs":{},"的":{"docs":{},"重":{"docs":{},"要":{"docs":{},"性":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},"国":{"docs":{},"际":{"docs":{},"上":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"，":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"，":{"docs":{},"e":{"docs":{},"n":{"docs":{},"m":{"docs":{},"l":{"docs":{},"p":{"docs":{},"和":{"docs":{},"n":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"是":{"docs":{},"默":{"docs":{},"认":{"docs":{},"的":{"docs":{},"四":{"docs":{},"大":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"顶":{"docs":{},"级":{"docs":{},"学":{"docs":{},"术":{"docs":{},"会":{"docs":{},"议":{"docs":{},"，":{"docs":{},"其":{"docs":{},"中":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"，":{"docs":{},"e":{"docs":{},"m":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},"和":{"docs":{},"n":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"都":{"docs":{},"是":{"docs":{},"由":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"及":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"子":{"docs":{},"组":{"docs":{},"织":{"docs":{},"举":{"docs":{},"办":{"docs":{},"的":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"中":{"docs":{},"，":{"docs":{},"一":{"docs":{},"般":{"docs":{},"来":{"docs":{},"将":{"docs":{},"，":{"docs":{},"大":{"docs":{},"家":{"docs":{},"更":{"docs":{},"关":{"docs":{},"注":{"docs":{},"学":{"docs":{},"术":{"docs":{},"会":{"docs":{},"议":{"docs":{},"，":{"docs":{},"其":{"docs":{},"主":{"docs":{},"要":{"docs":{},"原":{"docs":{},"因":{"docs":{},"是":{"docs":{},"发":{"docs":{},"表":{"docs":{},"周":{"docs":{},"期":{"docs":{},"短":{"docs":{},"，":{"docs":{},"通":{"docs":{},"过":{"docs":{},"会":{"docs":{},"议":{"docs":{},"也":{"docs":{},"可":{"docs":{},"以":{"docs":{},"进":{"docs":{},"行":{"docs":{},"深":{"docs":{},"入":{"docs":{},"的":{"docs":{},"交":{"docs":{},"流":{"docs":{},"。":{"docs":{},"但":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"领":{"docs":{},"域":{"docs":{},"也":{"docs":{},"有":{"docs":{},"自":{"docs":{},"己":{"docs":{},"的":{"docs":{},"学":{"docs":{},"术":{"docs":{},"期":{"docs":{},"刊":{"docs":{},"，":{"docs":{},"其":{"docs":{},"旗":{"docs":{},"舰":{"docs":{},"期":{"docs":{},"刊":{"docs":{},"有":{"docs":{},"两":{"docs":{},"个":{"docs":{},"，":{"docs":{},"分":{"docs":{},"别":{"docs":{},"是":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"u":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"也":{"docs":{},"是":{"docs":{},"其":{"docs":{},"主":{"docs":{},"流":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"，":{"docs":{},"在":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"领":{"docs":{},"域":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"学":{"docs":{},"术":{"docs":{},"会":{"docs":{},"议":{"docs":{},"包":{"docs":{},"括":{"docs":{},"i":{"docs":{},"c":{"docs":{},"m":{"docs":{},"l":{"docs":{},",":{"docs":{},"n":{"docs":{},"i":{"docs":{},"p":{"docs":{},"s":{"docs":{},",":{"docs":{},"u":{"docs":{},"a":{"docs":{},"i":{"docs":{},"及":{"docs":{},"a":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{},"s":{"docs":{},",":{"docs":{},"下":{"docs":{},"面":{"docs":{},"将":{"docs":{},"简":{"docs":{},"单":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"中":{"docs":{},"输":{"docs":{},"入":{"docs":{},"被":{"docs":{},"表":{"docs":{},"示":{"docs":{},"为":{"docs":{},"两":{"docs":{},"种":{"docs":{},"形":{"docs":{},"式":{"docs":{},"：":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"使":{"docs":{},"用":{"docs":{},"的":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"双":{"docs":{},"向":{"docs":{},"的":{"docs":{},"自":{"docs":{},"注":{"docs":{},"意":{"docs":{},"力":{"docs":{},"机":{"docs":{},"制":{"docs":{},"的":{"docs":{},"编":{"docs":{},"码":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"而":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"的":{"docs":{},"输":{"docs":{},"入":{"docs":{},"中":{"docs":{},"，":{"docs":{},"对":{"docs":{},"于":{"docs":{},"一":{"docs":{},"个":{"docs":{},"输":{"docs":{},"入":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"序":{"docs":{},"列":{"docs":{},"，":{"docs":{},"该":{"docs":{},"输":{"docs":{},"入":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"序":{"docs":{},"列":{"docs":{},"表":{"docs":{},"示":{"docs":{},"可":{"docs":{},"以":{"docs":{},"由":{"docs":{},"几":{"docs":{},"部":{"docs":{},"分":{"docs":{},"构":{"docs":{},"成":{"docs":{},"：":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"序":{"docs":{},"列":{"docs":{},"表":{"docs":{},"示":{"docs":{},"，":{"docs":{},"s":{"docs":{},"e":{"docs":{},"g":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"和":{"docs":{},"p":{"docs":{},"o":{"docs":{},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"e":{"docs":{},"r":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"，":{"docs":{},"由":{"docs":{},"于":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{},"并":{"docs":{},"不":{"docs":{},"知":{"docs":{},"道":{"docs":{},"哪":{"docs":{},"些":{"docs":{},"单":{"docs":{},"词":{"docs":{},"可":{"docs":{},"能":{"docs":{},"会":{"docs":{},"被":{"docs":{},"替":{"docs":{},"换":{"docs":{},"成":{"docs":{},"随":{"docs":{},"机":{"docs":{},"的":{"docs":{},"单":{"docs":{},"词":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{},"要":{"docs":{},"求":{"docs":{},"编":{"docs":{},"码":{"docs":{},"器":{"docs":{},"能":{"docs":{},"够":{"docs":{},"学":{"docs":{},"习":{"docs":{},"到":{"docs":{},"一":{"docs":{},"种":{"docs":{},"对":{"docs":{},"所":{"docs":{},"有":{"docs":{},"单":{"docs":{},"词":{"docs":{},"都":{"docs":{},"有":{"docs":{},"表":{"docs":{},"现":{"docs":{},"的":{"docs":{},"能":{"docs":{},"力":{"docs":{},"，":{"docs":{},"此":{"docs":{},"外":{"docs":{},"随":{"docs":{},"机":{"docs":{},"替":{"docs":{},"换":{"docs":{},"只":{"docs":{},"发":{"docs":{},"在":{"docs":{},"在":{"1":{"docs":{},".":{"5":{"docs":{},"%":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"中":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{},"不":{"docs":{},"会":{"docs":{},"影":{"docs":{},"响":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"整":{"docs":{},"体":{"docs":{},"理":{"docs":{},"解":{"docs":{},"能":{"docs":{},"力":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"实":{"docs":{},"验":{"docs":{},"中":{"docs":{},"，":{"docs":{},"作":{"docs":{},"者":{"docs":{},"同":{"docs":{},"时":{"docs":{},"发":{"docs":{},"现":{"1":{"0":{"0":{"docs":{},"k":{"docs":{},"+":{"docs":{},"左":{"docs":{},"右":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"对":{"docs":{},"于":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"的":{"docs":{},"变":{"docs":{},"化":{"docs":{},"是":{"docs":{},"极":{"docs":{},"小":{"docs":{},"的":{"docs":{},"。":{"docs":{},"因":{"docs":{},"此":{"docs":{},"微":{"docs":{},"调":{"docs":{},"过":{"docs":{},"程":{"docs":{},"是":{"docs":{},"十":{"docs":{},"分":{"docs":{},"快":{"docs":{},"速":{"docs":{},"的":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}},"微":{"docs":{},"调":{"docs":{},"训":{"docs":{},"练":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"，":{"docs":{},"大":{"docs":{},"部":{"docs":{},"分":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"和":{"docs":{},"原":{"docs":{},"来":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"是":{"docs":{},"一":{"docs":{},"样":{"docs":{},"的":{"docs":{},"。":{"docs":{},"除":{"docs":{},"了":{"docs":{},"b":{"docs":{},"a":{"docs":{},"t":{"docs":{},"c":{"docs":{},"h":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"构":{"docs":{},"建":{"docs":{},"训":{"docs":{},"练":{"docs":{},"集":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"在":{"docs":{},"选":{"docs":{},"择":{"docs":{},"句":{"docs":{},"子":{"docs":{},"a":{"docs":{},"的":{"docs":{},"下":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"时":{"docs":{},"，":{"docs":{},"通":{"docs":{},"过":{"docs":{},"随":{"docs":{},"机":{"docs":{},"选":{"docs":{},"择":{"docs":{},"下":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"实":{"docs":{},"现":{"docs":{},"构":{"docs":{},"建":{"docs":{},"训":{"docs":{},"练":{"docs":{},"集":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"每":{"docs":{},"层":{"docs":{},"中":{"docs":{},"我":{"docs":{},"们":{"docs":{},"设":{"docs":{},"置":{"docs":{},"d":{"docs":{},"r":{"docs":{},"o":{"docs":{},"p":{"docs":{},"o":{"docs":{},"u":{"docs":{},"t":{"docs":{},"概":{"docs":{},"率":{"docs":{},"为":{"0":{"docs":{},".":{"1":{"docs":{},"，":{"docs":{},"激":{"docs":{},"活":{"docs":{},"函":{"docs":{},"数":{"docs":{},"选":{"docs":{},"择":{"docs":{},"的":{"docs":{},"是":{"docs":{},"g":{"docs":{},"e":{"docs":{},"l":{"docs":{},"u":{"docs":{},"。":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"损":{"docs":{},"失":{"docs":{},"函":{"docs":{},"数":{"docs":{},"l":{"docs":{},"m":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"平":{"docs":{},"均":{"docs":{},"l":{"docs":{},"i":{"docs":{},"k":{"docs":{},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"h":{"docs":{},"o":{"docs":{},"o":{"docs":{},"d":{"docs":{},"的":{"docs":{},"和":{"docs":{},"以":{"docs":{},"及":{"docs":{},"预":{"docs":{},"测":{"docs":{},"下":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"平":{"docs":{},"均":{"docs":{},"l":{"docs":{},"i":{"docs":{},"k":{"docs":{},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"h":{"docs":{},"o":{"docs":{},"o":{"docs":{},"d":{"docs":{},".":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}},"第":{"docs":{},"三":{"docs":{},"节":{"docs":{},"中":{"docs":{},"，":{"docs":{},"详":{"docs":{},"细":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"模":{"docs":{},"型":{"docs":{},"机":{"docs":{},"器":{"docs":{},"实":{"docs":{},"现":{"docs":{},"细":{"docs":{},"节":{"docs":{},"。":{"docs":{},"该":{"docs":{},"节":{"docs":{},"首":{"docs":{},"选":{"docs":{},"叙":{"docs":{},"述":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"模":{"docs":{},"块":{"docs":{},"的":{"docs":{},"整":{"docs":{},"体":{"docs":{},"框":{"docs":{},"架":{"docs":{},"及":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"的":{"docs":{},"输":{"docs":{},"入":{"docs":{},"表":{"docs":{},"示":{"docs":{},"。":{"docs":{},"接":{"docs":{},"着":{"docs":{},"在":{"3":{"docs":{},".":{"3":{"docs":{},"节":{"docs":{},"中":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"以":{"docs":{},"及":{"docs":{},"核":{"docs":{},"心":{"docs":{},"的":{"docs":{},"创":{"docs":{},"新":{"docs":{},"。":{"3":{"docs":{},".":{"4":{"docs":{},"节":{"docs":{},"中":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"流":{"docs":{},"程":{"docs":{},"，":{"docs":{},"而":{"docs":{},"在":{"3":{"docs":{},".":{"5":{"docs":{},"节":{"docs":{},"中":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"微":{"docs":{},"调":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"。":{"docs":{},"最":{"docs":{},"后":{"docs":{},"在":{"3":{"docs":{},".":{"6":{"docs":{},"节":{"docs":{},"中":{"docs":{},"讨":{"docs":{},"论":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"和":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"训":{"docs":{},"练":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"，":{"docs":{},"作":{"docs":{},"者":{"docs":{},"在":{"docs":{},"配":{"docs":{},"置":{"docs":{},"有":{"4":{"docs":{},"c":{"docs":{},"l":{"docs":{},"o":{"docs":{},"u":{"docs":{},"d":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}},"该":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"训":{"docs":{},"练":{"docs":{},"时":{"docs":{},"使":{"docs":{},"用":{"docs":{},"的":{"docs":{},"额":{"docs":{},"学":{"docs":{},"习":{"docs":{},"率":{"docs":{},"为":{"5":{"docs":{},"e":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"docs":{}}}}}}}}}}}}}}}},"类":{"docs":{},"型":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"和":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"句":{"docs":{},"子":{"docs":{},"分":{"docs":{},"类":{"docs":{},"任":{"docs":{},"务":{"docs":{},"有":{"docs":{},"明":{"docs":{},"显":{"docs":{},"的":{"docs":{},"不":{"docs":{},"同":{"docs":{},"，":{"docs":{},"在":{"docs":{},"对":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"进":{"docs":{},"行":{"docs":{},"微":{"docs":{},"调":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"将":{"docs":{},"问":{"docs":{},"题":{"docs":{},"和":{"docs":{},"答":{"docs":{},"案":{"docs":{},"都":{"docs":{},"表":{"docs":{},"示":{"docs":{},"成":{"docs":{},"两":{"docs":{},"个":{"docs":{},"独":{"docs":{},"立":{"docs":{},"的":{"docs":{},"序":{"docs":{},"列":{"docs":{},"。":{"docs":{},"其":{"docs":{},"中":{"docs":{},"问":{"docs":{},"题":{"docs":{},"使":{"docs":{},"用":{"docs":{},"a":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"项":{"docs":{},"工":{"docs":{},"作":{"docs":{},"中":{"docs":{},"，":{"docs":{},"作":{"docs":{},"者":{"docs":{},"定":{"docs":{},"义":{"docs":{},"了":{"docs":{},"如":{"docs":{},"下":{"docs":{},"变":{"docs":{},"量":{"docs":{},"：":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}},"输":{"docs":{},"入":{"docs":{},"词":{"docs":{},"序":{"docs":{},"列":{"docs":{},"中":{"docs":{},"，":{"docs":{},"序":{"docs":{},"列":{"docs":{},"的":{"docs":{},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"词":{"docs":{},"都":{"docs":{},"是":{"docs":{},"以":{"docs":{},"[":{"docs":{},"c":{"docs":{},"l":{"docs":{},"s":{"docs":{},"]":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"开":{"docs":{},"头":{"docs":{},"，":{"docs":{},"针":{"docs":{},"对":{"docs":{},"该":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"的":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"这":{"docs":{},"篇":{"docs":{},"文":{"docs":{},"章":{"docs":{},"中":{"docs":{},"我":{"docs":{},"们":{"docs":{},"证":{"docs":{},"明":{"docs":{},"了":{"docs":{},"双":{"docs":{},"向":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"重":{"docs":{},"要":{"docs":{},"性":{"docs":{},"。":{"docs":{},"不":{"docs":{},"同":{"docs":{},"于":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"双":{"docs":{},"向":{"docs":{},"预":{"docs":{},"测":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"一":{"docs":{},"种":{"docs":{},"基":{"docs":{},"于":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{},"e":{"docs":{},"d":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"来":{"docs":{},"训":{"docs":{},"练":{"docs":{},"一":{"docs":{},"种":{"docs":{},"更":{"docs":{},"深":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{},"此":{"docs":{},"外":{"docs":{},"区":{"docs":{},"别":{"docs":{},"于":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"浅":{"docs":{},"层":{"docs":{},"双":{"docs":{},"向":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"是":{"docs":{},"一":{"docs":{},"种":{"docs":{},"使":{"docs":{},"用":{"docs":{},"深":{"docs":{},"层":{"docs":{},"次":{"docs":{},"的":{"docs":{},"双":{"docs":{},"向":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"任":{"docs":{},"务":{"docs":{},"目":{"docs":{},"前":{"docs":{},"的":{"docs":{},"技":{"docs":{},"术":{"docs":{},"严":{"docs":{},"重":{"docs":{},"限":{"docs":{},"制":{"docs":{},"了":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"在":{"docs":{},"语":{"docs":{},"言":{"docs":{},"表":{"docs":{},"达":{"docs":{},"方":{"docs":{},"面":{"docs":{},"的":{"docs":{},"能":{"docs":{},"力":{"docs":{},"，":{"docs":{},"特":{"docs":{},"别":{"docs":{},"是":{"docs":{},"针":{"docs":{},"对":{"docs":{},"微":{"docs":{},"调":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"。":{"docs":{},"最":{"docs":{},"大":{"docs":{},"的":{"docs":{},"缺":{"docs":{},"陷":{"docs":{},"是":{"docs":{},"目":{"docs":{},"前":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"型":{"docs":{},"都":{"docs":{},"是":{"docs":{},"双":{"docs":{},"向":{"docs":{},"的":{"docs":{},"，":{"docs":{},"其":{"docs":{},"严":{"docs":{},"格":{"docs":{},"限":{"docs":{},"制":{"docs":{},"了":{"docs":{},"模":{"docs":{},"型":{"docs":{},"在":{"docs":{},"进":{"docs":{},"行":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"时":{"docs":{},"的":{"docs":{},"选":{"docs":{},"择":{"docs":{},"能":{"docs":{},"力":{"docs":{},"。":{"docs":{},"例":{"docs":{},"如":{"docs":{},"在":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{},"g":{"docs":{},"p":{"docs":{},"中":{"docs":{},"，":{"docs":{},"作":{"docs":{},"者":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"从":{"docs":{},"左":{"docs":{},"到":{"docs":{},"右":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"使":{"docs":{},"得":{"docs":{},"句":{"docs":{},"子":{"docs":{},"中":{"docs":{},"每":{"docs":{},"个":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"只":{"docs":{},"能":{"docs":{},"被":{"docs":{},"以":{"docs":{},"前":{"docs":{},"的":{"docs":{},"词":{"docs":{},"所":{"docs":{},"关":{"docs":{},"注":{"docs":{},"。":{"docs":{},"这":{"docs":{},"种":{"docs":{},"方":{"docs":{},"式":{"docs":{},"对":{"docs":{},"句":{"docs":{},"子":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"是":{"docs":{},"次":{"docs":{},"优":{"docs":{},"的":{"docs":{},"(":{"docs":{},"s":{"docs":{},"u":{"docs":{},"b":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"论":{"docs":{},"文":{"docs":{},"中":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"提":{"docs":{},"出":{"docs":{},"了":{"docs":{},"一":{"docs":{},"种":{"docs":{},"基":{"docs":{},"于":{"docs":{},"微":{"docs":{},"调":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},":":{"docs":{},"b":{"docs":{},"i":{"docs":{},"d":{"docs":{},"i":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"节":{"docs":{},"中":{"docs":{},"我":{"docs":{},"们":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"基":{"docs":{},"于":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"生":{"docs":{},"成":{"docs":{},"语":{"docs":{},"言":{"docs":{},"表":{"docs":{},"示":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"相":{"docs":{},"关":{"docs":{},"方":{"docs":{},"方":{"docs":{},"法":{"docs":{},"，":{"docs":{},"同":{"docs":{},"时":{"docs":{},"简":{"docs":{},"要":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"一":{"docs":{},"下":{"docs":{},"在":{"docs":{},"这":{"docs":{},"个":{"docs":{},"领":{"docs":{},"域":{"docs":{},"目":{"docs":{},"前":{"docs":{},"最":{"docs":{},"流":{"docs":{},"行":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"进":{"docs":{},"行":{"docs":{},"命":{"docs":{},"名":{"docs":{},"实":{"docs":{},"体":{"docs":{},"识":{"docs":{},"别":{"docs":{},"的":{"docs":{},"微":{"docs":{},"调":{"docs":{},"时":{"docs":{},"时":{"docs":{},"，":{"docs":{},"针":{"docs":{},"对":{"docs":{},"每":{"docs":{},"个":{"docs":{},"词":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"其":{"docs":{},"最":{"docs":{},"后":{"docs":{},"一":{"docs":{},"个":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"状":{"docs":{},"态":{"docs":{},"c":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"针":{"docs":{},"对":{"docs":{},"g":{"docs":{},"l":{"docs":{},"u":{"docs":{},"e":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"的":{"docs":{},"微":{"docs":{},"调":{"docs":{},"训":{"docs":{},"练":{"docs":{},"中":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"每":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"输":{"docs":{},"入":{"docs":{},"[":{"docs":{},"c":{"docs":{},"l":{"docs":{},"s":{"docs":{},"]":{"docs":{},"的":{"docs":{},"最":{"docs":{},"后":{"docs":{},"一":{"docs":{},"个":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"层":{"docs":{},"输":{"docs":{},"出":{"docs":{},"c":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"中":{"docs":{},"，":{"docs":{},"在":{"docs":{},"每":{"docs":{},"个":{"docs":{},"e":{"docs":{},"p":{"docs":{},"o":{"docs":{},"c":{"docs":{},"h":{"docs":{},"中":{"docs":{},"，":{"docs":{},"输":{"docs":{},"入":{"docs":{},"句":{"docs":{},"子":{"docs":{},"数":{"docs":{},"量":{"docs":{},"为":{"2":{"5":{"6":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"，":{"docs":{},"每":{"docs":{},"个":{"docs":{},"e":{"docs":{},"p":{"docs":{},"o":{"docs":{},"c":{"docs":{},"h":{"docs":{},"迭":{"docs":{},"代":{"1":{"docs":{},",":{"0":{"0":{"0":{"docs":{},",":{"0":{"0":{"0":{"docs":{},"次":{"docs":{},"，":{"docs":{},"最":{"docs":{},"终":{"docs":{},"没":{"3":{"docs":{},".":{"3":{"docs":{},"亿":{"docs":{},"个":{"docs":{},"单":{"docs":{},"词":{"docs":{},"上":{"docs":{},"训":{"docs":{},"练":{"docs":{},"了":{"docs":{},"差":{"docs":{},"不":{"docs":{},"多":{"4":{"0":{"docs":{},"个":{"docs":{},"e":{"docs":{},"p":{"docs":{},"o":{"docs":{},"c":{"docs":{},"h":{"docs":{},".":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}},"处":{"docs":{},"理":{"docs":{},"和":{"docs":{},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"是":{"docs":{},"密":{"docs":{},"切":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"，":{"docs":{},"其":{"docs":{},"是":{"docs":{},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"研":{"docs":{},"究":{"docs":{},"的":{"docs":{},"重":{"docs":{},"要":{"docs":{},"内":{"docs":{},"容":{"docs":{},"，":{"docs":{},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"研":{"docs":{},"究":{"docs":{},"的":{"docs":{},"两":{"docs":{},"大":{"docs":{},"国":{"docs":{},"际":{"docs":{},"顶":{"docs":{},"会":{"docs":{},"是":{"docs":{},"a":{"docs":{},"a":{"docs":{},"a":{"docs":{},"i":{"docs":{},"和":{"docs":{},"i":{"docs":{},"j":{"docs":{},"c":{"docs":{},"a":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"期":{"docs":{},"刊":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}},"本":{"docs":{},"文":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"（":{"docs":{},"n":{"docs":{},"a":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}},"作":{"docs":{},"者":{"docs":{},"推":{"docs":{},"出":{"docs":{},"了":{"docs":{},"一":{"docs":{},"套":{"docs":{},"新":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"表":{"docs":{},"达":{"docs":{},"模":{"docs":{},"型":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"，":{"docs":{},"全":{"docs":{},"称":{"docs":{},"为":{"docs":{},"b":{"docs":{},"i":{"docs":{},"d":{"docs":{},"i":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.057692307692307696}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"领":{"docs":{},"域":{"docs":{},"中":{"docs":{},"与":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"顶":{"docs":{},"级":{"docs":{},"会":{"docs":{},"议":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}},"每":{"docs":{},"两":{"docs":{},"年":{"docs":{},"一":{"docs":{},"次":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"年":{"docs":{},"一":{"docs":{},"次":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0182648401826484}}}}}},"除":{"docs":{},"了":{"docs":{},"上":{"docs":{},"述":{"docs":{},"被":{"docs":{},"c":{"docs":{},"c":{"docs":{},"f":{"docs":{},"收":{"docs":{},"录":{"docs":{},"的":{"docs":{},"会":{"docs":{},"议":{"docs":{},"外":{"docs":{},"，":{"docs":{},"在":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"领":{"docs":{},"域":{"docs":{},"还":{"docs":{},"有":{"docs":{},"其":{"docs":{},"他":{"docs":{},"许":{"docs":{},"多":{"docs":{},"重":{"docs":{},"要":{"docs":{},"会":{"docs":{},"议":{"docs":{},"，":{"docs":{},"分":{"docs":{},"别":{"docs":{},"是":{"docs":{},"s":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"v":{"docs":{},"a":{"docs":{},"l":{"docs":{},",":{"docs":{},"l":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"等":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"直":{"docs":{},"接":{"docs":{},"与":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"相":{"docs":{},"关":{"docs":{},"外":{"docs":{},"，":{"docs":{},"还":{"docs":{},"有":{"docs":{},"其":{"docs":{},"他":{"docs":{},"许":{"docs":{},"多":{"docs":{},"与":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"学":{"docs":{},"术":{"docs":{},"会":{"docs":{},"议":{"docs":{},"，":{"docs":{},"包":{"docs":{},"括":{"docs":{},"信":{"docs":{},"息":{"docs":{},"检":{"docs":{},"索":{"docs":{},"，":{"docs":{},"数":{"docs":{},"据":{"docs":{},"挖":{"docs":{},"掘":{"docs":{},"及":{"docs":{},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"领":{"docs":{},"域":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"都":{"docs":{},"是":{"docs":{},"属":{"docs":{},"于":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"的":{"docs":{},"应":{"docs":{},"用":{"docs":{},"领":{"docs":{},"域":{"docs":{},"。":{"docs":{},"其":{"docs":{},"中":{"docs":{},"信":{"docs":{},"息":{"docs":{},"检":{"docs":{},"索":{"docs":{},"和":{"docs":{},"数":{"docs":{},"据":{"docs":{},"挖":{"docs":{},"掘":{"docs":{},"与":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"是":{"docs":{},"密":{"docs":{},"切":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"，":{"docs":{},"主":{"docs":{},"要":{"docs":{},"由":{"docs":{},"美":{"docs":{},"国":{"docs":{},"计":{"docs":{},"算":{"docs":{},"机":{"docs":{},"学":{"docs":{},"会":{"docs":{},"（":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"领":{"docs":{},"域":{"docs":{},"最":{"docs":{},"权":{"docs":{},"威":{"docs":{},"的":{"docs":{},"国":{"docs":{},"际":{"docs":{},"会":{"docs":{},"议":{"docs":{},"，":{"docs":{},"即":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"年":{"docs":{},"会":{"docs":{},"。":{"1":{"9":{"8":{"2":{"docs":{},"年":{"docs":{},"和":{"1":{"9":{"9":{"9":{"docs":{},"年":{"docs":{},"，":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"分":{"docs":{},"别":{"docs":{},"成":{"docs":{},"立":{"docs":{},"了":{"docs":{},"欧":{"docs":{},"洲":{"docs":{},"分":{"docs":{},"会":{"docs":{},"（":{"docs":{},"e":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"）":{"docs":{},"和":{"docs":{},"北":{"docs":{},"美":{"docs":{},"分":{"docs":{},"会":{"docs":{},"（":{"docs":{},"n":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"）":{"docs":{},"两":{"docs":{},"个":{"docs":{},"区":{"docs":{},"域":{"docs":{},"性":{"docs":{},"分":{"docs":{},"会":{"docs":{},"。":{"docs":{},"近":{"docs":{},"年":{"docs":{},"来":{"docs":{},"，":{"docs":{},"亚":{"docs":{},"太":{"docs":{},"地":{"docs":{},"区":{"docs":{},"在":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"方":{"docs":{},"面":{"docs":{},"的":{"docs":{},"研":{"docs":{},"究":{"docs":{},"进":{"docs":{},"步":{"docs":{},"显":{"docs":{},"著":{"docs":{},"，":{"2":{"0":{"1":{"8":{"docs":{},"年":{"7":{"docs":{},"月":{"1":{"5":{"docs":{},"日":{"docs":{},"，":{"docs":{},"第":{"5":{"6":{"docs":{},"届":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"年":{"docs":{},"会":{"docs":{},"在":{"docs":{},"澳":{"docs":{},"大":{"docs":{},"利":{"docs":{},"亚":{"docs":{},"墨":{"docs":{},"尔":{"docs":{},"本":{"docs":{},"举":{"docs":{},"行":{"docs":{},"。":{"docs":{},"开":{"docs":{},"幕":{"docs":{},"仪":{"docs":{},"式":{"docs":{},"上":{"docs":{},"，":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"主":{"docs":{},"席":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}},"下":{"docs":{},"载":{"docs":{},"地":{"docs":{},"址":{"docs":{},"：":{"docs":{},"链":{"docs":{},"接":{"docs":{},"：":{"docs":{},"h":{"docs":{},"t":{"docs":{},"t":{"docs":{},"p":{"docs":{},"s":{"docs":{},":":{"docs":{},"/":{"docs":{},"/":{"docs":{},"p":{"docs":{},"a":{"docs":{},"n":{"docs":{},".":{"docs":{},"b":{"docs":{},"a":{"docs":{},"i":{"docs":{},"d":{"docs":{},"u":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"/":{"docs":{},"s":{"docs":{},"/":{"1":{"docs":{},"n":{"docs":{},"m":{"docs":{},"h":{"3":{"3":{"docs":{},"y":{"docs":{},"k":{"8":{"0":{"docs":{},"s":{"docs":{},"z":{"docs":{},"n":{"docs":{},"k":{"docs":{},"i":{"8":{"docs":{},"v":{"docs":{},"h":{"docs":{},"m":{"docs":{},"e":{"docs":{},"e":{"docs":{},"e":{"docs":{},"a":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}},"docs":{}}}}}}},"docs":{}},"docs":{}}}},"docs":{}},"docs":{}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"密":{"docs":{},"码":{"docs":{},":":{"docs":{},"t":{"docs":{},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"d":{"docs":{},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}}}}},"时":{"docs":{},"间":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}},"：":{"2":{"0":{"1":{"9":{"docs":{},"年":{"3":{"docs":{},"月":{"2":{"7":{"docs":{},"日":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}},"8":{"docs":{},"日":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}},"概":{"docs":{},"述":{"docs":{},"：":{"docs":{},"该":{"docs":{},"p":{"docs":{},"p":{"docs":{},"t":{"docs":{},"是":{"docs":{},"做":{"docs":{},"汇":{"docs":{},"报":{"docs":{},"时":{"docs":{},"候":{"docs":{},"的":{"docs":{},"p":{"docs":{},"p":{"docs":{},"t":{"docs":{},"，":{"docs":{},"讲":{"docs":{},"了":{"docs":{},"知":{"docs":{},"识":{"docs":{},"图":{"docs":{},"谱":{"docs":{},"的":{"docs":{},"基":{"docs":{},"本":{"docs":{},"知":{"docs":{},"识":{"docs":{},"及":{"docs":{},"实":{"docs":{},"体":{"docs":{},"和":{"docs":{},"关":{"docs":{},"系":{"docs":{},"抽":{"docs":{},"取":{"docs":{},"相":{"docs":{},"关":{"docs":{},"论":{"docs":{},"文":{"docs":{},"，":{"docs":{},"有":{"docs":{},"需":{"docs":{},"要":{"docs":{},"可":{"docs":{},"以":{"docs":{},"下":{"docs":{},"载":{"docs":{},"。":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"（":{"docs":{},"永":{"docs":{},"久":{"docs":{},"有":{"docs":{},"效":{"docs":{},"）":{"docs":{},"提":{"docs":{},"取":{"docs":{},"码":{"docs":{},"：":{"docs":{},"m":{"docs":{},"d":{"docs":{},"a":{"docs":{},"n":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}}}}}}}},"v":{"1":{"docs":{},".":{"1":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},"的":{"docs":{},"问":{"docs":{},"答":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"其":{"docs":{},"f":{"1":{"docs":{},"值":{"docs":{},"达":{"docs":{},"到":{"docs":{},"了":{"9":{"3":{"docs":{},".":{"2":{"docs":{},"（":{"1":{"docs":{},".":{"5":{"docs":{},"%":{"docs":{},"）":{"docs":{},"的":{"docs":{},"绝":{"docs":{},"对":{"docs":{},"提":{"docs":{},"升":{"docs":{},"，":{"docs":{},"比":{"docs":{},"人":{"docs":{},"类":{"docs":{},"的":{"docs":{},"表":{"docs":{},"现":{"docs":{},"都":{"docs":{},"搞":{"docs":{},"了":{"2":{"docs":{},".":{"0":{"docs":{},"的":{"docs":{},"提":{"docs":{},"升":{"docs":{},"。":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.057692307692307696},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}}}}}},"docs":{}}}}}}}}}},"docs":{}}},"docs":{}},"目":{"docs":{},"录":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.019230769230769232}}},"前":{"docs":{},"在":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"领":{"docs":{},"域":{"docs":{},"可":{"docs":{},"以":{"docs":{},"用":{"docs":{},"于":{"docs":{},"和":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"进":{"docs":{},"行":{"docs":{},"比":{"docs":{},"较":{"docs":{},"的":{"docs":{},"是":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{},"g":{"docs":{},"p":{"docs":{},"t":{"docs":{},",":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{},"g":{"docs":{},"p":{"docs":{},"t":{"docs":{},"使":{"docs":{},"用":{"docs":{},"从":{"docs":{},"左":{"docs":{},"到":{"docs":{},"右":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"在":{"docs":{},"大":{"docs":{},"量":{"docs":{},"语":{"docs":{},"料":{"docs":{},"上":{"docs":{},"进":{"docs":{},"行":{"docs":{},"训":{"docs":{},"练":{"docs":{},"。":{"docs":{},"在":{"docs":{},"整":{"docs":{},"体":{"docs":{},"结":{"docs":{},"构":{"docs":{},"上":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"和":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{},"g":{"docs":{},"p":{"docs":{},"t":{"docs":{},"是":{"docs":{},"相":{"docs":{},"似":{"docs":{},"的":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{},"整":{"docs":{},"体":{"docs":{},"结":{"docs":{},"构":{"docs":{},"差":{"docs":{},"异":{"docs":{},"较":{"docs":{},"小":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"和":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{},"g":{"docs":{},"p":{"docs":{},"t":{"docs":{},"还":{"docs":{},"有":{"docs":{},"其":{"docs":{},"他":{"docs":{},"几":{"docs":{},"个":{"docs":{},"方":{"docs":{},"面":{"docs":{},"的":{"docs":{},"差":{"docs":{},"别":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"有":{"docs":{},"两":{"docs":{},"种":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"被":{"docs":{},"用":{"docs":{},"于":{"docs":{},"下":{"docs":{},"游":{"docs":{},"处":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"：":{"docs":{},"基":{"docs":{},"于":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"和":{"docs":{},"基":{"docs":{},"于":{"docs":{},"微":{"docs":{},"调":{"docs":{},"（":{"docs":{},"f":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"研":{"docs":{},"究":{"docs":{},"性":{"docs":{},"工":{"docs":{},"作":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.057692307692307696}}}}}}},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"，":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.057692307692307696},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}}}}},"任":{"docs":{},"务":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"流":{"docs":{},"程":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}},"#":{"docs":{},"#":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"*":{"docs":{},"a":{"docs":{},"*":{"docs":{},":":{"docs":{},"表":{"docs":{},"示":{"docs":{},"注":{"docs":{},"意":{"docs":{},"力":{"docs":{},"h":{"docs":{},"e":{"docs":{},"a":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{},"的":{"docs":{},"数":{"docs":{},"量":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}},",":{"docs":{},"w":{"docs":{},"i":{"docs":{},"n":{"docs":{},"o":{"docs":{},"g":{"docs":{},"r":{"docs":{},"a":{"docs":{},"d":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}},".":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"o":{"docs":{},"p":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"a":{"docs":{},"l":{"docs":{},")":{"docs":{},"，":{"docs":{},"而":{"docs":{},"在":{"docs":{},"针":{"docs":{},"对":{"docs":{},"一":{"docs":{},"些":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"如":{"docs":{},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"d":{"docs":{},"的":{"docs":{},"问":{"docs":{},"答":{"docs":{},"问":{"docs":{},"题":{"docs":{},"中":{"docs":{},"，":{"docs":{},"则":{"docs":{},"这":{"docs":{},"种":{"docs":{},"方":{"docs":{},"法":{"docs":{},"有":{"docs":{},"可":{"docs":{},"能":{"docs":{},"是":{"docs":{},"毁":{"docs":{},"灭":{"docs":{},"性":{"docs":{},"的":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{},"在":{"docs":{},"这":{"docs":{},"些":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"纳":{"docs":{},"入":{"docs":{},"两":{"docs":{},"个":{"docs":{},"方":{"docs":{},"面":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"是":{"docs":{},"至":{"docs":{},"关":{"docs":{},"重":{"docs":{},"要":{"docs":{},"的":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"q":{"docs":{},"n":{"docs":{},"l":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}},"q":{"docs":{},"p":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}},"问":{"docs":{},"题":{"docs":{},"对":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"二":{"docs":{},"进":{"docs":{},"制":{"docs":{},"分":{"docs":{},"类":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"用":{"docs":{},"于":{"docs":{},"确":{"docs":{},"定":{"docs":{},"连":{"docs":{},"个":{"docs":{},"问":{"docs":{},"题":{"docs":{},"的":{"docs":{},"语":{"docs":{},"义":{"docs":{},"是":{"docs":{},"否":{"docs":{},"是":{"docs":{},"相":{"docs":{},"等":{"docs":{},"的":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"​":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.17123287671232876}}},"不":{"docs":{},"像":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"从":{"docs":{},"左":{"docs":{},"到":{"docs":{},"右":{"docs":{},"或":{"docs":{},"者":{"docs":{},"从":{"docs":{},"右":{"docs":{},"到":{"docs":{},"左":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"在":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"中":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"两":{"docs":{},"种":{"docs":{},"原":{"docs":{},"创":{"docs":{},"的":{"docs":{},"无":{"docs":{},"监":{"docs":{},"督":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"分":{"docs":{},"别":{"docs":{},"是":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"是":{"docs":{},"所":{"docs":{},"有":{"docs":{},"被":{"docs":{},"选":{"docs":{},"择":{"docs":{},"的":{"docs":{},"单":{"docs":{},"词":{"docs":{},"都":{"docs":{},"会":{"docs":{},"被":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"，":{"docs":{},"而":{"docs":{},"是":{"docs":{},"其":{"docs":{},"中":{"8":{"0":{"docs":{},"%":{"docs":{},"的":{"docs":{},"可":{"docs":{},"能":{"docs":{},"会":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"该":{"docs":{},"单":{"docs":{},"词":{"docs":{},"，":{"1":{"0":{"docs":{},"%":{"docs":{},"的":{"docs":{},"时":{"docs":{},"间":{"docs":{},"将":{"docs":{},"该":{"docs":{},"单":{"docs":{},"词":{"docs":{},"替":{"docs":{},"换":{"docs":{},"成":{"docs":{},"一":{"docs":{},"个":{"docs":{},"随":{"docs":{},"机":{"docs":{},"的":{"docs":{},"单":{"docs":{},"词":{"docs":{},"。":{"docs":{},"而":{"docs":{},"另":{"docs":{},"外":{"docs":{},"还":{"docs":{},"是":{"docs":{},"有":{"1":{"0":{"docs":{},"的":{"docs":{},"时":{"docs":{},"间":{"docs":{},"是":{"docs":{},"该":{"docs":{},"单":{"docs":{},"词":{"docs":{},"保":{"docs":{},"持":{"docs":{},"不":{"docs":{},"变":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}},"为":{"3":{"2":{"docs":{},"，":{"docs":{},"训":{"docs":{},"练":{"docs":{},"迭":{"docs":{},"代":{"docs":{},"次":{"docs":{},"数":{"docs":{},"为":{"3":{"docs":{},"个":{"docs":{},"e":{"docs":{},"p":{"docs":{},"o":{"docs":{},"c":{"docs":{},"h":{"docs":{},".":{"docs":{},"实":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{},"如":{"docs":{},"下":{"docs":{},"：":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}},"docs":{}},"docs":{},"了":{"docs":{},"能":{"docs":{},"够":{"docs":{},"训":{"docs":{},"练":{"docs":{},"一":{"docs":{},"个":{"docs":{},"具":{"docs":{},"有":{"docs":{},"双":{"docs":{},"向":{"docs":{},"深":{"docs":{},"层":{"docs":{},"的":{"docs":{},"表":{"docs":{},"示":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"在":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"中":{"docs":{},"采":{"docs":{},"用":{"docs":{},"了":{"docs":{},"一":{"docs":{},"种":{"docs":{},"较":{"docs":{},"为":{"docs":{},"直":{"docs":{},"观":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"通":{"docs":{},"过":{"docs":{},"随":{"docs":{},"机":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"句":{"docs":{},"子":{"docs":{},"中":{"docs":{},"的":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"，":{"docs":{},"并":{"docs":{},"让":{"docs":{},"模":{"docs":{},"型":{"docs":{},"能":{"docs":{},"对":{"docs":{},"该":{"docs":{},"句":{"docs":{},"子":{"docs":{},"进":{"docs":{},"行":{"docs":{},"正":{"docs":{},"确":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"。":{"docs":{},"这":{"docs":{},"种":{"docs":{},"模":{"docs":{},"型":{"docs":{},"在":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"中":{"docs":{},"被":{"docs":{},"称":{"docs":{},"为":{"docs":{},"\"":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"评":{"docs":{},"估":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"在":{"docs":{},"序":{"docs":{},"列":{"docs":{},"标":{"docs":{},"注":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{},"，":{"docs":{},"论":{"docs":{},"文":{"docs":{},"还":{"docs":{},"基":{"docs":{},"于":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"在":{"docs":{},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"l":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"从":{"docs":{},"初":{"docs":{},"始":{"docs":{},"概":{"docs":{},"念":{"docs":{},"上":{"docs":{},"来":{"docs":{},"将":{"docs":{},"，":{"docs":{},"一":{"docs":{},"个":{"docs":{},"更":{"docs":{},"深":{"docs":{},"层":{"docs":{},"次":{"docs":{},"的":{"docs":{},"双":{"docs":{},"向":{"docs":{},"模":{"docs":{},"型":{"docs":{},"比":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"从":{"docs":{},"左":{"docs":{},"到":{"docs":{},"有":{"docs":{},"或":{"docs":{},"者":{"docs":{},"从":{"docs":{},"右":{"docs":{},"到":{"docs":{},"左":{"docs":{},"基":{"docs":{},"于":{"docs":{},"简":{"docs":{},"单":{"docs":{},"讲":{"docs":{},"从":{"docs":{},"左":{"docs":{},"到":{"docs":{},"右":{"docs":{},"或":{"docs":{},"者":{"docs":{},"从":{"docs":{},"右":{"docs":{},"到":{"docs":{},"左":{"docs":{},"模":{"docs":{},"型":{"docs":{},"进":{"docs":{},"行":{"docs":{},"组":{"docs":{},"合":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"有":{"docs":{},"更":{"docs":{},"好":{"docs":{},"的":{"docs":{},"效":{"docs":{},"果":{"docs":{},"。":{"docs":{},"然":{"docs":{},"后":{"docs":{},"，":{"docs":{},"目":{"docs":{},"前":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"则":{"docs":{},"往":{"docs":{},"往":{"docs":{},"只":{"docs":{},"能":{"docs":{},"通":{"docs":{},"过":{"docs":{},"从":{"docs":{},"左":{"docs":{},"到":{"docs":{},"右":{"docs":{},"或":{"docs":{},"者":{"docs":{},"从":{"docs":{},"右":{"docs":{},"到":{"docs":{},"左":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"进":{"docs":{},"行":{"docs":{},"训":{"docs":{},"练":{"docs":{},"。":{"docs":{},"在":{"docs":{},"双":{"docs":{},"向":{"docs":{},"的":{"docs":{},"环":{"docs":{},"境":{"docs":{},"中":{"docs":{},"，":{"docs":{},"句":{"docs":{},"子":{"docs":{},"中":{"docs":{},"任":{"docs":{},"意":{"docs":{},"一":{"docs":{},"个":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"都":{"docs":{},"有":{"docs":{},"可":{"docs":{},"能":{"docs":{},"被":{"docs":{},"预":{"docs":{},"测":{"docs":{},"到":{"docs":{},"在":{"docs":{},"一":{"docs":{},"个":{"docs":{},"多":{"docs":{},"层":{"docs":{},"的":{"docs":{},"网":{"docs":{},"络":{"docs":{},"环":{"docs":{},"境":{"docs":{},"中":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"实":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{},"中":{"docs":{},"可":{"docs":{},"以":{"docs":{},"看":{"docs":{},"到":{"docs":{},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"去":{"docs":{},"了":{"docs":{},"的":{"docs":{},"最":{"docs":{},"好":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"其":{"docs":{},"中":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"看":{"docs":{},"出":{"docs":{},"该":{"docs":{},"模":{"docs":{},"型":{"docs":{},"可":{"docs":{},"以":{"docs":{},"取":{"docs":{},"得":{"docs":{},"较":{"docs":{},"好":{"docs":{},"的":{"docs":{},"实":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{},"。":{"docs":{},"此":{"docs":{},"外":{"docs":{},"作":{"docs":{},"者":{"docs":{},"还":{"docs":{},"对":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"一":{"docs":{},"些":{"docs":{},"相":{"docs":{},"关":{"docs":{},"参":{"docs":{},"数":{"docs":{},"进":{"docs":{},"行":{"docs":{},"了":{"docs":{},"分":{"docs":{},"析":{"docs":{},"并":{"docs":{},"进":{"docs":{},"行":{"docs":{},"了":{"docs":{},"实":{"docs":{},"验":{"docs":{},"讨":{"docs":{},"论":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"可":{"docs":{},"以":{"docs":{},"看":{"docs":{},"出":{"docs":{},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"也":{"docs":{},"去":{"docs":{},"了":{"docs":{},"的":{"docs":{},"s":{"docs":{},"o":{"docs":{},"t":{"docs":{},"a":{"docs":{},"的":{"docs":{},"实":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"相":{"docs":{},"比":{"docs":{},"较":{"docs":{},"于":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}},"作":{"docs":{},"者":{"docs":{},"信":{"docs":{},"息":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}},"单":{"docs":{},"位":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}}}},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"包":{"docs":{},"含":{"3":{"0":{"docs":{},",":{"0":{"0":{"0":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"的":{"docs":{},"词":{"docs":{},"库":{"docs":{},"的":{"docs":{},"w":{"docs":{},"o":{"docs":{},"r":{"docs":{},"d":{"docs":{},"p":{"docs":{},"i":{"docs":{},"e":{"docs":{},"c":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}}}}}},"关":{"docs":{},"键":{"docs":{},"词":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}}},"其":{"docs":{},"他":{"docs":{},"解":{"docs":{},"读":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}}}},"内":{"docs":{},"容":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}},"则":{"docs":{},"分":{"docs":{},"类":{"docs":{},"结":{"docs":{},"果":{"docs":{},"接":{"docs":{},"上":{"docs":{},"一":{"docs":{},"个":{"docs":{},"s":{"docs":{},"o":{"docs":{},"f":{"docs":{},"t":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"层":{"docs":{},"后":{"docs":{},"得":{"docs":{},"到":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"各":{"docs":{},"个":{"docs":{},"类":{"docs":{},"别":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"p":{"docs":{},"=":{"docs":{},"s":{"docs":{},"o":{"docs":{},"f":{"docs":{},"t":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"(":{"docs":{},"c":{"docs":{},"w":{"docs":{},"t":{"docs":{},")":{"docs":{},"。":{"docs":{},"这":{"docs":{},"样":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"和":{"docs":{},"新":{"docs":{},"增":{"docs":{},"的":{"docs":{},"w":{"docs":{},"参":{"docs":{},"数":{"docs":{},"一":{"docs":{},"起":{"docs":{},"经":{"docs":{},"过":{"docs":{},"微":{"docs":{},"调":{"docs":{},"后":{"docs":{},"得":{"docs":{},"到":{"docs":{},"分":{"docs":{},"类":{"docs":{},"结":{"docs":{},"果":{"docs":{},"的":{"docs":{},"输":{"docs":{},"出":{"docs":{},"。":{"docs":{},"而":{"docs":{},"在":{"docs":{},"针":{"docs":{},"对":{"docs":{},"单":{"docs":{},"词":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"训":{"docs":{},"练":{"docs":{},"过":{"docs":{},"程":{"docs":{},"和":{"docs":{},"句":{"docs":{},"子":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"有":{"docs":{},"所":{"docs":{},"区":{"docs":{},"别":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"区":{"docs":{},"别":{"docs":{},"于":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"a":{"docs":{},"u":{"docs":{},"t":{"docs":{},"o":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}},"单":{"docs":{},"词":{"docs":{},")":{"docs":{},"。":{"docs":{},"在":{"docs":{},"维":{"docs":{},"基":{"docs":{},"百":{"docs":{},"科":{"docs":{},"语":{"docs":{},"料":{"docs":{},"中":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"目":{"docs":{},"前":{"docs":{},"只":{"docs":{},"提":{"docs":{},"取":{"docs":{},"了":{"docs":{},"文":{"docs":{},"本":{"docs":{},"消":{"docs":{},"息":{"docs":{},"而":{"docs":{},"忽":{"docs":{},"略":{"docs":{},"了":{"docs":{},"列":{"docs":{},"表":{"docs":{},"，":{"docs":{},"表":{"docs":{},"格":{"docs":{},"以":{"docs":{},"及":{"docs":{},"头":{"docs":{},"等":{"docs":{},"信":{"docs":{},"息":{"docs":{},"。":{"docs":{},"在":{"docs":{},"实":{"docs":{},"际":{"docs":{},"训":{"docs":{},"练":{"docs":{},"中":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"文":{"docs":{},"本":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"语":{"docs":{},"料":{"docs":{},"比":{"docs":{},"句":{"docs":{},"子":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"语":{"docs":{},"料":{"docs":{},"可":{"docs":{},"以":{"docs":{},"取":{"docs":{},"得":{"docs":{},"更":{"docs":{},"好":{"docs":{},"的":{"docs":{},"实":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{},"。":{"docs":{},"为":{"docs":{},"了":{"docs":{},"生":{"docs":{},"存":{"docs":{},"训":{"docs":{},"练":{"docs":{},"输":{"docs":{},"入":{"docs":{},"序":{"docs":{},"列":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"从":{"docs":{},"语":{"docs":{},"料":{"docs":{},"库":{"docs":{},"中":{"docs":{},"选":{"docs":{},"择":{"docs":{},"了":{"docs":{},"两":{"docs":{},"个":{"docs":{},"文":{"docs":{},"本":{"docs":{},"句":{"docs":{},"子":{"docs":{},"并":{"docs":{},"将":{"docs":{},"其":{"docs":{},"合":{"docs":{},"并":{"docs":{},"成":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"。":{"docs":{},"其":{"docs":{},"中":{"docs":{},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"使":{"docs":{},"用":{"docs":{},"句":{"docs":{},"子":{"docs":{},"a":{"docs":{},"的":{"docs":{},"e":{"docs":{},"m":{"docs":{},"b":{"docs":{},"e":{"docs":{},"d":{"docs":{},"d":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"，":{"docs":{},"而":{"docs":{},"第":{"docs":{},"二":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"使":{"docs":{},"用":{"docs":{},"句":{"docs":{},"子":{"docs":{},"b":{"docs":{},"的":{"docs":{},"e":{"docs":{},"m":{"docs":{},"b":{"docs":{},"e":{"docs":{},"d":{"docs":{},"d":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"。":{"docs":{},"在":{"docs":{},"生":{"docs":{},"成":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"句":{"docs":{},"子":{"docs":{},"b":{"docs":{},"的":{"docs":{},"e":{"docs":{},"m":{"docs":{},"b":{"docs":{},"e":{"docs":{},"d":{"docs":{},"d":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"有":{"5":{"0":{"docs":{},"%":{"docs":{},"的":{"docs":{},"可":{"docs":{},"能":{"docs":{},"是":{"docs":{},"随":{"docs":{},"机":{"docs":{},"选":{"docs":{},"择":{"docs":{},"的":{"docs":{},"，":{"docs":{},"用":{"docs":{},"于":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"型":{"docs":{},"对":{"docs":{},"下":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"。":{"docs":{},"两":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"合":{"docs":{},"并":{"docs":{},"起":{"docs":{},"来":{"docs":{},"的":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"序":{"docs":{},"列":{"docs":{},"长":{"docs":{},"度":{"docs":{},"小":{"docs":{},"于":{"5":{"1":{"2":{"docs":{},"个":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"。":{"docs":{},"句":{"docs":{},"子":{"docs":{},"首":{"docs":{},"先":{"docs":{},"使":{"docs":{},"用":{"docs":{},"w":{"docs":{},"o":{"docs":{},"r":{"docs":{},"d":{"docs":{},"p":{"docs":{},"i":{"docs":{},"e":{"docs":{},"c":{"docs":{},"e":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"生":{"docs":{},"成":{"docs":{},"输":{"docs":{},"入":{"docs":{},"表":{"docs":{},"示":{"docs":{},"并":{"docs":{},"在":{"docs":{},"其":{"docs":{},"中":{"docs":{},"随":{"docs":{},"机":{"docs":{},"挑":{"docs":{},"选":{"1":{"5":{"docs":{},"%":{"docs":{},"来":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"，":{"docs":{},"然":{"docs":{},"后":{"docs":{},"在":{"docs":{},"此":{"docs":{},"基":{"docs":{},"础":{"docs":{},"上":{"docs":{},"使":{"docs":{},"用":{"docs":{},"l":{"docs":{},"m":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"可":{"docs":{},"能":{"docs":{},"会":{"docs":{},"被":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{},"在":{"docs":{},"训":{"docs":{},"练":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"其":{"docs":{},"会":{"docs":{},"花":{"docs":{},"费":{"docs":{},"较":{"docs":{},"多":{"docs":{},"的":{"docs":{},"时":{"docs":{},"间":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"原":{"docs":{},"文":{"docs":{},"链":{"docs":{},"接":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}}}},"命":{"docs":{},"名":{"docs":{},"实":{"docs":{},"体":{"docs":{},"识":{"docs":{},"别":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}},"和":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{},"g":{"docs":{},"p":{"docs":{},"t":{"docs":{},"的":{"docs":{},"比":{"docs":{},"较":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}},"基":{"docs":{},"于":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"词":{"docs":{},"表":{"docs":{},"示":{"docs":{},"方":{"docs":{},"法":{"docs":{},"在":{"docs":{},"多":{"docs":{},"个":{"docs":{},"领":{"docs":{},"域":{"docs":{},"都":{"docs":{},"有":{"docs":{},"广":{"docs":{},"泛":{"docs":{},"的":{"docs":{},"应":{"docs":{},"用":{"docs":{},"。":{"docs":{},"其":{"docs":{},"中":{"docs":{},"包":{"docs":{},"括":{"docs":{},"非":{"docs":{},"神":{"docs":{},"经":{"docs":{},"元":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"和":{"docs":{},"基":{"docs":{},"于":{"docs":{},"神":{"docs":{},"经":{"docs":{},"网":{"docs":{},"络":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"。":{"docs":{},"基":{"docs":{},"于":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"表":{"docs":{},"示":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"方":{"docs":{},"法":{"docs":{},"目":{"docs":{},"前":{"docs":{},"是":{"docs":{},"现":{"docs":{},"代":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"系":{"docs":{},"统":{"docs":{},"中":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"重":{"docs":{},"要":{"docs":{},"集":{"docs":{},"成":{"docs":{},"部":{"docs":{},"分":{"docs":{},"，":{"docs":{},"其":{"docs":{},"对":{"docs":{},"系":{"docs":{},"统":{"docs":{},"的":{"docs":{},"后":{"docs":{},"续":{"docs":{},"处":{"docs":{},"理":{"docs":{},"提":{"docs":{},"供":{"docs":{},"了":{"docs":{},"有":{"docs":{},"强":{"docs":{},"大":{"docs":{},"的":{"docs":{},"提":{"docs":{},"升":{"docs":{},"。":{"docs":{},"除":{"docs":{},"了":{"docs":{},"词":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"外":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"方":{"docs":{},"法":{"docs":{},"也":{"docs":{},"被":{"docs":{},"推":{"docs":{},"广":{"docs":{},"到":{"docs":{},"粗":{"docs":{},"粒":{"docs":{},"度":{"docs":{},"的":{"docs":{},"句":{"docs":{},"子":{"docs":{},"级":{"docs":{},"别":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"以":{"docs":{},"及":{"docs":{},"锻":{"docs":{},"炼":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"中":{"docs":{},"。":{"docs":{},"在":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"中":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"学":{"docs":{},"习":{"docs":{},"到":{"docs":{},"的":{"docs":{},"表":{"docs":{},"示":{"docs":{},"方":{"docs":{},"法":{"docs":{},"往":{"docs":{},"往":{"docs":{},"作":{"docs":{},"为":{"docs":{},"下":{"docs":{},"游":{"docs":{},"任":{"docs":{},"务":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"输":{"docs":{},"入":{"docs":{},"到":{"docs":{},"下":{"docs":{},"游":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"。":{"docs":{},"e":{"docs":{},"l":{"docs":{},"m":{"docs":{},"o":{"docs":{},"(":{"docs":{},"p":{"docs":{},"e":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"微":{"docs":{},"调":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}},"监":{"docs":{},"督":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{},"方":{"docs":{},"法":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}},"多":{"docs":{},"类":{"docs":{},"型":{"docs":{},"的":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"推":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}},"如":{"docs":{},"果":{"docs":{},"输":{"docs":{},"入":{"docs":{},"是":{"docs":{},"单":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"，":{"docs":{},"则":{"docs":{},"直":{"docs":{},"接":{"docs":{},"使":{"docs":{},"用":{"docs":{},"句":{"docs":{},"子":{"docs":{},"a":{"docs":{},"的":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"表":{"docs":{},"示":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}},"实":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{},"如":{"docs":{},"图":{"docs":{},"所":{"docs":{},"示":{"docs":{},"：":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}},"部":{"docs":{},"分":{"docs":{},"主":{"docs":{},"要":{"docs":{},"叙":{"docs":{},"述":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"在":{"1":{"1":{"docs":{},"项":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"预":{"docs":{},"处":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}},"对":{"docs":{},"于":{"docs":{},"句":{"docs":{},"子":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"分":{"docs":{},"类":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"的":{"docs":{},"微":{"docs":{},"调":{"docs":{},"是":{"docs":{},"很":{"docs":{},"直":{"docs":{},"接":{"docs":{},"的":{"docs":{},"。":{"docs":{},"为":{"docs":{},"了":{"docs":{},"获":{"docs":{},"取":{"docs":{},"输":{"docs":{},"入":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"固":{"docs":{},"定":{"docs":{},"维":{"docs":{},"度":{"docs":{},"的":{"docs":{},"表":{"docs":{},"示":{"docs":{},",":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"输":{"docs":{},"入":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"词":{"docs":{},"的":{"docs":{},"最":{"docs":{},"后":{"docs":{},"一":{"docs":{},"个":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"层":{"docs":{},"的":{"docs":{},"输":{"docs":{},"入":{"docs":{},"c":{"docs":{},"（":{"docs":{},"r":{"docs":{},"h":{"docs":{},"）":{"docs":{},"。":{"docs":{},"在":{"docs":{},"进":{"docs":{},"行":{"docs":{},"分":{"docs":{},"类":{"docs":{},"时":{"docs":{},"，":{"docs":{},"在":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"输":{"docs":{},"出":{"docs":{},"的":{"docs":{},"基":{"docs":{},"础":{"docs":{},"上":{"docs":{},"在":{"docs":{},"增":{"docs":{},"加":{"docs":{},"一":{"docs":{},"个":{"docs":{},"分":{"docs":{},"类":{"docs":{},"层":{"docs":{},"，":{"docs":{},"分":{"docs":{},"类":{"docs":{},"层":{"docs":{},"的":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"表":{"docs":{},"示":{"docs":{},"为":{"docs":{},"w":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"常":{"docs":{},"识":{"docs":{},"推":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"（":{"docs":{},"s":{"docs":{},"w":{"docs":{},"a":{"docs":{},"g":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}},"微":{"docs":{},"调":{"docs":{},"过":{"docs":{},"程":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}},"总":{"docs":{},"结":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"我":{"docs":{},"们":{"docs":{},"使":{"docs":{},"用":{"docs":{},"饿":{"docs":{},"了":{"docs":{},"a":{"docs":{},"d":{"docs":{},"a":{"docs":{},"m":{"docs":{},"优":{"docs":{},"化":{"docs":{},"算":{"docs":{},"法":{"docs":{},"，":{"docs":{},"其":{"docs":{},"中":{"docs":{},"学":{"docs":{},"习":{"docs":{},"率":{"docs":{},"为":{"1":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}},"docs":{}}}}}}}}}}}}}}}}}}}},"展":{"docs":{},"示":{"docs":{},"了":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"块":{"docs":{},"可":{"docs":{},"以":{"docs":{},"消":{"docs":{},"除":{"docs":{},"在":{"docs":{},"很":{"docs":{},"多":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"需":{"docs":{},"要":{"docs":{},"依":{"docs":{},"赖":{"docs":{},"严":{"docs":{},"重":{"docs":{},"特":{"docs":{},"征":{"docs":{},"工":{"docs":{},"程":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"。":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"摘":{"docs":{},"要":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}},"无":{"docs":{},"监":{"docs":{},"督":{"docs":{},"学":{"docs":{},"习":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"方":{"docs":{},"法":{"docs":{},"的":{"docs":{},"优":{"docs":{},"势":{"docs":{},"是":{"docs":{},"有":{"docs":{},"大":{"docs":{},"量":{"docs":{},"的":{"docs":{},"无":{"docs":{},"标":{"docs":{},"签":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"可":{"docs":{},"以":{"docs":{},"大":{"docs":{},"量":{"docs":{},"的":{"docs":{},"获":{"docs":{},"取":{"docs":{},"，":{"docs":{},"基":{"docs":{},"于":{"docs":{},"有":{"docs":{},"标":{"docs":{},"签":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{},"目":{"docs":{},"前":{"docs":{},"也":{"docs":{},"被":{"docs":{},"证":{"docs":{},"明":{"docs":{},"在":{"docs":{},"许":{"docs":{},"多":{"docs":{},"文":{"docs":{},"本":{"docs":{},"处":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"有":{"docs":{},"较":{"docs":{},"好":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"例":{"docs":{},"如":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"推":{"docs":{},"理":{"docs":{},"、":{"docs":{},"机":{"docs":{},"器":{"docs":{},"翻":{"docs":{},"译":{"docs":{},"等":{"docs":{},"任":{"docs":{},"务":{"docs":{},"。":{"docs":{},"除":{"docs":{},"了":{"docs":{},"在":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},"处":{"docs":{},"理":{"docs":{},"领":{"docs":{},"域":{"docs":{},"外":{"docs":{},"，":{"docs":{},"很":{"docs":{},"多":{"docs":{},"机":{"docs":{},"器":{"docs":{},"视":{"docs":{},"觉":{"docs":{},"领":{"docs":{},"域":{"docs":{},"的":{"docs":{},"研":{"docs":{},"究":{"docs":{},"也":{"docs":{},"说":{"docs":{},"明":{"docs":{},"了":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"并":{"docs":{},"集":{"docs":{},"合":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"重":{"docs":{},"要":{"docs":{},"方":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"方":{"docs":{},"法":{"docs":{},"通":{"docs":{},"过":{"docs":{},"在":{"docs":{},"大":{"docs":{},"量":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"上":{"docs":{},"进":{"docs":{},"行":{"docs":{},"训":{"docs":{},"练":{"docs":{},"后":{"docs":{},"再":{"docs":{},"通":{"docs":{},"过":{"docs":{},"微":{"docs":{},"调":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"可":{"docs":{},"以":{"docs":{},"取":{"docs":{},"得":{"docs":{},"较":{"docs":{},"好":{"docs":{},"的":{"docs":{},"实":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"有":{"docs":{},"斯":{"docs":{},"坦":{"docs":{},"福":{"docs":{},"构":{"docs":{},"建":{"docs":{},"的":{"docs":{},"二":{"docs":{},"分":{"docs":{},"类":{"docs":{},"任":{"docs":{},"务":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"是":{"docs":{},"从":{"docs":{},"一":{"docs":{},"些":{"docs":{},"电":{"docs":{},"影":{"docs":{},"评":{"docs":{},"论":{"docs":{},"中":{"docs":{},"抽":{"docs":{},"取":{"docs":{},"出":{"docs":{},"来":{"docs":{},"的":{"docs":{},"句":{"docs":{},"子":{"docs":{},"并":{"docs":{},"且":{"docs":{},"包":{"docs":{},"含":{"docs":{},"人":{"docs":{},"工":{"docs":{},"的":{"docs":{},"标":{"docs":{},"注":{"docs":{},"信":{"docs":{},"息":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"推":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"其":{"docs":{},"中":{"docs":{},"的":{"docs":{},"正":{"docs":{},"样":{"docs":{},"本":{"docs":{},"是":{"docs":{},"一":{"docs":{},"些":{"docs":{},"问":{"docs":{},"题":{"docs":{},"，":{"docs":{},"答":{"docs":{},"案":{"docs":{},"对":{"docs":{},"的":{"docs":{},"句":{"docs":{},"子":{"docs":{},"则":{"docs":{},"是":{"docs":{},"来":{"docs":{},"自":{"docs":{},"于":{"docs":{},"同":{"docs":{},"一":{"docs":{},"个":{"docs":{},"段":{"docs":{},"落":{"docs":{},"的":{"docs":{},"句":{"docs":{},"子":{"docs":{},"，":{"docs":{},"其":{"docs":{},"并":{"docs":{},"不":{"docs":{},"形":{"docs":{},"成":{"docs":{},"问":{"docs":{},"题":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"基":{"docs":{},"于":{"docs":{},"微":{"docs":{},"调":{"docs":{},"方":{"docs":{},"法":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"表":{"docs":{},"示":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"并":{"docs":{},"且":{"docs":{},"在":{"docs":{},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"最":{"docs":{},"后":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"状":{"docs":{},"态":{"docs":{},"被":{"docs":{},"用":{"docs":{},"在":{"docs":{},"分":{"docs":{},"类":{"docs":{},"任":{"docs":{},"务":{"docs":{},"的":{"docs":{},"聚":{"docs":{},"合":{"docs":{},"表":{"docs":{},"示":{"docs":{},"中":{"docs":{},"。":{"docs":{},"而":{"docs":{},"如":{"docs":{},"果":{"docs":{},"针":{"docs":{},"对":{"docs":{},"的":{"docs":{},"是":{"docs":{},"非":{"docs":{},"分":{"docs":{},"类":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"则":{"docs":{},"该":{"docs":{},"词":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"向":{"docs":{},"量":{"docs":{},"被":{"docs":{},"删":{"docs":{},"除":{"docs":{},"掉":{"docs":{},"。":{"docs":{},"在":{"docs":{},"句":{"docs":{},"子":{"docs":{},"对":{"docs":{},"中":{"docs":{},"，":{"docs":{},"所":{"docs":{},"有":{"docs":{},"句":{"docs":{},"子":{"docs":{},"都":{"docs":{},"被":{"docs":{},"表":{"docs":{},"示":{"docs":{},"到":{"docs":{},"一":{"docs":{},"个":{"docs":{},"序":{"docs":{},"列":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"通":{"docs":{},"过":{"docs":{},"两":{"docs":{},"种":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"区":{"docs":{},"分":{"docs":{},"。":{"docs":{},"首":{"docs":{},"先":{"docs":{},"通":{"docs":{},"过":{"docs":{},"一":{"docs":{},"个":{"docs":{},"特":{"docs":{},"殊":{"docs":{},"的":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"[":{"docs":{},"e":{"docs":{},"s":{"docs":{},"p":{"docs":{},"]":{"docs":{},"来":{"docs":{},"对":{"docs":{},"两":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"进":{"docs":{},"行":{"docs":{},"区":{"docs":{},"分":{"docs":{},"，":{"docs":{},"在":{"docs":{},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"中":{"docs":{},"，":{"docs":{},"每":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"会":{"docs":{},"加":{"docs":{},"上":{"docs":{},"句":{"docs":{},"子":{"docs":{},"a":{"docs":{},"的":{"docs":{},"e":{"docs":{},"m":{"docs":{},"b":{"docs":{},"e":{"docs":{},"d":{"docs":{},"d":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"表":{"docs":{},"示":{"docs":{},"，":{"docs":{},"而":{"docs":{},"在":{"docs":{},"句":{"docs":{},"子":{"docs":{},"b":{"docs":{},"中":{"docs":{},"，":{"docs":{},"每":{"docs":{},"个":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"都":{"docs":{},"会":{"docs":{},"加":{"docs":{},"上":{"docs":{},"句":{"docs":{},"子":{"docs":{},"b":{"docs":{},"的":{"docs":{},"e":{"docs":{},"m":{"docs":{},"b":{"docs":{},"e":{"docs":{},"d":{"docs":{},"d":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"表":{"docs":{},"示":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"近":{"docs":{},"的":{"docs":{},"经":{"docs":{},"验":{"docs":{},"改":{"docs":{},"进":{"docs":{},"表":{"docs":{},"明":{"docs":{},"，":{"docs":{},"由":{"docs":{},"于":{"docs":{},"转":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{},"与":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"已":{"docs":{},"经":{"docs":{},"证":{"docs":{},"明":{"docs":{},"，":{"docs":{},"丰":{"docs":{},"富":{"docs":{},"，":{"docs":{},"无":{"docs":{},"监":{"docs":{},"督":{"docs":{},"的":{"docs":{},"预":{"docs":{},"培":{"docs":{},"训":{"docs":{},"是":{"docs":{},"许":{"docs":{},"多":{"docs":{},"语":{"docs":{},"言":{"docs":{},"理":{"docs":{},"解":{"docs":{},"系":{"docs":{},"统":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"组":{"docs":{},"成":{"docs":{},"部":{"docs":{},"分":{"docs":{},"。":{"docs":{},"特":{"docs":{},"别":{"docs":{},"是":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"结":{"docs":{},"果":{"docs":{},"使":{"docs":{},"得":{"docs":{},"即":{"docs":{},"使":{"docs":{},"是":{"docs":{},"低":{"docs":{},"资":{"docs":{},"源":{"docs":{},"任":{"docs":{},"务":{"docs":{},"也":{"docs":{},"能":{"docs":{},"从":{"docs":{},"非":{"docs":{},"常":{"docs":{},"深":{"docs":{},"的":{"docs":{},"单":{"docs":{},"向":{"docs":{},"架":{"docs":{},"构":{"docs":{},"中":{"docs":{},"受":{"docs":{},"益":{"docs":{},"。":{"docs":{},"我":{"docs":{},"们":{"docs":{},"的":{"docs":{},"主":{"docs":{},"要":{"docs":{},"贡":{"docs":{},"献":{"docs":{},"是":{"docs":{},"将":{"docs":{},"这":{"docs":{},"些":{"docs":{},"发":{"docs":{},"现":{"docs":{},"进":{"docs":{},"一":{"docs":{},"步":{"docs":{},"推":{"docs":{},"广":{"docs":{},"到":{"docs":{},"深":{"docs":{},"层":{"docs":{},"双":{"docs":{},"向":{"docs":{},"体":{"docs":{},"系":{"docs":{},"结":{"docs":{},"构":{"docs":{},"中":{"docs":{},"，":{"docs":{},"使":{"docs":{},"相":{"docs":{},"同":{"docs":{},"的":{"docs":{},"预":{"docs":{},"先":{"docs":{},"培":{"docs":{},"训":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"能":{"docs":{},"够":{"docs":{},"成":{"docs":{},"功":{"docs":{},"地":{"docs":{},"处":{"docs":{},"理":{"docs":{},"广":{"docs":{},"泛":{"docs":{},"的":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},"任":{"docs":{},"务":{"docs":{},"。":{"docs":{},"虽":{"docs":{},"然":{"docs":{},"经":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{},"很":{"docs":{},"强":{"docs":{},"，":{"docs":{},"但":{"docs":{},"在":{"docs":{},"某":{"docs":{},"些":{"docs":{},"情":{"docs":{},"况":{"docs":{},"下":{"docs":{},"，":{"docs":{},"超":{"docs":{},"过":{"docs":{},"了":{"docs":{},"人":{"docs":{},"类":{"docs":{},"的":{"docs":{},"表":{"docs":{},"现":{"docs":{},"，":{"docs":{},"未":{"docs":{},"来":{"docs":{},"的":{"docs":{},"重":{"docs":{},"要":{"docs":{},"工":{"docs":{},"作":{"docs":{},"是":{"docs":{},"研":{"docs":{},"究":{"docs":{},"语":{"docs":{},"言":{"docs":{},"现":{"docs":{},"象":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"模":{"docs":{},"块":{"docs":{},"的":{"docs":{},"结":{"docs":{},"构":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}},"步":{"docs":{},"骤":{"docs":{},"，":{"docs":{},"而":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"是":{"docs":{},"在":{"1":{"2":{"8":{"docs":{},",":{"0":{"0":{"0":{"docs":{},"个":{"docs":{},"单":{"docs":{},"词":{"docs":{},"中":{"docs":{},"每":{"docs":{},"个":{"docs":{},"e":{"docs":{},"p":{"docs":{},"o":{"docs":{},"c":{"docs":{},"h":{"docs":{},"训":{"docs":{},"练":{"docs":{},"了":{"1":{"docs":{},"m":{"docs":{},"步":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}},"docs":{}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}},"源":{"docs":{},"码":{"docs":{},"链":{"docs":{},"接":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}}}},"由":{"docs":{},"于":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"会":{"docs":{},"通":{"docs":{},"过":{"docs":{},"再":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"微":{"docs":{},"调":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"，":{"docs":{},"而":{"docs":{},"在":{"docs":{},"再":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"，":{"docs":{},"由":{"docs":{},"于":{"docs":{},"不":{"docs":{},"在":{"docs":{},"存":{"docs":{},"在":{"docs":{},"[":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{},"]":{"docs":{},"信":{"docs":{},"息":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{},"在":{"docs":{},"训":{"docs":{},"练":{"docs":{},"中":{"docs":{},"并":{"docs":{},"不":{"docs":{},"会":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{},"，":{"docs":{},"而":{"docs":{},"是":{"docs":{},"通":{"docs":{},"过":{"docs":{},"以":{"1":{"5":{"docs":{},"%":{"docs":{},"概":{"docs":{},"率":{"docs":{},"来":{"docs":{},"选":{"docs":{},"择":{"docs":{},"那":{"docs":{},"些":{"docs":{},"单":{"docs":{},"词":{"docs":{},"被":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"。":{"docs":{},"此":{"docs":{},"外":{"docs":{},"在":{"docs":{},"选":{"docs":{},"择":{"docs":{},"的":{"docs":{},"被":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"的":{"docs":{},"单":{"docs":{},"子":{"docs":{},"中":{"docs":{},"，":{"docs":{},"采":{"docs":{},"用":{"docs":{},"如":{"docs":{},"下":{"docs":{},"措":{"docs":{},"施":{"docs":{},"来":{"docs":{},"进":{"docs":{},"行":{"docs":{},"选":{"docs":{},"择":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"在":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"中":{"docs":{},"，":{"docs":{},"有":{"1":{"5":{"docs":{},"%":{"docs":{},"的":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}},"docs":{}},"docs":{}}}}}}}}}}},"的":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"版":{"docs":{},"本":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"为":{"3":{"4":{"0":{"docs":{},"m":{"docs":{},"个":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}},"微":{"docs":{},"调":{"docs":{},"过":{"docs":{},"程":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}},"词":{"docs":{},"嵌":{"docs":{},"表":{"docs":{},"示":{"docs":{},"。":{"docs":{},"图":{"2":{"docs":{},"中":{"docs":{},"表":{"docs":{},"示":{"docs":{},"的":{"docs":{},"是":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"的":{"docs":{},"输":{"docs":{},"入":{"docs":{},"表":{"docs":{},"示":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}},"答":{"docs":{},"案":{"docs":{},"对":{"docs":{},"的":{"docs":{},"形":{"docs":{},"式":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}},"经":{"docs":{},"过":{"docs":{},"和":{"docs":{},"一":{"docs":{},"个":{"docs":{},"转":{"docs":{},"移":{"docs":{},"矩":{"docs":{},"阵":{"docs":{},"t":{"docs":{},"转":{"docs":{},"移":{"docs":{},"后":{"docs":{},"在":{"docs":{},"利":{"docs":{},"用":{"docs":{},"一":{"docs":{},"个":{"docs":{},"分":{"docs":{},"类":{"docs":{},"网":{"docs":{},"络":{"docs":{},"来":{"docs":{},"进":{"docs":{},"行":{"docs":{},"分":{"docs":{},"类":{"docs":{},"，":{"docs":{},"微":{"docs":{},"调":{"docs":{},"的":{"docs":{},"过":{"docs":{},"程":{"docs":{},"如":{"docs":{},"下":{"docs":{},"图":{"docs":{},"所":{"docs":{},"示":{"docs":{},"：":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"结":{"docs":{},"合":{"docs":{},"一":{"docs":{},"个":{"docs":{},"单":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"效":{"docs":{},"果":{"docs":{},"比":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"1":{"docs":{},".":{"5":{"docs":{},"的":{"docs":{},"f":{"1":{"docs":{},"值":{"docs":{},"的":{"docs":{},"提":{"docs":{},"升":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}},"docs":{}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}},"训":{"docs":{},"练":{"docs":{},"任":{"docs":{},"务":{"docs":{},"只":{"docs":{},"是":{"docs":{},"预":{"docs":{},"测":{"docs":{},"被":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"的":{"docs":{},"单":{"docs":{},"词":{"docs":{},"而":{"docs":{},"不":{"docs":{},"是":{"docs":{},"重":{"docs":{},"新":{"docs":{},"构":{"docs":{},"建":{"docs":{},"整":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"许":{"docs":{},"多":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"如":{"docs":{},"问":{"docs":{},"题":{"docs":{},"回":{"docs":{},"答":{"docs":{},"，":{"docs":{},"语":{"docs":{},"言":{"docs":{},"推":{"docs":{},"理":{"docs":{},"都":{"docs":{},"依":{"docs":{},"赖":{"docs":{},"于":{"docs":{},"推":{"docs":{},"断":{"docs":{},"两":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"关":{"docs":{},"系":{"docs":{},"，":{"docs":{},"而":{"docs":{},"这":{"docs":{},"种":{"docs":{},"关":{"docs":{},"系":{"docs":{},"在":{"docs":{},"部":{"docs":{},"分":{"docs":{},"的":{"docs":{},"文":{"docs":{},"本":{"docs":{},"预":{"docs":{},"处":{"docs":{},"理":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"都":{"docs":{},"没":{"docs":{},"有":{"docs":{},"捕":{"docs":{},"获":{"docs":{},"。":{"docs":{},"为":{"docs":{},"了":{"docs":{},"能":{"docs":{},"够":{"docs":{},"训":{"docs":{},"练":{"docs":{},"一":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"能":{"docs":{},"够":{"docs":{},"对":{"docs":{},"两":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"关":{"docs":{},"系":{"docs":{},"进":{"docs":{},"行":{"docs":{},"推":{"docs":{},"理":{"docs":{},"，":{"docs":{},"在":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"引":{"docs":{},"入":{"docs":{},"对":{"docs":{},"对":{"docs":{},"下":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"训":{"docs":{},"练":{"docs":{},"目":{"docs":{},"标":{"docs":{},"。":{"docs":{},"在":{"docs":{},"训":{"docs":{},"练":{"docs":{},"中":{"docs":{},"，":{"docs":{},"在":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"对":{"docs":{},"（":{"docs":{},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"语":{"docs":{},"言":{"docs":{},"表":{"docs":{},"示":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"新":{"docs":{},"趋":{"docs":{},"势":{"docs":{},"就":{"docs":{},"是":{"docs":{},"通":{"docs":{},"过":{"docs":{},"利":{"docs":{},"用":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"一":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"让":{"docs":{},"后":{"docs":{},"基":{"docs":{},"于":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"将":{"docs":{},"其":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"到":{"docs":{},"其":{"docs":{},"他":{"docs":{},"的":{"docs":{},"下":{"docs":{},"游":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"在":{"docs":{},"下":{"docs":{},"游":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"在":{"docs":{},"对":{"docs":{},"这":{"docs":{},"些":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"进":{"docs":{},"行":{"docs":{},"微":{"docs":{},"调":{"docs":{},"。":{"docs":{},"这":{"docs":{},"些":{"docs":{},"方":{"docs":{},"法":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"优":{"docs":{},"势":{"docs":{},"就":{"docs":{},"是":{"docs":{},"只":{"docs":{},"有":{"docs":{},"较":{"docs":{},"少":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"需":{"docs":{},"要":{"docs":{},"进":{"docs":{},"行":{"docs":{},"重":{"docs":{},"新":{"docs":{},"学":{"docs":{},"习":{"docs":{},"。":{"docs":{},"在":{"docs":{},"这":{"docs":{},"方":{"docs":{},"面":{"docs":{},"的":{"docs":{},"工":{"docs":{},"作":{"docs":{},"中":{"docs":{},"，":{"docs":{},"最":{"docs":{},"新":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"工":{"docs":{},"作":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"型":{"docs":{},"已":{"docs":{},"经":{"docs":{},"被":{"docs":{},"证":{"docs":{},"明":{"docs":{},"能":{"docs":{},"有":{"docs":{},"效":{"docs":{},"提":{"docs":{},"升":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{},"。":{"docs":{},"这":{"docs":{},"些":{"docs":{},"任":{"docs":{},"务":{"docs":{},"包":{"docs":{},"括":{"docs":{},"句":{"docs":{},"子":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"如":{"docs":{},"语":{"docs":{},"言":{"docs":{},"推":{"docs":{},"理":{"docs":{},"及":{"docs":{},"解":{"docs":{},"析":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"任":{"docs":{},"务":{"docs":{},"其":{"docs":{},"目":{"docs":{},"的":{"docs":{},"是":{"docs":{},"从":{"docs":{},"句":{"docs":{},"子":{"docs":{},"级":{"docs":{},"别":{"docs":{},"推":{"docs":{},"断":{"docs":{},"句":{"docs":{},"子":{"docs":{},"间":{"docs":{},"相":{"docs":{},"互":{"docs":{},"关":{"docs":{},"系":{"docs":{},"。":{"docs":{},"此":{"docs":{},"外":{"docs":{},"还":{"docs":{},"包":{"docs":{},"括":{"docs":{},"一":{"docs":{},"些":{"docs":{},"序":{"docs":{},"列":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"如":{"docs":{},"命":{"docs":{},"名":{"docs":{},"实":{"docs":{},"体":{"docs":{},"识":{"docs":{},"别":{"docs":{},"，":{"docs":{},"文":{"docs":{},"本":{"docs":{},"理":{"docs":{},"解":{"docs":{},"挑":{"docs":{},"战":{"docs":{},"任":{"docs":{},"务":{"docs":{},"（":{"docs":{},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"d":{"docs":{},"）":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"任":{"docs":{},"务":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"需":{"docs":{},"要":{"docs":{},"在":{"docs":{},"序":{"docs":{},"列":{"docs":{},"级":{"docs":{},"别":{"docs":{},"产":{"docs":{},"生":{"docs":{},"经":{"docs":{},"过":{"docs":{},"微":{"docs":{},"调":{"docs":{},"后":{"docs":{},"的":{"docs":{},"更":{"docs":{},"好":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"这":{"docs":{},"篇":{"docs":{},"文":{"docs":{},"章":{"docs":{},"的":{"docs":{},"贡":{"docs":{},"献":{"docs":{},"如":{"docs":{},"下":{"docs":{},"：":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}}},"项":{"docs":{},"目":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"ref":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","tf":0.037037037037037035},"paper/nlp/papers/Deep contextualized word representations.html":{"ref":"paper/nlp/papers/Deep contextualized word representations.html","tf":0.04}}}},"：":{"docs":{},"表":{"docs":{},"示":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"层":{"docs":{},"数":{"docs":{},"量":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.003424657534246575}}}}}}}}}}},"length":511},"corpusTokens":["##","*a*:表示注意力header的数量。",",winograd",".","1.简介","11项自然语言处理任务中都取得了目前最好的性能，具体包括将glue的基准值提升到了80.4%(7.6%的绝对提升)，在multnli中准确率有了86.7的提升（5.6%的绝对提升），在squad","11项自然语言处理任务中都取得了目前最好的性能，具体包括将glue的基准值提升到了80.4%(7.6%的绝对提升)，在multnli中准确率达到86.7的（5.6%的绝对提升），在squad","2","2.1","2.2","2.3","2.相关工作","2003的命名实体识别任务中进行了测试。该数据集包含200k的训练数据集，其中实体类别为person（人物）,organization（组织）,location（地点），miscellaneous（其他项）和other(非实体词)。","2018","2018年","2019年","3","3.1","3.2","3.3","3.3.1","3.3.2","3.4","3.5","3.6","4,b1参数为0.9，b2参数为0.999，l2范数的权重为0.01.","4.1","4.1.1","4.2","4.4","4.实验及结果","43e","5","5,2e","5,3e","5,4e","5,batch_siz","5,总共迭代三个epoch.其对比的baseline为esim+glove和esim+elmo.实验结果如下：","5,而bert的学习率则是根据特定任务需求设置来进行微调的。","5等。实验结果如下图所示","a,sent","aaai,全称是associ","abstract：","abstract：本文作者推出了一套新的语言表达模型bert，全称为bidirect","ac(lth","acl","acl的全称是th","acl（tacl），此外还有一些期刊与自然语言处理相关。如tslp(（acm","acm","acm）主办，包括如下几个会议。","advanc","ai","ai,artifici","aistat","al.,2017)从另外一个重要角度来对词嵌进行处理。在该方法中，作者提出了从语言模型中提取出与上下文敏感特征的方法。通过将基于上下文敏感的词嵌和特定的任务结构相结合后，elmo在很多自然语言处理任务中都取得了soat(st","american","approach","ariv","art)的实验效果，包括基于squad的问题，语义分析以及命名识别识别等众多的任务。","artifici","asia","asian","associ","association)组织","a类","b","base","base版本的参数有110m个，而bert","batch","bert","bert:","bert是一个多层的基于双向转移编码器的模型，其根据tensor2tensor的方式来实践。bert使用的“transform","bert有两个版本：bertbase和bertlarge,其中bertbase的参数如下：l=12,h=768,a=12,而bertlarge的数据l=24,h=1024,a=16.因此bert","bert模型介绍","bert的训练语料是在一个bookcorpus(800m)和维基百科的文本语料中（2,500m个单词）。","bert的输入表示","bert的预训练过程和目前已有的方法类似，其再训练语料我们使用了两部分的语料：bookcorpus(800m","bidirect","blocks）","b类","b）中，一个句子b有50%是句子a的下一句，而50%的可能是随机另外一个句子。","ccf","chang,","chapter","cikm,该会议名称缩写和上一个相同，但是其全称为intern","cikmm,其全称是confer","classif","cola","cole","coling(intern","committe","comput","confer","conference，也是有关信息检索及数据挖掘。","conll","conll(confer","contextu","c类","data","data&corpu","databas","decis","deep","embedding:","embedding:使用了支持长度为512个token的词嵌表示方法。","embedding。在训练中，针对第i个单词，通过将其bert输出的词嵌表示和一个概率转移矩阵t相乘后利用softmax输出得到该但是是否是答案开始的概率。针对答案的结束，也是使用同样的方式预测某个词是否是答案的结果，而训练的目标是最大化正确开始位置和结束位置的释然概率。该过程的示意图如图所示：","embedding方法来实现词嵌表示。","embedding，而答案使用b","emnlp","emnlp(confer","emnlp是有acl下面的比较著名的兴趣小组（speci","empir","encod","encoder”编码器近来得到广泛的应用，因此本文中没有进行详细的介绍。","encoder模型，bert的","epochs:3,4","et","evaluation,其由欧洲语言资源组织进行elra(european","evaluation,该会议由acl的特殊兴趣小组siglex进行组织，每年都会举办，国内也有很多研究机构及公司参与，如哈工大科大讯飞等,其最新的链接网址如下：http://alt.qcri.org/semeval2019/index.php?id=task","gener","glue数据集","glue数据集是一个通用的自然语言理解评估数据集，用于评估对自然语言的理解任务。glue数据集包含多个子数据集。","glue数据集的实验结果","googl","gpt(gener","gpt,bilstm+elmo+attn等模型都有很大的提升，其中bertlarge和bertbase比起来有更多的提升。","gpt2018基于迁移学习的方法在许多句子级别的任务中取得了sota的实验结果。","gpt中则是使用的一种从左到右的编码模型，引起bert可以被看做迁移学习编码器，而openai","gpt使用的句子分隔符为[sep]，分类器的分隔符为[cls],并且这些只在微调过程中使用。而bert则是学习了[sep],[cls]以及句子分割信息（a/b）在预训练过程中。","gpt则是一种迁移学习的解码器。bert和openai","gpt在包含32,000个单词的语料库中每个epoch训练了1m","gpt在所有的微调步骤中都使用了相同的学习率5e","gpt的区别可以如图所示","gpt等结构的差别。","group","groups,sigs）之一的sigdata(speci","h","hearst正式宣布成立国际计算语言学学会亚太地区分会（aacl，th","http://coling2018.org","http://emnlp2018.org","http://naacl.org","http://www.conll.org/","https://arxiv.org/abs/1810.04805","https://blog.csdn.net/jilrvrtrc/article/details/83829470","https://blog.csdn.net/lyb3b3b/article/details/83548964","https://v.youku.com/v_show/id_xnda4mdyznzgwoa==.html?spm=a2h3j.8428770.3416059.1","https://www.aclweb.org/port","https://www.cnblogs.com/niuxichuan/p/7602012.html","icml,全称是","ijcai,全称是intern","improv","inform","intellig","intelligence，主要关注人工智能领域的最新研究进展，其中也有大量的人工智能相关的知识。","intelligence，也是关注人工智能领域的另外一个重要学术会议。","interest","intern","introduct","jacobdevlin,m","jair,journ","jmlr,","joint","journal","jumper:learn","kenton","keywords:","knowledg","knowledgebas","kristina","l:表示层级的数量（transform","languag","learn","learnin)","learning)举办的。其也是每年举办一次，由于naacl是acl在北美的分会，因此当acl在北美举办的时候，naacl就会停办一年。","lee,","level级别的任务中都取得了较好的效果。在很多有特定需要的任务中也取得了较好的效果。","levle和token","linguist","linguistics)","linguistics)组织的。该会议每两年举办一次。","linguistics,其由1965年创办，是由老牌的nlp学术会议组织iccl(th","linguistics,翻译过来是计算机语言协会，自然语言处理与计算语言学领域（以下简称nlp/cl）最权威的国际专业学会，acl成立于1962年，是自然语言处理(nlp)领域影响力最大、最具活力的顶级国际学术组织，每年举办一次。这个学会主办了","linguistics,该会议是acl在北美的分会，也是有acl主办。其是有acl下面的兴趣小组signal(speci","linguistics和transact","linguistics）。此次成立acl亚太分会，将进一步促进亚太地区nlp相关技术和研究的发展。据悉，首届aacl会议预计在2020年举行，此后将每两年举行一次。","lm","lm\"(mlm)模型。在该模型中，被隐藏掉的token会被输入到softmax模型中。在所有的预训练实验中，bert采用15%的概率随机隐藏掉输入句子中的token.","lm和next","lrec","lrec全称是intern","machin","machinery,","make","management，其关注信息管理","mask","masking模型。","method","mining,由名称可知，该会议关注搜索和数据挖掘。","mnli","mrpc,该数据集微软研究解释语料库由自动提取的句子对组成来自在线新闻来源，带有人工注释判断两个句子的语义是否一致相等的","naacl","naacl(th","naacl的全称是th","natur","nc,neurocompt","neural","next","nip","nli是一个小型的自然语言推断数据集，在glue的网页中指出该数据集在构造上存在一定我的问题。因此在模型中排除了openaigpt。","nlp","nlp/cl","nlp相关的其他国际会议","north","number","optimal)，而在针对一些token级别的任务中，如squad的问答问题中，则这种方法有可能是毁灭性的，因此在这些任务中，纳入两个方面的信息是至关重要的。","pacif","paper","posit","pre","predictioni","prediction模型。","process","processing)","processing)),其他相关期刊及投稿链接如下：","processing)举办的。","processing,nlp）领域的一些著名会议及期刊，将按照ccf对会议的分级标准对相关会议进行整理，介绍每个会议的关注主题，召开周期、会议网站及相关的信息，方便后面查找最新的相关论文，从而对该领域进行比较深入的研究。每个会议及期刊都有相关的入口链接，方便进行查看。","processing.","processing）)以及talip((acm","qnli","qqp","qqp问题对是一个二进制分类任务，用于确定连个问题的语义是否是相等的。","rate(adam):5","rate)以及训练的epoch次数不同外。在预训练过程中，dropout的概率一直设置为0.1，而新增参数的相关设置是和任务先关的。作者在训练bert的过程中，发现如下参数在多个任务中具有较好的表现。","read","represent","research","resourc","retrieval,其主要关注信息检索。","rte","search","semant","semev","semeval,其全称是intern","sentenc","sigir,其全称是speci","size:16,32","size，学习率(learn","speech","sqqad","squad","sst","st","statist","system","token","tool","toutanova","tps的默契上进行了训练，每个训练都花费了4天左右完成。","tpus的集群上进行了训练，而bertlarge则是在配置为16cloud","train","transact","transform","transform)引入了最小任务的参数，在下游任务中，其通过简单的微调的方式来调整模型的参数。在以前的工作中，所有的预训练方法都是通过使用同样的目标函数并通过双向语言模型来学习更好的通用语言表示。","transformers(基于双向翻译编码的表示模型)。bert通过提出了两个新的预训练目标来解决以前的提到的单向限制问题：基于“masked”的语言模型（mlm），这种方法在1953年就曾经被提出。这种语言模型通过随机将输入句子中的token进行隐藏，然后预训练的目标就是基于上下文来预测出被覆盖掉的单词。不同于传统的从左到右的预训练语言模型，mlm训练目标使得我们能够充分利用左边和右边信息来训练一个更深的双向翻译模型。除了利用masked语言模型外，我们还在模型中引入了预测下一个句子的训练目标来同时训练句子对级别的表示模型。","transformers。与近年来提出的语言模型不一样的地方在于，bert不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的bert模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改。bert是一种概念上简单，但在实际中却有强大的性能的模型。bert在","transformers。与近年来提出的语言模型不一样的地方在于，bert不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的bert模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改。bert是一种概念上简单，但根据经验推断其具有强大的性能。bert在","tuning）的方法。在基于特征的方法中，elmo(2018)是一种基于特征任务的模型，其包含一个预训练模型以及一些其他的特征。在微调模型中，openai","uai","uncertainti","understand","v1.1","v1.1的问答任务中其f1值达到了93.2（1.5%）的绝对提升，比人类的表现都搞了2.0的提升。","web","wei","wide","wnli","word","words)和英语维基百科(2,500m","workshop","world","wsdm,该会议全称为web","​","下载地址：链接：https://pan.baidu.com/s/1nmh33yk80sznki8vhmeeea","不像传统的从左到右或者从右到左的预训练语言模型任务，在bert中使用了两种原创的无监督学习的预训练任务，分别是mask","不是所有被选择的单词都会被隐藏掉，而是其中80%的可能会隐藏掉该单词，10%的时间将该单词替换成一个随机的单词。而另外还是有10的时间是该单词保持不变。","为32，训练迭代次数为3个epoch.实验结果如下：","为了能够训练一个具有双向深层的表示模型，在bert中采用了一种较为直观的模型，通过随机隐藏掉句子中的token，并让模型能对该句子进行正确的预测。这种模型在bert中被称为\"mask","为了评估bert在序列标注任务中的性能，论文还基于bert在conl","人工智能领域与自然语言处理相关的会议","从初始概念上来将，一个更深层次的双向模型比传统的从左到有或者从右到左基于简单讲从左到右或者从右到左模型进行组合的方法有更好的效果。然后，目前传统的语言模型则往往只能通过从左到右或者从右到左的模型进行训练。在双向的环境中，句子中任意一个token都有可能被预测到在一个多层的网络环境中。","从实验结果中可以看到，bert去了的最好的结果，其中bertlarg","从实验结果中，可以看出该模型可以取得较好的实验结果。此外作者还对模型的一些相关参数进行了分析并进行了实验讨论。","从实验结果可以看出，bert也去了的sota的实验结果。","从实验结果可以，bert相比较于openai","以下是根据ccf2015年的对自然语言处理领域的会议的分级标准来进行罗列和整理","以及分类矩阵w相结合并经过softmax分了的方式来进行微调。在每个任务中，使用的学习率为5","会议名称","会议网站","作者信息","作者单位","使用了包含30,000token的词库的wordpiec","全称","全称是confer","全称是intern","关键词","其他解读","内容","出版社","分享者：刘露平","分析解读最新的自然语言处理方面的论文，供大家一起进行交流和学习。","分类等级","则分类结果接上一个softmax层后得到相应的各个类别的概率p=softmax(cwt)。这样所有的bert的参数和新增的w参数一起经过微调后得到分类结果的输出。而在针对单词级别的任务中，训练过程和句子级别的有所区别。","区别于传统的auto","单词)。在维基百科语料中，我们目前只提取了文本消息而忽略了列表，表格以及头等信息。在实际训练中，使用文本级别的语料比句子级别的语料可以取得更好的实验结果。为了生存训练输入序列，我们从语料库中选择了两个文本句子并将其合并成一个句子。其中第一个句子使用句子a的embedding，而第二个句子使用句子b的embedding。在生成句子的时候，句子b的embedding有50%的可能是随机选择的，用于训练模型对下一个句子的预测。两个句子合并起来的token序列长度小于512个token。句子首先使用wordpiece的方法生成输入表示并在其中随机挑选15%来隐藏掉，然后在此基础上使用lm","单词可能会被隐藏掉，因此在训练过程中其会花费较多的时间。","原文链接","参考文献","召开周期","命名实体识别","和openaigpt的比较","国际会议","在","在11项自然语言处理任务中都取得了sota的结果。表明双向语言模型在文本处理中的重要性。","在bert中输入被表示为两种形式：","在bert中，bert使用的是一个双向的自注意力机制的编码模型，而openai","在bert的输入中，对于一个输入token序列，该输入句子的序列表示可以由几部分构成：token序列表示，segment和posit","在transformer模型中，由于encoder并不知道哪些单词可能会被替换成随机的单词，因此要求编码器能够学习到一种对所有单词都有表现的能力，此外随机替换只发在在1.5%的概率中，因此不会影响模型的整体理解能力。","在国际上acl，coling，enmlp和naacl是默认的四大自然语言处理顶级学术会议，其中acl，emnlp和naacl都是由acl及相应的子组织举办的。","在实验中，作者同时发现100k+左右的数据对于模型的参数的变化是极小的。因此微调过程是十分快速的。","在微调训练过程中，大部分模型的参数和原来预训练的过程是一样的。除了batch","在构建训练集时候，在选择句子a的下一个句子时，通过随机选择下一个句子的方式来实现构建训练集。","在每层中我们设置dropout概率为0.1，激活函数选择的是gelu。训练的损失函数lm模型的平均likelihood的和以及预测下一个句子的平均likelihood.","在第三节中，详细介绍bert模型机器实现细节。该节首选叙述bert模块的整体框架及bert的输入表示。接着在3.3节中介绍预训练的任务以及核心的创新。3.4节中介绍预训练的流程，而在3.5节中介绍微调的方法。最后在3.6节中讨论bert和openai","在自然语言处理中，一般来将，大家更关注学术会议，其主要原因是发表周期短，通过会议也可以进行深入的交流。但自然语言处理领域也有自己的学术期刊，其旗舰期刊有两个，分别是comput","在自然语言处理中，机器学习也是其主流的方法，在机器学习领域相关的学术会议包括icml,nips,uai及aistats,下面将简单介绍。","在训练bertbase的过程中，作者在配置有4cloud","在该任务中，训练时使用的额学习率为5e","在该类型的任务中，和传统的句子分类任务有明显的不同，在对bert进行微调的任务中，将问题和答案都表示成两个独立的序列。其中问题使用a","在该项工作中，作者定义了如下变量：","在输入词序列中，序列的第一个词都是以[cls]的方式开头，针对该token的","在这篇文章中我们证明了双向预训练模型的重要性。不同于传统的双向预测模型，bert使用了一种基于masked的语言模型来训练一种更深的语言模型。此外区别于传统的浅层双向语言模型，bert是一种使用深层次的双向语言模型。","在这篇文章中，我们任务目前的技术严重限制了预训练在语言表达方面的能力，特别是针对微调的方法。最大的缺陷是目前的预训练模型都是双向的，其严格限制了模型在进行预训练时的选择能力。例如在openaigp中，作者使用了从左到右的模型，使得句子中每个token只能被以前的词所关注。这种方式对句子级别的任务是次优的(sub","在这篇论文中，我们提出了一种基于微调的方法bert:bidirect","在这节中我们介绍基于预训练的方式来生成语言表示模型的相关方方法，同时简要介绍一下在这个领域目前最流行的方法。","在进行命名实体识别的微调时时，针对每个词，使用其最后一个隐藏状态c","在针对glue数据集的微调训练中，使用了每个句子的第一个输入[cls]的最后一个隐藏层输出c","在预训练中，在每个epoch中，输入句子数量为256个句子，每个epoch迭代1,000,000次，最终没3.3亿个单词上训练了差不多40个epoch.","基于学习的词表示方法在多个领域都有广泛的应用。其中包括非神经元的方法和基于神经网络的方法。基于词嵌表示的预训练方法目前是现代语言处理系统中的一个重要集成部分，其对系统的后续处理提供了有强大的提升。除了词级别的词嵌外，这些方法也被推广到粗粒度的句子级别词嵌以及锻炼词嵌中。在传统的方法中，这些学习到的表示方法往往作为下游任务的特征输入到下游模型中。elmo(pet","基于微调的方法","基于特征的方法","基于监督数据的迁移学习方法","处理和人工智能是密切相关的，其是人工智能研究的重要内容，人工智能研究的两大国际顶会是aaai和ijcai","多类型的自然语言推理任务数据集。","如果输入是单个句子，则直接使用句子a的词嵌表示。","实验结果如图所示：","实验部分主要叙述bert在11项自然语言预处理任务中","密码:trsadmin","对于句子级别的分类任务，bert的微调是很直接的。为了获取输入句子的固定维度的表示,bert使用了输入句子的第一个词的最后一个隐藏层的输入c（rh）。在进行分类时，在bert输出的基础上在增加一个分类层，分类层的矩阵表示为w。","将相关的知识进行整理后分享，方便后面的人学习","将自然语言处理方面的工具进行整理汇总","工具整理","常识推理任务（swag","微调过程","总结","我们使用饿了adam优化算法，其中学习率为1","我们展示了预训练模块可以消除在很多任务中需要依赖严重特征工程的任务。bert","摘要","收集整理相关成员上传的资料及相应的数据集，方便有需要的同学进行下载。","收集整理自然语言处理领域的会议及期刊，方便大家进行相关的查阅。","数据集","数据集是一个由斯坦福收集的包含100k的问题/答案对数据集。该数据集是通过给定一个问题以及维基百科中包含该答案的段落，该任务从段落中预测出相应的答案。其简单示例如下：","数据集）","无监督学习预训练方法的优势是有大量的无标签的数据可以大量的获取，基于有标签数据的迁移学习目前也被证明在许多文本处理任务中有较好的结果，例如自然语言推理、机器翻译等任务。除了在nlp处理领域外，很多机器视觉领域的研究也说明了预训练并集合迁移学习的重要方，这些方法通过在大量的预训练数据上进行训练后再通过微调的方式可以取得较好的实验结果。","时间","时间：2019年3月27日","时间：2019年3月28日","是一个有斯坦福构建的二分类任务的数据集，是从一些电影评论中抽取出来的句子并且包含人工的标注信息。","是一个自然语言推理任务数据集，其中的正样本是一些问题，答案对的句子则是来自于同一个段落的句子，其并不形成问题","是第一个基于微调方法的语言表示模型，并且在sentenc","最后隐藏状态被用在分类任务的聚合表示中。而如果针对的是非分类任务，则该词对应的向量被删除掉。在句子对中，所有句子都被表示到一个序列任务中，我们通过两种不同的方式来区分。首先通过一个特殊的token[esp]来对两个句子进行区分，在第一个句子中，每个句子的token会加上句子a的embedding表示，而在句子b中，每个token都会加上句子b的embedding表示。","最近的经验改进表明，由于转移学习与语言模型已经证明，丰富，无监督的预培训是许多语言理解系统的一个组成部分。特别是，这些结果使得即使是低资源任务也能从非常深的单向架构中受益。我们的主要贡献是将这些发现进一步推广到深层双向体系结构中，使相同的预先培训的模型能够成功地处理广泛的nlp任务。虽然经验结果很强，但在某些情况下，超过了人类的表现，未来的重要工作是研究语言现象。","期刊","本文介绍自然语言处理（natur","本文作者推出了一套新的语言表达模型bert，全称为bidirect","机器学习领域中与自然语言处理相关的顶级会议","概述：该ppt是做汇报时候的ppt，讲了知识图谱的基本知识及实体和关系抽取相关论文，有需要可以下载。","模块的结构","步骤，而bert是在128,000个单词中每个epoch训练了1m步。","每两年一次","每年一次","源码链接","由于bert会通过再训练的方式来微调模型的参数，而在再训练模型中，由于不在存在[mask]信息，因此在训练中并不会隐藏掉所有的mask，而是通过以15%概率来选择那些单词被隐藏掉。此外在选择的被隐藏的单子中，采用如下措施来进行选择。","由于在bert中，有15%的","的large版本的参数为340m个。","的微调过程。","的词嵌表示。图2中表示的是bert的输入表示。","目前在预训练领域可以用于和bert进行比较的是openaigpt,openaigpt使用从左到右的方式在大量语料上进行训练。在整体结构上bert和openaigpt是相似的，因此整体结构差异较小，但是bert和openaigpt还有其他几个方面的差别。","目前有两种预训练的方法被用于下游处理任务中：基于特征的和基于微调（fine","目录","知识分享","知识图谱概述ppt","研究性工作","答案对的形式。","经过和一个转移矩阵t转移后在利用一个分类网络来进行分类，微调的过程如下图所示：","结合一个单个模型的效果比传统的方法1.5的f1值的提升。","自然语言","自然语言处理会议及期刊","训练任务只是预测被隐藏掉的单词而不是重新构建整个句子。","许多自然语言处理任务中，如问题回答，语言推理都依赖于推断两个句子的关系，而这种关系在部分的文本预处理模型中都没有捕获。为了能够训练一个模型能够对两个句子的关系进行推理，在bert的预训练任务中，引入对对下一个句子预测的训练目标。在训练中，在一个句子对（sent","论文解读","该任务是在一个有对抗生成网络生成的数据集（包含113k句子）上进行的实验任务，其任务是给定一个视频字幕中的一句话，推断最由可能的后续情节。其简单示意如下图所示：","该任务的微调过程和glue是类似的，在每个例子中，我们构建四个输入句子，其中每个都是一个数据句子a和可能后续进展b的结合。我们同样也引入一个转移矩阵t,通过将每个输入和矩阵t进行相乘后利用softmax来预测概率。在进行微调训练中，使用的学习速率为2","该博客主要收集整理与自然语言处理相关工具，论文以及代码等资源信息，对目前自然语言处理最前沿的技术、论文及相关相关代码进行整理后进行分享，主要关注点包括自然语言处理基础方法，知识图谱构建，智能问答等几个方面的知识。通过不断地积累让大家在这条路上越走越远。","该数据集是一个二分类的任务，其任务目标类似于mnli,但是其训练数据较少。","该数据集是一个文本语义相似性数据集，其收集与一些新闻标题和其他来源，其预测任务是预测两个句子在语义上的多大的相似性。","该数据集是一个语言课接受性的二分类语料库，其预测的任务是一个英语句子在语言上是否是可以接受的。","该材料是一个视频，讲述的是基于强化学习的文本分类模型，其用于阅读理解中，感谢图像所王大东同学提供的资料。有兴趣的可以观看。","该部分主要是分享目前自然语言及知识图谱处理方法的基础知识，目前包含如下三个方面：自然语言处理基础技术，知识图谱构建，智能问答,论文整理将按照年份和板块来进行整理。","该页面收集整理日常工作中相关的数据集和资料，大家可以通过将相关资料放到网盘或者其他github仓库中，然后这里放上相应的链接。","语言表示模型的一个新趋势就是通过利用语言模型预训练一个模型，让后基于迁移学习的方法将其迁移到其他的下游任务中，在下游任务中在对这些模型的参数进行微调。这些方法的一个优势就是只有较少的参数需要进行重新学习。在这方面的工作中，最新的一个工作openai","语言预训练模型已经被证明能有效提升自然语言处理任务的性能。这些任务包括句子级别的任务如语言推理及解析，这些任务其目的是从句子级别推断句子间相互关系。此外还包括一些序列级别的任务，如命名实体识别，文本理解挑战任务（squad），这些任务模型中需要在序列级别产生经过微调后的更好的结果。","这篇文章的贡献如下：","除了上述被ccf收录的会议外，在自然语言处理领域还有其他许多重要会议，分别是semeval,lrec等。","除了直接与自然语言处理相关外，还有其他许多与自然语言处理相关的学术会议，包括信息检索，数据挖掘及人工智能领域，这些都是属于自然语言处理的应用领域。其中信息检索和数据挖掘与自然语言处理是密切相关的，主要由美国计算机学会（associ","项目","预训练任务","预训练流程","预训练，迁移学习","领域最权威的国际会议，即acl年会。1982年和1999年，acl分别成立了欧洲分会（eacl）和北美分会（naacl）两个区域性分会。近年来，亚太地区在自然语言处理方面的研究进步显著，2018年7月15日，第56届acl年会在澳大利亚墨尔本举行。开幕仪式上，acl主席marti","（永久有效）提取码：mdan","：表示隐藏层数量"],"pipeline":["stopWordFilter","stemmer"]},"store":{"./":{"url":"./","title":"Introduction","keywords":"","body":"该博客主要收集整理与自然语言处理相关工具，论文以及代码等资源信息，对目前自然语言处理最前沿的技术、论文及相关相关代码进行整理后进行分享，主要关注点包括自然语言处理基础方法，知识图谱构建，智能问答等几个方面的知识。通过不断地积累让大家在这条路上越走越远。\n自然语言处理会议及期刊\n收集整理自然语言处理领域的会议及期刊，方便大家进行相关的查阅。\n工具整理\n将自然语言处理方面的工具进行整理汇总\n数据集\n收集整理相关成员上传的资料及相应的数据集，方便有需要的同学进行下载。\n论文解读\n分析解读最新的自然语言处理方面的论文，供大家一起进行交流和学习。\n知识分享\n将相关的知识进行整理后分享，方便后面的人学习\n"},"metting/summit metting.html":{"url":"metting/summit metting.html","title":"Conferences and journals","keywords":"","body":"本文介绍自然语言处理（Natural Language Processing,NLP）领域的一些著名会议及期刊，将按照CCF对会议的分级标准对相关会议进行整理，介绍每个会议的关注主题，召开周期、会议网站及相关的信息，方便后面查找最新的相关论文，从而对该领域进行比较深入的研究。每个会议及期刊都有相关的入口链接，方便进行查看。\n国际会议\n以下是根据CCF2015年的对自然语言处理领域的会议的分级标准来进行罗列和整理\n\n   \n      分类等级\n      会议名称\n      召开周期\n      出版社\n      会议网站\n   \n   \n      CCF A类\n      AC(LThe Association for Computational Linguistics)\n      每年一次\n      ACL\n      https://www.aclweb.org/portal\n   \n   \n      \n   \n   \n      CCF B类\n      COLING(International Conference on Computational Linguistics)\n      每年一次\n      ACL\n      http://coling2018.org\n   \n   \n      \n   \n   \n      \n      EMNLP(Conference on Empirical Methods in Natural Language Processing)\n      每年一次\n      ACM\n      http://emnlp2018.org\n   \n   \n      \n   \n   \n      CCF C类\n      NAACL(The North American Chapter of the Association for Computational Linguistics)\n      每年一次\n      NAACL\n      http://naacl.org\n   \n   \n      \n   \n   \n      \n      CoNLL(Conference on Computational Natural Language Learnin)\n      每两年一次\n      CoNLL\n      http://www.conll.org/\n   \n   \n      \n   \n\n\n\n在国际上ACL，COLING，ENMLP和NAACL是默认的四大自然语言处理顶级学术会议，其中ACL，EMNLP和NAACL都是由ACL及相应的子组织举办的。\nACL\nACL的全称是The Association for Computational Linguistics,翻译过来是计算机语言协会，自然语言处理与计算语言学领域（以下简称NLP/CL）最权威的国际专业学会，ACL成立于1962年，是自然语言处理(NLP)领域影响力最大、最具活力的顶级国际学术组织，每年举办一次。这个学会主办了 NLP/CL 领域最权威的国际会议，即ACL年会。1982年和1999年，ACL分别成立了欧洲分会（EACL）和北美分会（NAACL）两个区域性分会。近年来，亚太地区在自然语言处理方面的研究进步显著，2018年7月15日，第56届ACL年会在澳大利亚墨尔本举行。开幕仪式上，ACL主席Marti Hearst正式宣布成立国际计算语言学学会亚太地区分会（AACL，The Asia-Pacific Chapter of Association for Computational Linguistics）。此次成立ACL亚太分会，将进一步促进亚太地区NLP相关技术和研究的发展。据悉，首届AACL会议预计在2020年举行，此后将每两年举行一次。\nCOLING\nCOLING 全称是International Conference on Computational Linguistics,其由1965年创办，是由老牌的NLP学术会议组织ICCL(The International Committee on Computational Linguistics)组织的。该会议每两年举办一次。\nEMNLP\nEMNLP 全称是Conference on Empirical Methods in Natural Language Processing. EMNLP是有ACL下面的比较著名的兴趣小组（Special Interest Groups,SIGS）之一的SIGDATA(Special Interest Group on Linguistic Data&Corpus-based Approaches to Natural Language Processing)举办的。\nNAACL\nNAACL的全称是The North American Chapter of the Association for Computational Linguistics,该会议是ACL在北美的分会，也是有ACL主办。其是有ACL下面的兴趣小组SIGNAL(special Interest Group on Natural Language Learning)举办的。其也是每年举办一次，由于NAACL是ACL在北美的分会，因此当ACL在北美举办的时候，NAACL就会停办一年。\n除了上述被CCF收录的会议外，在自然语言处理领域还有其他许多重要会议，分别是SemEval,LREC等。\nSemEval\nSemEval,其全称是International Workshop on Semantic Evaluation,该会议由ACL的特殊兴趣小组SIGLEX进行组织，每年都会举办，国内也有很多研究机构及公司参与，如哈工大科大讯飞等,其最新的链接网址如下：http://alt.qcri.org/semeval2019/index.php?id=tasks\nLREC\nLREC全称是International Conference on Language Resource and Evaluation,其由欧洲语言资源组织进行ELRA(European Language Resources Association)组织\nNLP相关的其他国际会议\n除了直接与自然语言处理相关外，还有其他许多与自然语言处理相关的学术会议，包括信息检索，数据挖掘及人工智能领域，这些都是属于自然语言处理的应用领域。其中信息检索和数据挖掘与自然语言处理是密切相关的，主要由美国计算机学会（Association for Computing Machinery, ACM）主办，包括如下几个会议。\nSIGIR,其全称是Special Interest Group on Informational Retrieval,其主要关注信息检索。\nCIKMM,其全称是Conference on Information and Knowledge Management，其关注信息管理\nCIKM,该会议名称缩写和上一个相同，但是其全称为International World Wide Web Conference，也是有关信息检索及数据挖掘。\nWSDM,该会议全称为Web Search and Data Mining,由名称可知，该会议关注搜索和数据挖掘。\n人工智能领域与自然语言处理相关的会议\n自然语言 处理和人工智能是密切相关的，其是人工智能研究的重要内容，人工智能研究的两大国际顶会是AAAI和IJCAI\nAAAI,全称是Association for the Advancement of Artificial Intelligence，主要关注人工智能领域的最新研究进展，其中也有大量的人工智能相关的知识。\nIJCAI,全称是International Joint Conferences on Artificial Intelligence，也是关注人工智能领域的另外一个重要学术会议。\n机器学习领域中与自然语言处理相关的顶级会议\n在自然语言处理中，机器学习也是其主流的方法，在机器学习领域相关的学术会议包括ICML,NIPS,UAI及AISTATS,下面将简单介绍。\nICML,全称是 International Conference on Machine Learning\nNIPS 全称 Conference on Neural Information Processing Systems\nUAI 全称 Conference on Uncertainty in Artificial Intelligence\nAISTATS 全称 International Conference on Artificial Intelligence and Statistics\n期刊\n在自然语言处理中，一般来将，大家更关注学术会议，其主要原因是发表周期短，通过会议也可以进行深入的交流。但自然语言处理领域也有自己的学术期刊，其旗舰期刊有两个，分别是Computational Linguistics和Transactions of ACL（TACL），此外还有一些期刊与自然语言处理相关。如TSLP(（ACM Transactions on Speech and Language Processing）)以及TALIP((ACM Transactions on Asian Language Information Processing)),其他相关期刊及投稿链接如下：\nAI,Artificial Intelligence\nJAIR,Journal of AI Research\nJMLR, Journal of Machine Learning Research\nNC,Neurocompting\n参考文献\n\nhttps://blog.csdn.net/lyb3b3b/article/details/83548964\nhttps://www.cnblogs.com/niuxichuan/p/7602012.html\n\n"},"tools/readme.html":{"url":"tools/readme.html","title":"tools","keywords":"","body":""},"database/readme.html":{"url":"database/readme.html","title":"database","keywords":"","body":"该页面收集整理日常工作中相关的数据集和资料，大家可以通过将相关资料放到网盘或者其他github仓库中，然后这里放上相应的链接。\n\n知识图谱概述PPT\n\n时间：2019年3月27日\n分享者：刘露平\n下载地址：链接：https://pan.baidu.com/s/1nmH33YK80SZNKI8VHmEeEA （永久有效）提取码：mdan \n概述：该PPT是做汇报时候的PPT，讲了知识图谱的基本知识及实体和关系抽取相关论文，有需要可以下载。\n\n\nJumper:Learning When to Make Classification Decision in Reading\n\n时间：2019年3月28日\n分享者：刘露平\nhttps://v.youku.com/v_show/id_XNDA4MDYzNzgwOA==.html?spm=a2h3j.8428770.3416059.1  密码:trsadmin\n该材料是一个视频，讲述的是基于强化学习的文本分类模型，其用于阅读理解中，感谢图像所王大东同学提供的资料。有兴趣的可以观看。\n\n\n\n"},"paper/readme.html":{"url":"paper/readme.html","title":"paper","keywords":"","body":"该部分主要是分享目前自然语言及知识图谱处理方法的基础知识，目前包含如下三个方面：自然语言处理基础技术，知识图谱构建，智能问答,论文整理将按照年份和板块来进行整理。\n\n2018\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding--ariv\n\nImproving Language Understanding by Generative Pre-Training\n\nDeep contextualized word representations\n\n\n"},"paper/nlp/readme.html":{"url":"paper/nlp/readme.html","title":"NLP","keywords":"","body":"目录\n\n2019年\n\n2018年\n研究性工作\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding--ariv\nAbstract： 本文作者推出了一套新的语言表达模型BERT，全称为Bidirectional Encoder Representations from Transformers。与近年来提出的语言模型不一样的地方在于，BERT不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的BERT模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改。Bert是一种概念上简单，但根据经验推断其具有强大的性能。BERT在 11项自然语言处理任务中都取得了目前最好的性能，具体包括将GLUE的基准值提升到了80.4%(7.6%的绝对提升)，在MultNLI中准确率有了86.7的提升（5.6%的绝对提升），在SQuAD v1.1的问答任务中其F1值达到了93.2（1.5%）的绝对提升，比人类的表现都搞了2.0的提升。       \nkeywords:  预训练，迁移学习\n\n\n\n研究性工作\n\nImproving Language Understanding by Generative Pre-Training\nAbstract： 本文作者推出了一套新的语言表达模型BERT，全称为Bidirectional Encoder Representations from Transformers。与近年来提出的语言模型不一样的地方在于，BERT不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的BERT模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改。Bert是一种概念上简单，但根据经验推断其具有强大的性能。BERT在 11项自然语言处理任务中都取得了目前最好的性能，具体包括将GLUE的基准值提升到了80.4%(7.6%的绝对提升)，在MultNLI中准确率有了86.7的提升（5.6%的绝对提升），在SQuAD v1.1的问答任务中其F1值达到了93.2（1.5%）的绝对提升，比人类的表现都搞了2.0的提升。       \nkeywords:  预训练，迁移学习\n\n\n\n研究性工作\n\nDeep contextualized word representations\nAbstract： 本文作者推出了一套新的语言表达模型BERT，全称为Bidirectional Encoder Representations from Transformers。与近年来提出的语言模型不一样的地方在于，BERT不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的BERT模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改。Bert是一种概念上简单，但根据经验推断其具有强大的性能。BERT在 11项自然语言处理任务中都取得了目前最好的性能，具体包括将GLUE的基准值提升到了80.4%(7.6%的绝对提升)，在MultNLI中准确率有了86.7的提升（5.6%的绝对提升），在SQuAD v1.1的问答任务中其F1值达到了93.2（1.5%）的绝对提升，比人类的表现都搞了2.0的提升。       \nkeywords:  预训练，迁移学习\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"url":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","title":"BERT Pre-training of Deep Bidirectional Transformers for Language Understanding","keywords":"","body":"\nBERT Pre-training of Deep Bidirectional Transformers for Language Understanding\n\n\n\n\n项目\n内容\n\n\n\n\n时间\n2018年\n\n\n作者单位\nGoogle\n\n\n作者信息\nJacobDevlin,Ming-Wei Chang, Kenton Lee, Kristina Toutanova\n\n\n源码链接\n\n\n\n原文链接\nhttps://arxiv.org/abs/1810.04805\n\n\n其他解读\nhttps://blog.csdn.net/jilrvrtrc/article/details/83829470\n\n\n关键词\n预训练，迁移学习\n\n\n\n摘要\nAbstract：本文作者推出了一套新的语言表达模型BERT，全称为Bidirectional Encoder Representations from Transformers。与近年来提出的语言模型不一样的地方在于，BERT不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的BERT模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改。Bert是一种概念上简单，但在实际中却有强大的性能的模型。BERT在 11项自然语言处理任务中都取得了目前最好的性能，具体包括将GLUE的基准值提升到了80.4%(7.6%的绝对提升)，在MultNLI中准确率达到86.7的（5.6%的绝对提升），在SQuAD v1.1的问答任务中其F1值达到了93.2（1.5%）的绝对提升，比人类的表现都搞了2.0的提升。   \n1.简介\n​    语言预训练模型已经被证明能有效提升自然语言处理任务的性能。这些任务包括句子级别的任务如语言推理及解析，这些任务其目的是从句子级别推断句子间相互关系。此外还包括一些序列级别的任务，如命名实体识别，文本理解挑战任务（SQuAD），这些任务模型中需要在序列级别产生经过微调后的更好的结果。\n​    目前有两种预训练的方法被用于下游处理任务中：基于特征的和基于微调（fine-tuning）的方法。在基于特征的方法中，ELMO(2018)是一种基于特征任务的模型，其包含一个预训练模型以及一些其他的特征。在微调模型中，OpenAI GPT(Generative Pre-trained Transform)引入了最小任务的参数，在下游任务中，其通过简单的微调的方式来调整模型的参数。在以前的工作中，所有的预训练方法都是通过使用同样的目标函数并通过双向语言模型来学习更好的通用语言表示。\n​    在这篇文章中，我们任务目前的技术严重限制了预训练在语言表达方面的能力，特别是针对微调的方法。最大的缺陷是目前的预训练模型都是双向的，其严格限制了模型在进行预训练时的选择能力。例如在OpenAIGP中，作者使用了从左到右的模型，使得句子中每个token只能被以前的词所关注。这种方式对句子级别的任务是次优的(sub-optimal)，而在针对一些token级别的任务中，如SQuAD的问答问题中，则这种方法有可能是毁灭性的，因此在这些任务中，纳入两个方面的信息是至关重要的。\n​    在这篇论文中，我们提出了一种基于微调的方法BERT:Bidirectional Encoder Representations from Transformers(基于双向翻译编码的表示模型)。BERT通过提出了两个新的预训练目标来解决以前的提到的单向限制问题：基于“masked”的语言模型（MLM），这种方法在1953年就曾经被提出。这种语言模型通过随机将输入句子中的token进行隐藏，然后预训练的目标就是基于上下文来预测出被覆盖掉的单词。不同于传统的从左到右的预训练语言模型，MLM训练目标使得我们能够充分利用左边和右边信息来训练一个更深的双向翻译模型。除了利用masked语言模型外，我们还在模型中引入了预测下一个句子的训练目标来同时训练句子对级别的表示模型。\n​    这篇文章的贡献如下：\n\n在这篇文章中我们证明了双向预训练模型的重要性。不同于传统的双向预测模型，BERT使用了一种基于masked的语言模型来训练一种更深的语言模型。此外区别于传统的浅层双向语言模型，BERT是一种使用深层次的双向语言模型。\n我们展示了预训练模块可以消除在很多任务中需要依赖严重特征工程的任务。BERT 是第一个基于微调方法的语言表示模型，并且在sentence-levle和token-level级别的任务中都取得了较好的效果。在很多有特定需要的任务中也取得了较好的效果。\nBERT 在11项自然语言处理任务中都取得了SOTA的结果。表明双向语言模型在文本处理中的重要性。\n\n2.相关工作\n​    在这节中我们介绍基于预训练的方式来生成语言表示模型的相关方方法，同时简要介绍一下在这个领域目前最流行的方法。\n2.1 基于特征的方法\n​    基于学习的词表示方法在多个领域都有广泛的应用。其中包括非神经元的方法和基于神经网络的方法。基于词嵌表示的预训练方法目前是现代语言处理系统中的一个重要集成部分，其对系统的后续处理提供了有强大的提升。除了词级别的词嵌外，这些方法也被推广到粗粒度的句子级别词嵌以及锻炼词嵌中。在传统的方法中，这些学习到的表示方法往往作为下游任务的特征输入到下游模型中。ELMo(Peters et al.,2017)从另外一个重要角度来对词嵌进行处理。在该方法中，作者提出了从语言模型中提取出与上下文敏感特征的方法。通过将基于上下文敏感的词嵌和特定的任务结构相结合后，ELMo在很多自然语言处理任务中都取得了SOAT(State of the ART)的实验效果，包括基于SQuAD的问题，语义分析以及命名识别识别等众多的任务。\n2.2 基于微调的方法\n​    语言表示模型的一个新趋势就是通过利用语言模型预训练一个模型，让后基于迁移学习的方法将其迁移到其他的下游任务中，在下游任务中在对这些模型的参数进行微调。这些方法的一个优势就是只有较少的参数需要进行重新学习。在这方面的工作中，最新的一个工作OpenAI GPT2018基于迁移学习的方法在许多句子级别的任务中取得了SOTA的实验结果。\n2.3 基于监督数据的迁移学习方法\n​    无监督学习预训练方法的优势是有大量的无标签的数据可以大量的获取，基于有标签数据的迁移学习目前也被证明在许多文本处理任务中有较好的结果，例如自然语言推理、机器翻译等任务。除了在NLP处理领域外，很多机器视觉领域的研究也说明了预训练并集合迁移学习的重要方，这些方法通过在大量的预训练数据上进行训练后再通过微调的方式可以取得较好的实验结果。\n3 . Bert模型介绍\n​    在第三节中，详细介绍BERT模型机器实现细节。该节首选叙述Bert模块的整体框架及Bert的输入表示。接着在3.3节中介绍预训练的任务以及核心的创新。3.4节中介绍预训练的流程，而在3.5节中介绍微调的方法。最后在3.6节中讨论Bert和OpenAI GPT等结构的差别。\n3.1 模块的结构\n​    BERT是一个多层的基于双向转移编码器的模型，其根据tensor2tensor的方式来实践。BERT使用的“Transformer Encoder”编码器近来得到广泛的应用，因此本文中没有进行详细的介绍。\n​    在该项工作中，作者定义了如下变量：\n​    L:表示层级的数量（Transformer blocks）\n​    H ：表示隐藏层数量\n *A*:表示注意力header的数量。\n​    bert有两个版本：BERTbase和BERTlarge,其中BERTbase的参数如下：L=12,H=768,A=12,而BERTlarge的数据L=24,H=1024,A=16.因此BERT base版本的参数有110M个，而BERT 的large版本的参数为340M个。\n​    在BERT中，BERT使用的是一个双向的自注意力机制的编码模型，而OpenAI GPT中则是使用的一种从左到右的编码模型，引起BERT可以被看做迁移学习编码器，而OpenAI GPT则是一种迁移学习的解码器。BERT和OpenAI GPT的区别可以如图所示\n## 3.2 BERT的输入表示\n​    在bert的输入中，对于一个输入token序列，该输入句子的序列表示可以由几部分构成：token序列表示，segment和position 的词嵌表示。图2中表示的是BERT的输入表示。\n在BERT中输入被表示为两种形式：\n\nToken Embedding: 使用了包含30,000token的词库的WordPiece embedding方法来实现词嵌表示。\nPositional Embedding:使用了支持长度为512个token的词嵌表示方法。\n\n​    在输入词序列中，序列的第一个词都是以[CLS]的方式开头，针对该token的 最后隐藏状态被用在分类任务的聚合表示中。而如果针对的是非分类任务，则该词对应的向量被删除掉。在句子对中，所有句子都被表示到一个序列任务中，我们通过两种不同的方式来区分。首先通过一个特殊的token[ESP]来对两个句子进行区分，在第一个句子中，每个句子的token会加上句子A的embedding表示，而在句子B中，每个token都会加上句子B的embedding表示。\n​    如果输入是单个句子，则直接使用句子A的词嵌表示。\n3.3 预训练任务\n​    不像传统的从左到右或者从右到左的预训练语言模型任务，在BERT中使用了两种原创的无监督学习的预训练任务，分别是Masked LM和Next Prediction模型。\n3.3.1  Masked LM\n​    从初始概念上来将，一个更深层次的双向模型比传统的从左到有或者从右到左基于简单讲从左到右或者从右到左模型进行组合的方法有更好的效果。然后，目前传统的语言模型则往往只能通过从左到右或者从右到左的模型进行训练。在双向的环境中，句子中任意一个token都有可能被预测到在一个多层的网络环境中。\n​    为了能够训练一个具有双向深层的表示模型，在bert中采用了一种较为直观的模型，通过随机隐藏掉句子中的token，并让模型能对该句子进行正确的预测。这种模型在BERT中被称为\"masked LM\"(MLM)模型。在该模型中，被隐藏掉的token会被输入到softmax模型中。在所有的预训练实验中，BERT采用15%的概率随机隐藏掉输入句子中的token. 区别于传统的auto-encoder模型，BERT的 训练任务只是预测被隐藏掉的单词而不是重新构建整个句子。\n​    由于BERT会通过再训练的方式来微调模型的参数，而在再训练模型中，由于不在存在[MASK]信息，因此在训练中并不会隐藏掉所有的mask，而是通过以15%概率来选择那些单词被隐藏掉。此外在选择的被隐藏的单子中，采用如下措施来进行选择。\n\n不是所有被选择的单词都会被隐藏掉，而是其中80%的可能会隐藏掉该单词，10%的时间将该单词替换成一个随机的单词。而另外还是有10的时间是该单词保持不变。\n\n  ​    在Transformer模型中，由于encoder并不知道哪些单词可能会被替换成随机的单词，因此要求编码器能够学习到一种对所有单词都有表现的能力，此外随机替换只发在在1.5%的概率中，因此不会影响模型的整体理解能力。\n  ​    由于在BERT中，有15%的 单词可能会被隐藏掉，因此在训练过程中其会花费较多的时间。\n3.3.2 Next Sentence Predictioni\n  ​    在 许多自然语言处理任务中，如问题回答，语言推理都依赖于推断两个句子的关系，而这种关系在部分的文本预处理模型中都没有捕获。为了能够训练一个模型能够对两个句子的关系进行推理，在BERT的预训练任务中，引入对对下一个句子预测的训练目标。在训练中，在一个句子对（sentence A,sentence B）中，一个句子B有50%是句子A的下一句，而50%的可能是随机另外一个句子。\n  \n  ​    在构建训练集时候，在选择句子A的下一个句子时，通过随机选择下一个句子的方式来实现构建训练集。\n3.4 预训练流程\n  ​    BERT的预训练过程和目前已有的方法类似，其再训练语料我们使用了两部分的语料：BookCorpus(800M words)和英语维基百科(2,500M 单词)。在维基百科语料中，我们目前只提取了文本消息而忽略了列表，表格以及头等信息。在实际训练中，使用文本级别的语料比句子级别的语料可以取得更好的实验结果。为了生存训练输入序列，我们从语料库中选择了两个文本句子并将其合并成一个句子。其中第一个句子使用句子A的embedding，而第二个句子使用句子B的embedding。在生成句子的时候，句子B的embedding有50%的可能是随机选择的，用于训练模型对下一个句子的预测。两个句子合并起来的token序列长度小于512个token。句子首先使用WordPiece的方法生成输入表示并在其中随机挑选15%来隐藏掉，然后在此基础上使用LM masking模型。\n  ​    在预训练中，在每个epoch中，输入句子数量为256个句子，每个epoch迭代1,000,000次，最终没3.3亿个单词上训练了差不多40个epoch. 我们使用饿了Adam优化算法，其中学习率为1e-4,b1参数为0.9，b2参数为0.999，L2范数的权重为0.01. 在每层中我们设置dropout概率为0.1，激活函数选择的是gelu。训练的损失函数LM模型的平均likelihood的和以及预测下一个句子的平均likelihood.\n  ​    在训练BERTbase的过程中，作者在配置有4Cloud TPUs的集群上进行了训练，而BERTlarge则是在配置为16Cloud TPs的默契上进行了训练，每个训练都花费了4天左右完成。\n3.5 微调过程\n  ​    对于句子级别的分类任务，BERT的微调是很直接的。为了获取输入句子的固定维度的表示,BERT使用了输入句子的第一个词的最后一个隐藏层的输入C（RH）。在进行分类时，在bert输出的基础上在增加一个分类层，分类层的矩阵表示为W。 则分类结果接上一个SoftMax层后得到相应的各个类别的概率P=softmax(CWT)。这样所有的BERT的参数和新增的W参数一起经过微调后得到分类结果的输出。而在针对单词级别的任务中，训练过程和句子级别的有所区别。\n  ​    在微调训练过程中，大部分模型的参数和原来预训练的过程是一样的。除了batch size，学习率(learning rate)以及训练的epoch次数不同外。在预训练过程中，dropout的概率一直设置为0.1，而新增参数的相关设置是和任务先关的。作者在训练BERT的过程中，发现如下参数在多个任务中具有较好的表现。\n\nBatch size:16,32\n\nLearning rate(Adam):5e-5,3e-5,2e-5\n\nNumber of epochs:3,4\n在实验中，作者同时发现100k+左右的数据对于模型的参数的变化是极小的。因此微调过程是十分快速的。\n3.6 和OpenAIGPT的比较\n​    目前在预训练领域可以用于和BERT进行比较的是OpenAIGPT,OpenAIGPT使用从左到右的方式在大量语料上进行训练。在整体结构上BERT和OpenAIGPT是相似的，因此整体结构差异较小，但是BERT和OpenAIGPT还有其他几个方面的差别。\n\nBERT的训练语料是在一个BookCorpus(800M)和维基百科的文本语料中（2,500M个单词）。\nGPT使用的句子分隔符为[SEP]，分类器的分隔符为[CLS],并且这些只在微调过程中使用。而BERT则是学习了[SEP],[CLS]以及句子分割信息（A/B）在预训练过程中。\nGPT在包含32,000个单词的语料库中每个epoch训练了1M 步骤，而BERT是在128,000个单词中每个epoch训练了1M步。\nGPT在所有的微调步骤中都使用了相同的学习率5e-5,而BERT的学习率则是根据特定任务需求设置来进行微调的。\n\n\n\n4.实验及结果\n​    实验部分主要叙述BERT在11项自然语言预处理任务中 的微调过程。\n4.1 GLUE数据集\n​    GLUE数据集是一个通用的自然语言理解评估数据集，用于评估对自然语言的理解任务。GLUE数据集包含多个子数据集。\n\nMNLI 多类型的自然语言推理任务数据集。\nQQP QQP问题对是一个二进制分类任务，用于确定连个问题的语义是否是相等的。\nQNLI 是一个自然语言推理任务数据集，其中的正样本是一些问题，答案对的句子则是来自于同一个段落的句子，其并不形成问题-答案对的形式。\nSST-2 是一个有斯坦福构建的二分类任务的数据集，是从一些电影评论中抽取出来的句子并且包含人工的标注信息。\nCoLA 该数据集是一个语言课接受性的二分类语料库，其预测的任务是一个英语句子在语言上是否是可以接受的。\nSTS-B 该数据集是一个文本语义相似性数据集，其收集与一些新闻标题和其他来源，其预测任务是预测两个句子在语义上的多大的相似性。\nMRPC,该数据集微软研究解释语料库由自动提取的句子对组成来自在线新闻来源，带有人工注释判断两个句子的语义是否一致相等的\nRTE 该数据集是一个二分类的任务，其任务目标类似于MNLI,但是其训练数据较少。\nWNLI ,Winograd NLI是一个小型的自然语言推断数据集，在GLUE的网页中指出该数据集在构造上存在一定我的问题。因此在模型中排除了OpenAIGPT。\n\n4.1.1 GLUE数据集的实验结果\n​    在针对GLUE数据集的微调训练中，使用了每个句子的第一个输入[CLS]的最后一个隐藏层输出C 以及分类矩阵W相结合并经过softmax分了的方式来进行微调。在每个任务中，使用的学习率为5e-5,4e-43e-5等。实验结果如下图所示\n​    从实验结果可以，BERT相比较于OpenAI GPT,BiLSTM+ELMo+Attn等模型都有很大的提升，其中BERTlarge和BERTbase比起来有更多的提升。\n4.2 SQuAD v1.1\n​    SQqAD 数据集是一个由斯坦福收集的包含100k的问题/答案对数据集。该数据集是通过给定一个问题以及维基百科中包含该答案的段落，该任务从段落中预测出相应的答案。其简单示例如下：\n\n​    在该类型的任务中，和传统的句子分类任务有明显的不同，在对BERT进行微调的任务中，将问题和答案都表示成两个独立的序列。其中问题使用A embedding，而答案使用B embedding。在训练中，针对第i个单词，通过将其bert输出的词嵌表示和一个概率转移矩阵T相乘后利用softmax输出得到该但是是否是答案开始的概率。针对答案的结束，也是使用同样的方式预测某个词是否是答案的结果，而训练的目标是最大化正确开始位置和结束位置的释然概率。该过程的示意图如图所示：\n\n​    在该任务中，训练时使用的额学习率为5e-5,batch_size 为32，训练迭代次数为3个epoch.实验结果如下：\n\n​    从实验结果中可以看到，BERT去了的最好的结果，其中BERTlarge 结合一个单个模型的效果比传统的方法1.5的F1值的提升。\n命名实体识别\n​    为了评估bert在序列标注任务中的性能，论文还基于bert在CoNLL 2003的命名实体识别任务中进行了测试。该数据集包含200K的训练数据集，其中实体类别为Person（人物）,Organization（组织）,Location（地点），Miscellaneous（其他项）和Other(非实体词)。\n​    在进行命名实体识别的微调时时，针对每个词，使用其最后一个隐藏状态C 经过和一个转移矩阵T转移后在利用一个分类网络来进行分类，微调的过程如下图所示：\n\n​    实验结果如图所示：\n\n​    从实验结果可以看出，BERT也去了的SOTA的实验结果。\n4.4 常识推理任务（SWAG 数据集）\n​    该任务是在一个有对抗生成网络生成的数据集（包含113k句子）上进行的实验任务，其任务是给定一个视频字幕中的一句话，推断最由可能的后续情节。其简单示意如下图所示：\n\n​        该任务的微调过程和GLUE是类似的，在每个例子中，我们构建四个输入句子，其中每个都是一个数据句子A和可能后续进展B的结合。我们同样也引入一个转移矩阵T,通过将每个输入和矩阵T进行相乘后利用softmax来预测概率。在进行微调训练中，使用的学习速率为2e-5,总共迭代三个epoch.其对比的BaseLine为ESIM+Glove和ESIM+ELMo.实验结果如下：\n\n​    从实验结果中，可以看出该模型可以取得较好的实验结果。此外作者还对模型的一些相关参数进行了分析并进行了实验讨论。\n总结\n​    最近的经验改进表明，由于转移学习与语言模型已经证明，丰富，无监督的预培训是许多语言理解系统的一个组成部分。特别是，这些结果使得即使是低资源任务也能从非常深的单向架构中受益。我们的主要贡献是将这些发现进一步推广到深层双向体系结构中，使相同的预先培训的模型能够成功地处理广泛的NLP任务。虽然经验结果很强，但在某些情况下，超过了人类的表现，未来的重要工作是研究语言现象。\n​      \n"},"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html":{"url":"paper/nlp/papers/Improving Language Understanding by Generative Pre-Training.html","title":"Improving Language Understanding by Generative Pre-Training","keywords":"","body":"\nImproving Language Understanding by Generative Pre-Training\n\n\n\n\n项目\n内容\n\n\n\n\n时间\n2018年\n\n\n作者单位\nGoogle\n\n\n作者信息\nJacobDevlin,Ming-Wei Chang, Kenton Lee, Kristina Toutanova\n\n\n源码链接\n\n\n\n原文链接\nhttps://arxiv.org/abs/1810.04805\n\n\n其他解读\nhttps://blog.csdn.net/jilrvrtrc/article/details/83829470\n\n\n关键词\n预训练，迁移学习\n\n\n\n"},"paper/nlp/papers/Deep contextualized word representations.html":{"url":"paper/nlp/papers/Deep contextualized word representations.html","title":"Deep contextualized word representations","keywords":"","body":"\nDeep contextualized word representations\n\n\n\n\n项目\n内容\n\n\n\n\n时间\n2018年\n\n\n作者单位\nGoogle\n\n\n作者信息\nJacobDevlin,Ming-Wei Chang, Kenton Lee, Kristina Toutanova\n\n\n源码链接\n\n\n\n原文链接\nhttps://arxiv.org/abs/1810.04805\n\n\n其他解读\nhttps://blog.csdn.net/jilrvrtrc/article/details/83829470\n\n\n关键词\n预训练，迁移学习\n\n\n\n"},"paper/KnowledgeBase/readme.html":{"url":"paper/KnowledgeBase/readme.html","title":"KnowledgeBase","keywords":"","body":""}}}