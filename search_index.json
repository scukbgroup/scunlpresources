{"index":{"version":"0.5.12","fields":[{"name":"title","boost":10},{"name":"keywords","boost":15},{"name":"body","boost":1}],"ref":"url","documentStore":{"store":{"./":["introduct","分析解读最新的自然语言处理方面的论文，供大家一起进行交流和学习。","将相关的知识进行整理后分享，方便后面的人学习","将自然语言处理方面的工具进行整理汇总","工具整理","收集整理相关成员上传的资料及相应的数据集，方便有需要的同学进行下载。","收集整理自然语言处理领域的会议及期刊，方便大家进行相关的查阅。","数据集","知识分享","自然语言处理会议及期刊","论文解读","该博客主要收集整理与自然语言处理相关工具，论文以及代码等资源信息，对目前自然语言处理最前沿的技术、论文及相关相关代码进行整理后进行分享，主要关注点包括自然语言处理基础方法，知识图谱构建，智能问答等几个方面的知识。通过不断地积累让大家在这条路上越走越远。"],"metting/summit metting.html":["aaai,全称是associ","ac(lth","acl","acl的全称是th","acl（tacl），此外还有一些期刊与自然语言处理相关。如tslp(（acm","acm","acm）主办，包括如下几个会议。","advanc","ai","ai,artifici","aistat","american","approach","artifici","asia","asian","associ","association)组织","a类","base","b类","ccf","chapter","cikm,该会议名称缩写和上一个相同，但是其全称为intern","cikmm,其全称是confer","cole","coling(intern","committe","comput","confer","conference，也是有关信息检索及数据挖掘。","conll","conll(confer","c类","data","data&corpu","emnlp","emnlp(confer","emnlp是有acl下面的比较著名的兴趣小组（speci","empir","evaluation,其由欧洲语言资源组织进行elra(european","evaluation,该会议由acl的特殊兴趣小组siglex进行组织，每年都会举办，国内也有很多研究机构及公司参与，如哈工大科大讯飞等,其最新的链接网址如下：http://alt.qcri.org/semeval2019/index.php?id=task","group","groups,sigs）之一的sigdata(speci","hearst正式宣布成立国际计算语言学学会亚太地区分会（aacl，th","http://coling2018.org","http://emnlp2018.org","http://naacl.org","http://www.conll.org/","https://blog.csdn.net/lyb3b3b/article/details/83548964","https://www.aclweb.org/port","https://www.cnblogs.com/niuxichuan/p/7602012.html","icml,全称是","ijcai,全称是intern","inform","intellig","intelligence，主要关注人工智能领域的最新研究进展，其中也有大量的人工智能相关的知识。","intelligence，也是关注人工智能领域的另外一个重要学术会议。","interest","intern","jair,journ","jmlr,","joint","journal","knowledg","languag","learn","learnin)","learning)举办的。其也是每年举办一次，由于naacl是acl在北美的分会，因此当acl在北美举办的时候，naacl就会停办一年。","linguist","linguistics)","linguistics)组织的。该会议每两年举办一次。","linguistics,其由1965年创办，是由老牌的nlp学术会议组织iccl(th","linguistics,翻译过来是计算机语言协会，自然语言处理与计算语言学领域（以下简称nlp/cl）最权威的国际专业学会，acl成立于1962年，是自然语言处理(nlp)领域影响力最大、最具活力的顶级国际学术组织，每年举办一次。这个学会主办了","linguistics,该会议是acl在北美的分会，也是有acl主办。其是有acl下面的兴趣小组signal(speci","linguistics和transact","linguistics）。此次成立acl亚太分会，将进一步促进亚太地区nlp相关技术和研究的发展。据悉，首届aacl会议预计在2020年举行，此后将每两年举行一次。","lrec","lrec全称是intern","machin","machinery,","management，其关注信息管理","method","mining,由名称可知，该会议关注搜索和数据挖掘。","naacl","naacl(th","naacl的全称是th","natur","nc,neurocompt","neural","nip","nlp/cl","nlp相关的其他国际会议","north","pacif","process","processing)","processing)),其他相关期刊及投稿链接如下：","processing)举办的。","processing,nlp）领域的一些著名会议及期刊，将按照ccf对会议的分级标准对相关会议进行整理，介绍每个会议的关注主题，召开周期、会议网站及相关的信息，方便后面查找最新的相关论文，从而对该领域进行比较深入的研究。每个会议及期刊都有相关的入口链接，方便进行查看。","processing.","processing）)以及talip((acm","research","resourc","retrieval,其主要关注信息检索。","search","semant","semev","semeval,其全称是intern","sigir,其全称是speci","speech","statist","system","transact","uai","uncertainti","web","wide","workshop","world","wsdm,该会议全称为web","人工智能领域与自然语言处理相关的会议","以下是根据ccf2015年的对自然语言处理领域的会议的分级标准来进行罗列和整理","会议名称","会议网站","全称","全称是confer","全称是intern","出版社","分类等级","参考文献","召开周期","国际会议","在国际上acl，coling，enmlp和naacl是默认的四大自然语言处理顶级学术会议，其中acl，emnlp和naacl都是由acl及相应的子组织举办的。","在自然语言处理中，一般来将，大家更关注学术会议，其主要原因是发表周期短，通过会议也可以进行深入的交流。但自然语言处理领域也有自己的学术期刊，其旗舰期刊有两个，分别是comput","在自然语言处理中，机器学习也是其主流的方法，在机器学习领域相关的学术会议包括icml,nips,uai及aistats,下面将简单介绍。","处理和人工智能是密切相关的，其是人工智能研究的重要内容，人工智能研究的两大国际顶会是aaai和ijcai","期刊","本文介绍自然语言处理（natur","机器学习领域中与自然语言处理相关的顶级会议","每两年一次","每年一次","自然语言","除了上述被ccf收录的会议外，在自然语言处理领域还有其他许多重要会议，分别是semeval,lrec等。","除了直接与自然语言处理相关外，还有其他许多与自然语言处理相关的学术会议，包括信息检索，数据挖掘及人工智能领域，这些都是属于自然语言处理的应用领域。其中信息检索和数据挖掘与自然语言处理是密切相关的，主要由美国计算机学会（associ","领域最权威的国际会议，即acl年会。1982年和1999年，acl分别成立了欧洲分会（eacl）和北美分会（naacl）两个区域性分会。近年来，亚太地区在自然语言处理方面的研究进步显著，2018年7月15日，第56届acl年会在澳大利亚墨尔本举行。开幕仪式上，acl主席marti"],"tools/readme.html":["tool"],"database/readme.html":["classif","databas","decis","https://v.youku.com/v_show/id_xnda4mdyznzgwoa==.html?spm=a2h3j.8428770.3416059.1","jumper:learn","make","read","下载地址：链接：https://pan.baidu.com/s/1nmh33yk80sznki8vhmeeea","分享者：刘露平","密码:trsadmin","时间：2019年3月27日","时间：2019年3月28日","概述：该ppt是做汇报时候的ppt，讲了知识图谱的基本知识及实体和关系抽取相关论文，有需要可以下载。","知识图谱概述ppt","该材料是一个视频，讲述的是基于强化学习的文本分类模型，其用于阅读理解中，感谢图像所王大东同学提供的资料。有兴趣的可以观看。","该页面收集整理日常工作中相关的数据集和资料，大家可以通过将相关资料放到网盘或者其他github仓库中，然后这里放上相应的链接。","（永久有效）提取码：mdan"],"paper/readme.html":["2019","ariv","bert:","bidirect","deep","languag","paper","pre","train","transform","understand","该部分主要是分享目前自然语言及知识图谱处理方法的基础知识，目前包含如下三个方面：自然语言处理基础技术，知识图谱构建，智能问答,论文整理将按照年份和板块来进行整理。"],"paper/nlp/readme.html":["11项自然语言处理任务中都取得了目前最好的性能，具体包括将glue的基准值提升到了80.4%(7.6%的绝对提升)，在multnli中准确率有了86.7的提升（5.6%的绝对提升），在squad","2018年","2019年","abstract：","ariv","bert:","bidirect","deep","encod","keywords:","languag","nlp","pre","represent","train","transform","transformers。与近年来提出的语言模型不一样的地方在于，bert不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的bert模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改。bert是一种概念上简单，但根据经验推断其具有强大的性能。bert在","understand","v1.1的问答任务中其f1值达到了93.2（1.5%）的绝对提升，比人类的表现都搞了2.0的提升。","本文作者推出了一套新的语言表达模型bert，全称为bidirect","目录","研究性工作","预训练，迁移学习"],"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":["##","*a*:表示注意力header的数量。",".","1.简介","11项自然语言处理任务中都取得了目前最好的性能，具体包括将glue的基准值提升到了80.4%(7.6%的绝对提升)，在multnli中准确率达到86.7的（5.6%的绝对提升），在squad","2.1","2.2","2.3","2.相关工作","2018年","3","3.1","3.2","3.3","3.3.1","3.3.2","3.4","a,sent","abstract：本文作者推出了一套新的语言表达模型bert，全称为bidirect","al.,2017)从另外一个重要角度来对词嵌进行处理。在该方法中，作者提出了从语言模型中提取出与上下文敏感特征的方法。通过将基于上下文敏感的词嵌和特定的任务结构相结合后，elmo在很多自然语言处理任务中都取得了soat(st","art)的实验效果，包括基于squad的问题，语义分析以及命名识别识别等众多的任务。","base版本的参数有110m个，而bert","bert","bert是一个多层的基于双向转移编码器的模型，其根据tensor2tensor的方式来实践。bert使用的“transform","bert有两个版本：bertbase和bertlarge,其中bertbase的参数如下：l=12,h=768,a=12,而bertlarge的数据l=24,h=1024,a=16.因此bert","bert模型介绍","bert的输入表示","bidirect","blocks）","b）中，一个句子b有50%是句子a的下一句，而50%的可能是随机另外一个句子。","chang,","deep","embedding:","embedding:使用了支持长度为512个token的词嵌表示方法。","embedding方法来实现词嵌表示。","encod","encoder”编码器近来得到广泛的应用，因此本文中没有进行详细的介绍。","encoder模型，bert的","et","googl","gpt(gener","gpt2018基于迁移学习的方法在许多句子级别的任务中取得了sota的实验结果。","gpt中则是使用的一种从左到右的编码模型，引起bert可以被看做迁移学习编码器，而openai","gpt则是一种迁移学习的解码器。bert和openai","gpt的区别可以如图所示","gpt等结构的差别。","h","https://arxiv.org/abs/1810.04805","https://blog.csdn.net/jilrvrtrc/article/details/83829470","jacobdevlin,m","kenton","kristina","l:表示层级的数量（transform","languag","lee,","level级别的任务中都取得了较好的效果。在很多有特定需要的任务中也取得了较好的效果。","levle和token","lm","lm\"(mlm)模型。在该模型中，被隐藏掉的token会被输入到softmax模型中。在所有的预训练实验中，bert采用15%的概率随机隐藏掉输入句子中的token.","lm和next","mask","next","optimal)，而在针对一些token级别的任务中，如squad的问答问题中，则这种方法有可能是毁灭性的，因此在这些任务中，纳入两个方面的信息是至关重要的。","posit","pre","predictioni","prediction模型。","represent","sentenc","token","toutanova","train","transform","transform)引入了最小任务的参数，在下游任务中，其通过简单的微调的方式来调整模型的参数。在以前的工作中，所有的预训练方法都是通过使用同样的目标函数并通过双向语言模型来学习更好的通用语言表示。","transformers(基于双向翻译编码的表示模型)。bert通过提出了两个新的预训练目标来解决以前的提到的单向限制问题：基于“masked”的语言模型（mlm），这种方法在1953年就曾经被提出。这种语言模型通过随机将输入句子中的token进行隐藏，然后预训练的目标就是基于上下文来预测出被覆盖掉的单词。不同于传统的从左到右的预训练语言模型，mlm训练目标使得我们能够充分利用左边和右边信息来训练一个更深的双向翻译模型。除了利用masked语言模型外，我们还在模型中引入了预测下一个句子的训练目标来同时训练句子对级别的表示模型。","transformers。与近年来提出的语言模型不一样的地方在于，bert不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的bert模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改。bert是一种概念上简单，但在实际中却有强大的性能的模型。bert在","tuning）的方法。在基于特征的方法中，elmo(2018)是一种基于特征任务的模型，其包含一个预训练模型以及一些其他的特征。在微调模型中，openai","understand","v1.1的问答任务中其f1值达到了93.2（1.5%）的绝对提升，比人类的表现都搞了2.0的提升。","wei","​","不像传统的从左到右或者从右到左的预训练语言模型任务，在bert中使用了两种原创的无监督学习的预训练任务，分别是mask","不是所有被选择的单词都会被隐藏掉，而是其中80%的可能会隐藏掉该单词，10%的时间将该单词替换成一个随机的单词。而另外还是有10的时间是该单词保持不变。","为了能够训练一个具有双向深层的表示模型，在bert中采用了一种较为直观的模型，通过随机隐藏掉句子中的token，并让模型能对该句子进行正确的预测。这种模型在bert中被称为\"mask","从初始概念上来将，一个更深层次的双向模型比传统的从左到有或者从右到左基于简单讲从左到右或者从右到左模型进行组合的方法有更好的效果。然后，目前传统的语言模型则往往只能通过从左到右或者从右到左的模型进行训练。在双向的环境中，句子中任意一个token都有可能被预测到在一个多层的网络环境中。","作者信息","作者单位","使用了包含30,000token的词库的wordpiec","关键词","其他解读","内容","再训练流程","区别于传统的auto","单词可能会被隐藏掉，因此在训练过程中其会花费较多的时间。","原文链接","在","在11项自然语言处理任务中都取得了sota的结果。表明双向语言模型在文本处理中的重要性。","在bert中输入被表示为两种形式：","在bert中，bert使用的是一个双向的自注意力机制的编码模型，而openai","在bert的输入中，对于一个输入token序列，该输入句子的序列表示可以由几部分构成：token序列表示，segment和posit","在transformer模型中，由于encoder并不知道哪些单词可能会被替换成随机的单词，因此要求编码器能够学习到一种对所有单词都有表现的能力，此外随机替换只发在在1.5%的概率中，因此不会影响模型的整体理解能力。","在构建训练集时候，在选择句子a的下一个句子时，通过随机选择下一个句子的方式来实现构建训练集。","在第三节中，详细介绍bert模型机器实现细节。该节首选叙述bert模块的整体框架及bert的输入表示。接着在3.3节中介绍预训练的任务以及核心的创新。3.4节中介绍预训练的流程，而在3.5节中介绍微调的方法。最后在3.6节中讨论bert和openai","在该项工作中，作者定义了如下变量：","在输入词序列中，序列的第一个词都是以[cls]的方式开头，针对该token的","在这篇文章中我们证明了双向预训练模型的重要性。不同于传统的双向预测模型，bert使用了一种基于masked的语言模型来训练一种更深的语言模型。此外区别于传统的浅层双向语言模型，bert是一种使用深层次的双向语言模型。","在这篇文章中，我们任务目前的技术严重限制了预训练在语言表达方面的能力，特别是针对微调的方法。最大的缺陷是目前的预训练模型都是双向的，其严格限制了模型在进行预训练时的选择能力。例如在openaigp中，作者使用了从左到右的模型，使得句子中每个token只能被以前的词所关注。这种方式对句子级别的任务是次优的(sub","在这篇论文中，我们提出了一种基于微调的方法bert:bidirect","在这节中我们介绍基于预训练的方式来生成语言表示模型的相关方方法，同时简要介绍一下在这个领域目前最流行的方法。","基于学习的词表示方法在多个领域都有广泛的应用。其中包括非神经元的方法和基于神经网络的方法。基于词嵌表示的预训练方法目前是现代语言处理系统中的一个重要集成部分，其对系统的后续处理提供了有强大的提升。除了词级别的词嵌外，这些方法也被推广到粗粒度的句子级别词嵌以及锻炼词嵌中。在传统的方法中，这些学习到的表示方法往往作为下游任务的特征输入到下游模型中。elmo(pet","基于微调的方法","基于特征的方法","基于监督数据的迁移学习方法","如果输入是单个句子，则直接使用句子a的词嵌表示。","实验及结果","总结","我们展示了预训练模块可以消除在很多任务中需要依赖严重特征工程的任务。bert","摘要","无监督学习预训练方法的优势是有大量的无标签的数据可以大量的获取，基于有标签数据的迁移学习目前也被证明在许多文本处理任务中有较好的结果，例如自然语言推理、机器翻译等任务。除了在nlp处理领域外，很多机器视觉领域的研究也说明了预训练并集合迁移学习的重要方，这些方法通过在大量的预训练数据上进行训练后再通过微调的方式可以取得较好的实验结果。","时间","是第一个基于微调方法的语言表示模型，并且在sentenc","最后隐藏状态被用在分类任务的聚合表示中。而如果针对的是非分类任务，则该词对应的向量被删除掉。在句子对中，所有句子都被表示到一个序列任务中，我们通过两种不同的方式来区分。首先通过一个特殊的token[esp]来对两个句子进行区分，在第一个句子中，每个句子的token会加上句子a的embedding表示，而在句子b中，每个token都会加上句子b的embedding表示。","模块的结构","源码链接","由于bert会通过再训练的方式来微调模型的参数，而在再训练模型中，由于不在存在[mask]信息，因此在训练中并不会隐藏掉所有的mask，而是通过以15%概率来选择那些单词被隐藏掉。此外在选择的被隐藏的单子中，采用如下措施来进行选择。","由于在bert中，有15%的","的large版本的参数为340m个。","的词嵌表示。图2中表示的是bert的输入表示。","目前有两种预训练的方法被用于下游处理任务中：基于特征的和基于微调（fine","训练任务只是预测被隐藏掉的单词而不是重新构建整个句子。","许多自然语言处理任务中，如问题回答，语言推理都依赖于推断两个句子的关系，而这种关系在部分的文本预处理模型中都没有捕获。为了能够训练一个模型能够对两个句子的关系进行推理，在bert的预训练任务中，引入对对下一个句子预测的训练目标。在训练中，在一个句子对（sent","语言表示模型的一个新趋势就是通过利用语言模型预训练一个模型，让后基于迁移学习的方法将其迁移到其他的下游任务中，在下游任务中在对这些模型的参数进行微调。这些方法的一个优势就是只有较少的参数需要进行重新学习。在这方面的工作中，最新的一个工作openai","语言预训练模型已经被证明能有效提升自然语言处理任务的性能。这些任务包括句子级别的任务如语言推理及解析，这些任务其目的是从句子级别推断句子间相互关系。此外还包括一些序列级别的任务，如命名实体识别，文本理解挑战任务（squad），这些任务模型中需要在序列级别产生经过微调后的更好的结果。","这篇文章的贡献如下：","项目","预训练任务","预训练，迁移学习","：表示隐藏层数量"],"paper/KnowledgeBase/readme.html":["knowledgebas"]},"length":8},"tokenStore":{"root":{"1":{"1":{"docs":{},"项":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"都":{"docs":{},"取":{"docs":{},"得":{"docs":{},"了":{"docs":{},"目":{"docs":{},"前":{"docs":{},"最":{"docs":{},"好":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{},"，":{"docs":{},"具":{"docs":{},"体":{"docs":{},"包":{"docs":{},"括":{"docs":{},"将":{"docs":{},"g":{"docs":{},"l":{"docs":{},"u":{"docs":{},"e":{"docs":{},"的":{"docs":{},"基":{"docs":{},"准":{"docs":{},"值":{"docs":{},"提":{"docs":{},"升":{"docs":{},"到":{"docs":{},"了":{"8":{"0":{"docs":{},".":{"4":{"docs":{},"%":{"docs":{},"(":{"7":{"docs":{},".":{"6":{"docs":{},"%":{"docs":{},"的":{"docs":{},"绝":{"docs":{},"对":{"docs":{},"提":{"docs":{},"升":{"docs":{},")":{"docs":{},"，":{"docs":{},"在":{"docs":{},"m":{"docs":{},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{},"n":{"docs":{},"l":{"docs":{},"i":{"docs":{},"中":{"docs":{},"准":{"docs":{},"确":{"docs":{},"率":{"docs":{},"有":{"docs":{},"了":{"8":{"6":{"docs":{},".":{"7":{"docs":{},"的":{"docs":{},"提":{"docs":{},"升":{"docs":{},"（":{"5":{"docs":{},".":{"6":{"docs":{},"%":{"docs":{},"的":{"docs":{},"绝":{"docs":{},"对":{"docs":{},"提":{"docs":{},"升":{"docs":{},"）":{"docs":{},"，":{"docs":{},"在":{"docs":{},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"d":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}},"docs":{}}},"docs":{}},"docs":{}}},"达":{"docs":{},"到":{"8":{"6":{"docs":{},".":{"7":{"docs":{},"的":{"docs":{},"（":{"5":{"docs":{},".":{"6":{"docs":{},"%":{"docs":{},"的":{"docs":{},"绝":{"docs":{},"对":{"docs":{},"提":{"docs":{},"升":{"docs":{},"）":{"docs":{},"，":{"docs":{},"在":{"docs":{},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"d":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}},"docs":{}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}},"docs":{}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{},".":{"docs":{},"简":{"docs":{},"介":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}},"2":{"0":{"1":{"8":{"docs":{},"年":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}},"9":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.09090909090909091}},"年":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456}}}},"docs":{}},"docs":{}},"docs":{},".":{"1":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}},"2":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}},"3":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}},"docs":{},"相":{"docs":{},"关":{"docs":{},"工":{"docs":{},"作":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}},"3":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}},".":{"1":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}},"2":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}},"3":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}},".":{"1":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}},"2":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}},"docs":{}}},"4":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}},"docs":{}}},"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"r":{"docs":{},"o":{"docs":{},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":10}}}}}}}},"e":{"docs":{},"l":{"docs":{},"l":{"docs":{},"i":{"docs":{},"g":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"，":{"docs":{},"主":{"docs":{},"要":{"docs":{},"关":{"docs":{},"注":{"docs":{},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"领":{"docs":{},"域":{"docs":{},"的":{"docs":{},"最":{"docs":{},"新":{"docs":{},"研":{"docs":{},"究":{"docs":{},"进":{"docs":{},"展":{"docs":{},"，":{"docs":{},"其":{"docs":{},"中":{"docs":{},"也":{"docs":{},"有":{"docs":{},"大":{"docs":{},"量":{"docs":{},"的":{"docs":{},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"知":{"docs":{},"识":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"也":{"docs":{},"是":{"docs":{},"关":{"docs":{},"注":{"docs":{},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"领":{"docs":{},"域":{"docs":{},"的":{"docs":{},"另":{"docs":{},"外":{"docs":{},"一":{"docs":{},"个":{"docs":{},"重":{"docs":{},"要":{"docs":{},"学":{"docs":{},"术":{"docs":{},"会":{"docs":{},"议":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0182648401826484}}}}},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}}}}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0182648401826484}}}}}}},"c":{"docs":{},"m":{"docs":{},"l":{"docs":{},",":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}},"j":{"docs":{},"c":{"docs":{},"a":{"docs":{},"i":{"docs":{},",":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}},"分":{"docs":{},"析":{"docs":{},"解":{"docs":{},"读":{"docs":{},"最":{"docs":{},"新":{"docs":{},"的":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"方":{"docs":{},"面":{"docs":{},"的":{"docs":{},"论":{"docs":{},"文":{"docs":{},"，":{"docs":{},"供":{"docs":{},"大":{"docs":{},"家":{"docs":{},"一":{"docs":{},"起":{"docs":{},"进":{"docs":{},"行":{"docs":{},"交":{"docs":{},"流":{"docs":{},"和":{"docs":{},"学":{"docs":{},"习":{"docs":{},"。":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"类":{"docs":{},"等":{"docs":{},"级":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"享":{"docs":{},"者":{"docs":{},"：":{"docs":{},"刘":{"docs":{},"露":{"docs":{},"平":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.11764705882352941}}}}}}}}},"将":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"知":{"docs":{},"识":{"docs":{},"进":{"docs":{},"行":{"docs":{},"整":{"docs":{},"理":{"docs":{},"后":{"docs":{},"分":{"docs":{},"享":{"docs":{},"，":{"docs":{},"方":{"docs":{},"便":{"docs":{},"后":{"docs":{},"面":{"docs":{},"的":{"docs":{},"人":{"docs":{},"学":{"docs":{},"习":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}}}}},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"方":{"docs":{},"面":{"docs":{},"的":{"docs":{},"工":{"docs":{},"具":{"docs":{},"进":{"docs":{},"行":{"docs":{},"整":{"docs":{},"理":{"docs":{},"汇":{"docs":{},"总":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}},"工":{"docs":{},"具":{"docs":{},"整":{"docs":{},"理":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}},"收":{"docs":{},"集":{"docs":{},"整":{"docs":{},"理":{"docs":{},"相":{"docs":{},"关":{"docs":{},"成":{"docs":{},"员":{"docs":{},"上":{"docs":{},"传":{"docs":{},"的":{"docs":{},"资":{"docs":{},"料":{"docs":{},"及":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"方":{"docs":{},"便":{"docs":{},"有":{"docs":{},"需":{"docs":{},"要":{"docs":{},"的":{"docs":{},"同":{"docs":{},"学":{"docs":{},"进":{"docs":{},"行":{"docs":{},"下":{"docs":{},"载":{"docs":{},"。":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"领":{"docs":{},"域":{"docs":{},"的":{"docs":{},"会":{"docs":{},"议":{"docs":{},"及":{"docs":{},"期":{"docs":{},"刊":{"docs":{},"，":{"docs":{},"方":{"docs":{},"便":{"docs":{},"大":{"docs":{},"家":{"docs":{},"进":{"docs":{},"行":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"查":{"docs":{},"阅":{"docs":{},"。":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}},"知":{"docs":{},"识":{"docs":{},"分":{"docs":{},"享":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}},"图":{"docs":{},"谱":{"docs":{},"概":{"docs":{},"述":{"docs":{},"p":{"docs":{},"p":{"docs":{},"t":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}}},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"处":{"docs":{},"理":{"docs":{},"会":{"docs":{},"议":{"docs":{},"及":{"docs":{},"期":{"docs":{},"刊":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}}}}}}}}},"论":{"docs":{},"文":{"docs":{},"解":{"docs":{},"读":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}},"该":{"docs":{},"博":{"docs":{},"客":{"docs":{},"主":{"docs":{},"要":{"docs":{},"收":{"docs":{},"集":{"docs":{},"整":{"docs":{},"理":{"docs":{},"与":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"相":{"docs":{},"关":{"docs":{},"工":{"docs":{},"具":{"docs":{},"，":{"docs":{},"论":{"docs":{},"文":{"docs":{},"以":{"docs":{},"及":{"docs":{},"代":{"docs":{},"码":{"docs":{},"等":{"docs":{},"资":{"docs":{},"源":{"docs":{},"信":{"docs":{},"息":{"docs":{},"，":{"docs":{},"对":{"docs":{},"目":{"docs":{},"前":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"最":{"docs":{},"前":{"docs":{},"沿":{"docs":{},"的":{"docs":{},"技":{"docs":{},"术":{"docs":{},"、":{"docs":{},"论":{"docs":{},"文":{"docs":{},"及":{"docs":{},"相":{"docs":{},"关":{"docs":{},"相":{"docs":{},"关":{"docs":{},"代":{"docs":{},"码":{"docs":{},"进":{"docs":{},"行":{"docs":{},"整":{"docs":{},"理":{"docs":{},"后":{"docs":{},"进":{"docs":{},"行":{"docs":{},"分":{"docs":{},"享":{"docs":{},"，":{"docs":{},"主":{"docs":{},"要":{"docs":{},"关":{"docs":{},"注":{"docs":{},"点":{"docs":{},"包":{"docs":{},"括":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"基":{"docs":{},"础":{"docs":{},"方":{"docs":{},"法":{"docs":{},"，":{"docs":{},"知":{"docs":{},"识":{"docs":{},"图":{"docs":{},"谱":{"docs":{},"构":{"docs":{},"建":{"docs":{},"，":{"docs":{},"智":{"docs":{},"能":{"docs":{},"问":{"docs":{},"答":{"docs":{},"等":{"docs":{},"几":{"docs":{},"个":{"docs":{},"方":{"docs":{},"面":{"docs":{},"的":{"docs":{},"知":{"docs":{},"识":{"docs":{},"。":{"docs":{},"通":{"docs":{},"过":{"docs":{},"不":{"docs":{},"断":{"docs":{},"地":{"docs":{},"积":{"docs":{},"累":{"docs":{},"让":{"docs":{},"大":{"docs":{},"家":{"docs":{},"在":{"docs":{},"这":{"docs":{},"条":{"docs":{},"路":{"docs":{},"上":{"docs":{},"越":{"docs":{},"走":{"docs":{},"越":{"docs":{},"远":{"docs":{},"。":{"docs":{"./":{"ref":"./","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"材":{"docs":{},"料":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"视":{"docs":{},"频":{"docs":{},"，":{"docs":{},"讲":{"docs":{},"述":{"docs":{},"的":{"docs":{},"是":{"docs":{},"基":{"docs":{},"于":{"docs":{},"强":{"docs":{},"化":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"文":{"docs":{},"本":{"docs":{},"分":{"docs":{},"类":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"其":{"docs":{},"用":{"docs":{},"于":{"docs":{},"阅":{"docs":{},"读":{"docs":{},"理":{"docs":{},"解":{"docs":{},"中":{"docs":{},"，":{"docs":{},"感":{"docs":{},"谢":{"docs":{},"图":{"docs":{},"像":{"docs":{},"所":{"docs":{},"王":{"docs":{},"大":{"docs":{},"东":{"docs":{},"同":{"docs":{},"学":{"docs":{},"提":{"docs":{},"供":{"docs":{},"的":{"docs":{},"资":{"docs":{},"料":{"docs":{},"。":{"docs":{},"有":{"docs":{},"兴":{"docs":{},"趣":{"docs":{},"的":{"docs":{},"可":{"docs":{},"以":{"docs":{},"观":{"docs":{},"看":{"docs":{},"。":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"页":{"docs":{},"面":{"docs":{},"收":{"docs":{},"集":{"docs":{},"整":{"docs":{},"理":{"docs":{},"日":{"docs":{},"常":{"docs":{},"工":{"docs":{},"作":{"docs":{},"中":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"和":{"docs":{},"资":{"docs":{},"料":{"docs":{},"，":{"docs":{},"大":{"docs":{},"家":{"docs":{},"可":{"docs":{},"以":{"docs":{},"通":{"docs":{},"过":{"docs":{},"将":{"docs":{},"相":{"docs":{},"关":{"docs":{},"资":{"docs":{},"料":{"docs":{},"放":{"docs":{},"到":{"docs":{},"网":{"docs":{},"盘":{"docs":{},"或":{"docs":{},"者":{"docs":{},"其":{"docs":{},"他":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"u":{"docs":{},"b":{"docs":{},"仓":{"docs":{},"库":{"docs":{},"中":{"docs":{},"，":{"docs":{},"然":{"docs":{},"后":{"docs":{},"这":{"docs":{},"里":{"docs":{},"放":{"docs":{},"上":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"链":{"docs":{},"接":{"docs":{},"。":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"部":{"docs":{},"分":{"docs":{},"主":{"docs":{},"要":{"docs":{},"是":{"docs":{},"分":{"docs":{},"享":{"docs":{},"目":{"docs":{},"前":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"及":{"docs":{},"知":{"docs":{},"识":{"docs":{},"图":{"docs":{},"谱":{"docs":{},"处":{"docs":{},"理":{"docs":{},"方":{"docs":{},"法":{"docs":{},"的":{"docs":{},"基":{"docs":{},"础":{"docs":{},"知":{"docs":{},"识":{"docs":{},"，":{"docs":{},"目":{"docs":{},"前":{"docs":{},"包":{"docs":{},"含":{"docs":{},"如":{"docs":{},"下":{"docs":{},"三":{"docs":{},"个":{"docs":{},"方":{"docs":{},"面":{"docs":{},"：":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"基":{"docs":{},"础":{"docs":{},"技":{"docs":{},"术":{"docs":{},"，":{"docs":{},"知":{"docs":{},"识":{"docs":{},"图":{"docs":{},"谱":{"docs":{},"构":{"docs":{},"建":{"docs":{},"，":{"docs":{},"智":{"docs":{},"能":{"docs":{},"问":{"docs":{},"答":{"docs":{},",":{"docs":{},"论":{"docs":{},"文":{"docs":{},"整":{"docs":{},"理":{"docs":{},"将":{"docs":{},"按":{"docs":{},"照":{"docs":{},"年":{"docs":{},"份":{"docs":{},"和":{"docs":{},"板":{"docs":{},"块":{"docs":{},"来":{"docs":{},"进":{"docs":{},"行":{"docs":{},"整":{"docs":{},"理":{"docs":{},"。":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.09090909090909091}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"a":{"docs":{},"a":{"docs":{},"i":{"docs":{},",":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}},"c":{"docs":{},"(":{"docs":{},"l":{"docs":{},"t":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"l":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}},"的":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"t":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}},"（":{"docs":{},"t":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"）":{"docs":{},"，":{"docs":{},"此":{"docs":{},"外":{"docs":{},"还":{"docs":{},"有":{"docs":{},"一":{"docs":{},"些":{"docs":{},"期":{"docs":{},"刊":{"docs":{},"与":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"相":{"docs":{},"关":{"docs":{},"。":{"docs":{},"如":{"docs":{},"t":{"docs":{},"s":{"docs":{},"l":{"docs":{},"p":{"docs":{},"(":{"docs":{},"（":{"docs":{},"a":{"docs":{},"c":{"docs":{},"m":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"m":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"）":{"docs":{},"主":{"docs":{},"办":{"docs":{},"，":{"docs":{},"包":{"docs":{},"括":{"docs":{},"如":{"docs":{},"下":{"docs":{},"几":{"docs":{},"个":{"docs":{},"会":{"docs":{},"议":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}},"d":{"docs":{},"v":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},",":{"docs":{},"a":{"docs":{},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},"m":{"docs":{},"e":{"docs":{},"r":{"docs":{},"i":{"docs":{},"c":{"docs":{},"a":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}}}}}}}},"p":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0182648401826484}}}}}}},")":{"docs":{},"的":{"docs":{},"实":{"docs":{},"验":{"docs":{},"效":{"docs":{},"果":{"docs":{},"，":{"docs":{},"包":{"docs":{},"括":{"docs":{},"基":{"docs":{},"于":{"docs":{},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"d":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{},"，":{"docs":{},"语":{"docs":{},"义":{"docs":{},"分":{"docs":{},"析":{"docs":{},"以":{"docs":{},"及":{"docs":{},"命":{"docs":{},"名":{"docs":{},"识":{"docs":{},"别":{"docs":{},"识":{"docs":{},"别":{"docs":{},"等":{"docs":{},"众":{"docs":{},"多":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"v":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.09090909090909091},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456}}}}},"s":{"docs":{},"i":{"docs":{},"a":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"s":{"docs":{},"o":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0228310502283105}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},")":{"docs":{},"组":{"docs":{},"织":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}},"类":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}},"b":{"docs":{},"s":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{},"：":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456}},"本":{"docs":{},"文":{"docs":{},"作":{"docs":{},"者":{"docs":{},"推":{"docs":{},"出":{"docs":{},"了":{"docs":{},"一":{"docs":{},"套":{"docs":{},"新":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"表":{"docs":{},"达":{"docs":{},"模":{"docs":{},"型":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"，":{"docs":{},"全":{"docs":{},"称":{"docs":{},"为":{"docs":{},"b":{"docs":{},"i":{"docs":{},"d":{"docs":{},"i":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},",":{"docs":{},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}},"l":{"docs":{},".":{"docs":{},",":{"2":{"0":{"1":{"7":{"docs":{},")":{"docs":{},"从":{"docs":{},"另":{"docs":{},"外":{"docs":{},"一":{"docs":{},"个":{"docs":{},"重":{"docs":{},"要":{"docs":{},"角":{"docs":{},"度":{"docs":{},"来":{"docs":{},"对":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"进":{"docs":{},"行":{"docs":{},"处":{"docs":{},"理":{"docs":{},"。":{"docs":{},"在":{"docs":{},"该":{"docs":{},"方":{"docs":{},"法":{"docs":{},"中":{"docs":{},"，":{"docs":{},"作":{"docs":{},"者":{"docs":{},"提":{"docs":{},"出":{"docs":{},"了":{"docs":{},"从":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"提":{"docs":{},"取":{"docs":{},"出":{"docs":{},"与":{"docs":{},"上":{"docs":{},"下":{"docs":{},"文":{"docs":{},"敏":{"docs":{},"感":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"。":{"docs":{},"通":{"docs":{},"过":{"docs":{},"将":{"docs":{},"基":{"docs":{},"于":{"docs":{},"上":{"docs":{},"下":{"docs":{},"文":{"docs":{},"敏":{"docs":{},"感":{"docs":{},"的":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"和":{"docs":{},"特":{"docs":{},"定":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"结":{"docs":{},"构":{"docs":{},"相":{"docs":{},"结":{"docs":{},"合":{"docs":{},"后":{"docs":{},"，":{"docs":{},"e":{"docs":{},"l":{"docs":{},"m":{"docs":{},"o":{"docs":{},"在":{"docs":{},"很":{"docs":{},"多":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"都":{"docs":{},"取":{"docs":{},"得":{"docs":{},"了":{"docs":{},"s":{"docs":{},"o":{"docs":{},"a":{"docs":{},"t":{"docs":{},"(":{"docs":{},"s":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"版":{"docs":{},"本":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"有":{"1":{"1":{"0":{"docs":{},"m":{"docs":{},"个":{"docs":{},"，":{"docs":{},"而":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}},"类":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.261764705882353}},":":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.09090909090909091},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456}}},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"多":{"docs":{},"层":{"docs":{},"的":{"docs":{},"基":{"docs":{},"于":{"docs":{},"双":{"docs":{},"向":{"docs":{},"转":{"docs":{},"移":{"docs":{},"编":{"docs":{},"码":{"docs":{},"器":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"其":{"docs":{},"根":{"docs":{},"据":{"docs":{},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"2":{"docs":{},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"实":{"docs":{},"践":{"docs":{},"。":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"使":{"docs":{},"用":{"docs":{},"的":{"docs":{},"“":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"有":{"docs":{},"两":{"docs":{},"个":{"docs":{},"版":{"docs":{},"本":{"docs":{},"：":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"和":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},",":{"docs":{},"其":{"docs":{},"中":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"如":{"docs":{},"下":{"docs":{},"：":{"docs":{},"l":{"docs":{},"=":{"1":{"2":{"docs":{},",":{"docs":{},"h":{"docs":{},"=":{"7":{"6":{"8":{"docs":{},",":{"docs":{},"a":{"docs":{},"=":{"1":{"2":{"docs":{},",":{"docs":{},"而":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"l":{"docs":{},"=":{"2":{"4":{"docs":{},",":{"docs":{},"h":{"docs":{},"=":{"1":{"0":{"2":{"4":{"docs":{},",":{"docs":{},"a":{"docs":{},"=":{"1":{"6":{"docs":{},".":{"docs":{},"因":{"docs":{},"此":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}},"docs":{}},"docs":{}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}},"docs":{}},"docs":{}},"docs":{}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"模":{"docs":{},"型":{"docs":{},"介":{"docs":{},"绍":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}},"的":{"docs":{},"输":{"docs":{},"入":{"docs":{},"表":{"docs":{},"示":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}},"i":{"docs":{},"d":{"docs":{},"i":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.09090909090909091},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.2558823529411764}}}}}}}}},"l":{"docs":{},"o":{"docs":{},"c":{"docs":{},"k":{"docs":{},"s":{"docs":{},"）":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}},"）":{"docs":{},"中":{"docs":{},"，":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"b":{"docs":{},"有":{"5":{"0":{"docs":{},"%":{"docs":{},"是":{"docs":{},"句":{"docs":{},"子":{"docs":{},"a":{"docs":{},"的":{"docs":{},"下":{"docs":{},"一":{"docs":{},"句":{"docs":{},"，":{"docs":{},"而":{"5":{"0":{"docs":{},"%":{"docs":{},"的":{"docs":{},"可":{"docs":{},"能":{"docs":{},"是":{"docs":{},"随":{"docs":{},"机":{"docs":{},"另":{"docs":{},"外":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}},"c":{"docs":{},"c":{"docs":{},"f":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}}}},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}}}}}},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}},"i":{"docs":{},"k":{"docs":{},"m":{"docs":{},",":{"docs":{},"该":{"docs":{},"会":{"docs":{},"议":{"docs":{},"名":{"docs":{},"称":{"docs":{},"缩":{"docs":{},"写":{"docs":{},"和":{"docs":{},"上":{"docs":{},"一":{"docs":{},"个":{"docs":{},"相":{"docs":{},"同":{"docs":{},"，":{"docs":{},"但":{"docs":{},"是":{"docs":{},"其":{"docs":{},"全":{"docs":{},"称":{"docs":{},"为":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"m":{"docs":{},",":{"docs":{},"其":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}},"o":{"docs":{},"l":{"docs":{},"e":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"(":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}},"m":{"docs":{},"m":{"docs":{},"i":{"docs":{},"t":{"docs":{},"t":{"docs":{},"e":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},"p":{"docs":{},"u":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.045662100456621}}}}}},"n":{"docs":{},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":5.036529680365296}},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"，":{"docs":{},"也":{"docs":{},"是":{"docs":{},"有":{"docs":{},"关":{"docs":{},"信":{"docs":{},"息":{"docs":{},"检":{"docs":{},"索":{"docs":{},"及":{"docs":{},"数":{"docs":{},"据":{"docs":{},"挖":{"docs":{},"掘":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"l":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"(":{"docs":{},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}},"类":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}},"l":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"&":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"p":{"docs":{},"u":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":10}}}}}}}},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}},"e":{"docs":{},"p":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.09090909090909091},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.2558823529411764}}}}}},"e":{"docs":{},"m":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}},"(":{"docs":{},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}},"是":{"docs":{},"有":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"下":{"docs":{},"面":{"docs":{},"的":{"docs":{},"比":{"docs":{},"较":{"docs":{},"著":{"docs":{},"名":{"docs":{},"的":{"docs":{},"兴":{"docs":{},"趣":{"docs":{},"小":{"docs":{},"组":{"docs":{},"（":{"docs":{},"s":{"docs":{},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"i":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}}}},"b":{"docs":{},"e":{"docs":{},"d":{"docs":{},"d":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},":":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"支":{"docs":{},"持":{"docs":{},"长":{"docs":{},"度":{"docs":{},"为":{"5":{"1":{"2":{"docs":{},"个":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"的":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"表":{"docs":{},"示":{"docs":{},"方":{"docs":{},"法":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}},"方":{"docs":{},"法":{"docs":{},"来":{"docs":{},"实":{"docs":{},"现":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"表":{"docs":{},"示":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}},"v":{"docs":{},"a":{"docs":{},"l":{"docs":{},"u":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{},"其":{"docs":{},"由":{"docs":{},"欧":{"docs":{},"洲":{"docs":{},"语":{"docs":{},"言":{"docs":{},"资":{"docs":{},"源":{"docs":{},"组":{"docs":{},"织":{"docs":{},"进":{"docs":{},"行":{"docs":{},"e":{"docs":{},"l":{"docs":{},"r":{"docs":{},"a":{"docs":{},"(":{"docs":{},"e":{"docs":{},"u":{"docs":{},"r":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}},"该":{"docs":{},"会":{"docs":{},"议":{"docs":{},"由":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"的":{"docs":{},"特":{"docs":{},"殊":{"docs":{},"兴":{"docs":{},"趣":{"docs":{},"小":{"docs":{},"组":{"docs":{},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"l":{"docs":{},"e":{"docs":{},"x":{"docs":{},"进":{"docs":{},"行":{"docs":{},"组":{"docs":{},"织":{"docs":{},"，":{"docs":{},"每":{"docs":{},"年":{"docs":{},"都":{"docs":{},"会":{"docs":{},"举":{"docs":{},"办":{"docs":{},"，":{"docs":{},"国":{"docs":{},"内":{"docs":{},"也":{"docs":{},"有":{"docs":{},"很":{"docs":{},"多":{"docs":{},"研":{"docs":{},"究":{"docs":{},"机":{"docs":{},"构":{"docs":{},"及":{"docs":{},"公":{"docs":{},"司":{"docs":{},"参":{"docs":{},"与":{"docs":{},"，":{"docs":{},"如":{"docs":{},"哈":{"docs":{},"工":{"docs":{},"大":{"docs":{},"科":{"docs":{},"大":{"docs":{},"讯":{"docs":{},"飞":{"docs":{},"等":{"docs":{},",":{"docs":{},"其":{"docs":{},"最":{"docs":{},"新":{"docs":{},"的":{"docs":{},"链":{"docs":{},"接":{"docs":{},"网":{"docs":{},"址":{"docs":{},"如":{"docs":{},"下":{"docs":{},"：":{"docs":{},"h":{"docs":{},"t":{"docs":{},"t":{"docs":{},"p":{"docs":{},":":{"docs":{},"/":{"docs":{},"/":{"docs":{},"a":{"docs":{},"l":{"docs":{},"t":{"docs":{},".":{"docs":{},"q":{"docs":{},"c":{"docs":{},"r":{"docs":{},"i":{"docs":{},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{},"/":{"docs":{},"s":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"v":{"docs":{},"a":{"docs":{},"l":{"2":{"0":{"1":{"9":{"docs":{},"/":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"x":{"docs":{},".":{"docs":{},"p":{"docs":{},"h":{"docs":{},"p":{"docs":{},"?":{"docs":{},"i":{"docs":{},"d":{"docs":{},"=":{"docs":{},"t":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"c":{"docs":{},"o":{"docs":{},"d":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.011764705882352941}},"e":{"docs":{},"r":{"docs":{},"”":{"docs":{},"编":{"docs":{},"码":{"docs":{},"器":{"docs":{},"近":{"docs":{},"来":{"docs":{},"得":{"docs":{},"到":{"docs":{},"广":{"docs":{},"泛":{"docs":{},"的":{"docs":{},"应":{"docs":{},"用":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{},"本":{"docs":{},"文":{"docs":{},"中":{"docs":{},"没":{"docs":{},"有":{"docs":{},"进":{"docs":{},"行":{"docs":{},"详":{"docs":{},"细":{"docs":{},"的":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"的":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}},"g":{"docs":{},"r":{"docs":{},"o":{"docs":{},"u":{"docs":{},"p":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}},"s":{"docs":{},",":{"docs":{},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"s":{"docs":{},"）":{"docs":{},"之":{"docs":{},"一":{"docs":{},"的":{"docs":{},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{},"(":{"docs":{},"s":{"docs":{},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"o":{"docs":{},"g":{"docs":{},"l":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}},"p":{"docs":{},"t":{"2":{"0":{"1":{"8":{"docs":{},"基":{"docs":{},"于":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"在":{"docs":{},"许":{"docs":{},"多":{"docs":{},"句":{"docs":{},"子":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"取":{"docs":{},"得":{"docs":{},"了":{"docs":{},"s":{"docs":{},"o":{"docs":{},"t":{"docs":{},"a":{"docs":{},"的":{"docs":{},"实":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{},"(":{"docs":{},"g":{"docs":{},"e":{"docs":{},"n":{"docs":{},"e":{"docs":{},"r":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}},"中":{"docs":{},"则":{"docs":{},"是":{"docs":{},"使":{"docs":{},"用":{"docs":{},"的":{"docs":{},"一":{"docs":{},"种":{"docs":{},"从":{"docs":{},"左":{"docs":{},"到":{"docs":{},"右":{"docs":{},"的":{"docs":{},"编":{"docs":{},"码":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"引":{"docs":{},"起":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"可":{"docs":{},"以":{"docs":{},"被":{"docs":{},"看":{"docs":{},"做":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{},"编":{"docs":{},"码":{"docs":{},"器":{"docs":{},"，":{"docs":{},"而":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"则":{"docs":{},"是":{"docs":{},"一":{"docs":{},"种":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"解":{"docs":{},"码":{"docs":{},"器":{"docs":{},"。":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"和":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}},"的":{"docs":{},"区":{"docs":{},"别":{"docs":{},"可":{"docs":{},"以":{"docs":{},"如":{"docs":{},"图":{"docs":{},"所":{"docs":{},"示":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}},"等":{"docs":{},"结":{"docs":{},"构":{"docs":{},"的":{"docs":{},"差":{"docs":{},"别":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}},"h":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"s":{"docs":{},"t":{"docs":{},"正":{"docs":{},"式":{"docs":{},"宣":{"docs":{},"布":{"docs":{},"成":{"docs":{},"立":{"docs":{},"国":{"docs":{},"际":{"docs":{},"计":{"docs":{},"算":{"docs":{},"语":{"docs":{},"言":{"docs":{},"学":{"docs":{},"学":{"docs":{},"会":{"docs":{},"亚":{"docs":{},"太":{"docs":{},"地":{"docs":{},"区":{"docs":{},"分":{"docs":{},"会":{"docs":{},"（":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"，":{"docs":{},"t":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"t":{"docs":{},"p":{"docs":{},":":{"docs":{},"/":{"docs":{},"/":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"2":{"0":{"1":{"8":{"docs":{},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}},"e":{"docs":{},"m":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"2":{"0":{"1":{"8":{"docs":{},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}},"n":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}},"w":{"docs":{},"w":{"docs":{},"w":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"l":{"docs":{},"l":{"docs":{},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{},"/":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}},"s":{"docs":{},":":{"docs":{},"/":{"docs":{},"/":{"docs":{},"b":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},".":{"docs":{},"c":{"docs":{},"s":{"docs":{},"d":{"docs":{},"n":{"docs":{},".":{"docs":{},"n":{"docs":{},"e":{"docs":{},"t":{"docs":{},"/":{"docs":{},"l":{"docs":{},"y":{"docs":{},"b":{"3":{"docs":{},"b":{"3":{"docs":{},"b":{"docs":{},"/":{"docs":{},"a":{"docs":{},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"l":{"docs":{},"e":{"docs":{},"/":{"docs":{},"d":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"i":{"docs":{},"l":{"docs":{},"s":{"docs":{},"/":{"8":{"3":{"5":{"4":{"8":{"9":{"6":{"4":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}},"j":{"docs":{},"i":{"docs":{},"l":{"docs":{},"r":{"docs":{},"v":{"docs":{},"r":{"docs":{},"t":{"docs":{},"r":{"docs":{},"c":{"docs":{},"/":{"docs":{},"a":{"docs":{},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"l":{"docs":{},"e":{"docs":{},"/":{"docs":{},"d":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"i":{"docs":{},"l":{"docs":{},"s":{"docs":{},"/":{"8":{"3":{"8":{"2":{"9":{"4":{"7":{"0":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"w":{"docs":{},"w":{"docs":{},"w":{"docs":{},".":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"w":{"docs":{},"e":{"docs":{},"b":{"docs":{},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{},"/":{"docs":{},"p":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}},"c":{"docs":{},"n":{"docs":{},"b":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"s":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"/":{"docs":{},"n":{"docs":{},"i":{"docs":{},"u":{"docs":{},"x":{"docs":{},"i":{"docs":{},"c":{"docs":{},"h":{"docs":{},"u":{"docs":{},"a":{"docs":{},"n":{"docs":{},"/":{"docs":{},"p":{"docs":{},"/":{"7":{"6":{"0":{"2":{"0":{"1":{"2":{"docs":{},".":{"docs":{},"h":{"docs":{},"t":{"docs":{},"m":{"docs":{},"l":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"v":{"docs":{},".":{"docs":{},"y":{"docs":{},"o":{"docs":{},"u":{"docs":{},"k":{"docs":{},"u":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"/":{"docs":{},"v":{"docs":{},"_":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"w":{"docs":{},"/":{"docs":{},"i":{"docs":{},"d":{"docs":{},"_":{"docs":{},"x":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"4":{"docs":{},"m":{"docs":{},"d":{"docs":{},"y":{"docs":{},"z":{"docs":{},"n":{"docs":{},"z":{"docs":{},"g":{"docs":{},"w":{"docs":{},"o":{"docs":{},"a":{"docs":{},"=":{"docs":{},"=":{"docs":{},".":{"docs":{},"h":{"docs":{},"t":{"docs":{},"m":{"docs":{},"l":{"docs":{},"?":{"docs":{},"s":{"docs":{},"p":{"docs":{},"m":{"docs":{},"=":{"docs":{},"a":{"2":{"docs":{},"h":{"3":{"docs":{},"j":{"docs":{},".":{"8":{"4":{"2":{"8":{"7":{"7":{"0":{"docs":{},".":{"3":{"4":{"1":{"6":{"0":{"5":{"9":{"docs":{},".":{"1":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"r":{"docs":{},"x":{"docs":{},"i":{"docs":{},"v":{"docs":{},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{},"/":{"docs":{},"a":{"docs":{},"b":{"docs":{},"s":{"docs":{},"/":{"1":{"8":{"1":{"0":{"docs":{},".":{"0":{"4":{"8":{"0":{"5":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}},"j":{"docs":{},"a":{"docs":{},"i":{"docs":{},"r":{"docs":{},",":{"docs":{},"j":{"docs":{},"o":{"docs":{},"u":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}},"c":{"docs":{},"o":{"docs":{},"b":{"docs":{},"d":{"docs":{},"e":{"docs":{},"v":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},",":{"docs":{},"m":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}},"m":{"docs":{},"l":{"docs":{},"r":{"docs":{},",":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"o":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"u":{"docs":{},"r":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":5.004566210045662}}}}}}}},"u":{"docs":{},"m":{"docs":{},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},":":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}}}}}},"k":{"docs":{},"n":{"docs":{},"o":{"docs":{},"w":{"docs":{},"l":{"docs":{},"e":{"docs":{},"d":{"docs":{},"g":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"e":{"docs":{},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{"paper/KnowledgeBase/readme.html":{"ref":"paper/KnowledgeBase/readme.html","tf":10}}}}}}}}}}}}},"e":{"docs":{},"y":{"docs":{},"w":{"docs":{},"o":{"docs":{},"r":{"docs":{},"d":{"docs":{},"s":{"docs":{},":":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456}}}}}}}}},"n":{"docs":{},"t":{"docs":{},"o":{"docs":{},"n":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"n":{"docs":{},"a":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}},"l":{"docs":{},"a":{"docs":{},"n":{"docs":{},"g":{"docs":{},"u":{"docs":{},"a":{"docs":{},"g":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.045662100456621},"paper/readme.html":{"ref":"paper/readme.html","tf":0.09090909090909091},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.2558823529411764}}}}}}}},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}},"i":{"docs":{},"n":{"docs":{},")":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}},"g":{"docs":{},")":{"docs":{},"举":{"docs":{},"办":{"docs":{},"的":{"docs":{},"。":{"docs":{},"其":{"docs":{},"也":{"docs":{},"是":{"docs":{},"每":{"docs":{},"年":{"docs":{},"举":{"docs":{},"办":{"docs":{},"一":{"docs":{},"次":{"docs":{},"，":{"docs":{},"由":{"docs":{},"于":{"docs":{},"n":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"是":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"在":{"docs":{},"北":{"docs":{},"美":{"docs":{},"的":{"docs":{},"分":{"docs":{},"会":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{},"当":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"在":{"docs":{},"北":{"docs":{},"美":{"docs":{},"举":{"docs":{},"办":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"n":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"就":{"docs":{},"会":{"docs":{},"停":{"docs":{},"办":{"docs":{},"一":{"docs":{},"年":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},",":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}},"v":{"docs":{},"e":{"docs":{},"l":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"都":{"docs":{},"取":{"docs":{},"得":{"docs":{},"了":{"docs":{},"较":{"docs":{},"好":{"docs":{},"的":{"docs":{},"效":{"docs":{},"果":{"docs":{},"。":{"docs":{},"在":{"docs":{},"很":{"docs":{},"多":{"docs":{},"有":{"docs":{},"特":{"docs":{},"定":{"docs":{},"需":{"docs":{},"要":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"也":{"docs":{},"取":{"docs":{},"得":{"docs":{},"了":{"docs":{},"较":{"docs":{},"好":{"docs":{},"的":{"docs":{},"效":{"docs":{},"果":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"e":{"docs":{},"和":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"u":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"i":{"docs":{},"c":{"docs":{},"s":{"docs":{},")":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}},"组":{"docs":{},"织":{"docs":{},"的":{"docs":{},"。":{"docs":{},"该":{"docs":{},"会":{"docs":{},"议":{"docs":{},"每":{"docs":{},"两":{"docs":{},"年":{"docs":{},"举":{"docs":{},"办":{"docs":{},"一":{"docs":{},"次":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}},",":{"docs":{},"其":{"docs":{},"由":{"1":{"9":{"6":{"5":{"docs":{},"年":{"docs":{},"创":{"docs":{},"办":{"docs":{},"，":{"docs":{},"是":{"docs":{},"由":{"docs":{},"老":{"docs":{},"牌":{"docs":{},"的":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},"学":{"docs":{},"术":{"docs":{},"会":{"docs":{},"议":{"docs":{},"组":{"docs":{},"织":{"docs":{},"i":{"docs":{},"c":{"docs":{},"c":{"docs":{},"l":{"docs":{},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"翻":{"docs":{},"译":{"docs":{},"过":{"docs":{},"来":{"docs":{},"是":{"docs":{},"计":{"docs":{},"算":{"docs":{},"机":{"docs":{},"语":{"docs":{},"言":{"docs":{},"协":{"docs":{},"会":{"docs":{},"，":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"与":{"docs":{},"计":{"docs":{},"算":{"docs":{},"语":{"docs":{},"言":{"docs":{},"学":{"docs":{},"领":{"docs":{},"域":{"docs":{},"（":{"docs":{},"以":{"docs":{},"下":{"docs":{},"简":{"docs":{},"称":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},"/":{"docs":{},"c":{"docs":{},"l":{"docs":{},"）":{"docs":{},"最":{"docs":{},"权":{"docs":{},"威":{"docs":{},"的":{"docs":{},"国":{"docs":{},"际":{"docs":{},"专":{"docs":{},"业":{"docs":{},"学":{"docs":{},"会":{"docs":{},"，":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"成":{"docs":{},"立":{"docs":{},"于":{"1":{"9":{"6":{"2":{"docs":{},"年":{"docs":{},"，":{"docs":{},"是":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"(":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},")":{"docs":{},"领":{"docs":{},"域":{"docs":{},"影":{"docs":{},"响":{"docs":{},"力":{"docs":{},"最":{"docs":{},"大":{"docs":{},"、":{"docs":{},"最":{"docs":{},"具":{"docs":{},"活":{"docs":{},"力":{"docs":{},"的":{"docs":{},"顶":{"docs":{},"级":{"docs":{},"国":{"docs":{},"际":{"docs":{},"学":{"docs":{},"术":{"docs":{},"组":{"docs":{},"织":{"docs":{},"，":{"docs":{},"每":{"docs":{},"年":{"docs":{},"举":{"docs":{},"办":{"docs":{},"一":{"docs":{},"次":{"docs":{},"。":{"docs":{},"这":{"docs":{},"个":{"docs":{},"学":{"docs":{},"会":{"docs":{},"主":{"docs":{},"办":{"docs":{},"了":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"该":{"docs":{},"会":{"docs":{},"议":{"docs":{},"是":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"在":{"docs":{},"北":{"docs":{},"美":{"docs":{},"的":{"docs":{},"分":{"docs":{},"会":{"docs":{},"，":{"docs":{},"也":{"docs":{},"是":{"docs":{},"有":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"主":{"docs":{},"办":{"docs":{},"。":{"docs":{},"其":{"docs":{},"是":{"docs":{},"有":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"下":{"docs":{},"面":{"docs":{},"的":{"docs":{},"兴":{"docs":{},"趣":{"docs":{},"小":{"docs":{},"组":{"docs":{},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"(":{"docs":{},"s":{"docs":{},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"和":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}},"）":{"docs":{},"。":{"docs":{},"此":{"docs":{},"次":{"docs":{},"成":{"docs":{},"立":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"亚":{"docs":{},"太":{"docs":{},"分":{"docs":{},"会":{"docs":{},"，":{"docs":{},"将":{"docs":{},"进":{"docs":{},"一":{"docs":{},"步":{"docs":{},"促":{"docs":{},"进":{"docs":{},"亚":{"docs":{},"太":{"docs":{},"地":{"docs":{},"区":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},"相":{"docs":{},"关":{"docs":{},"技":{"docs":{},"术":{"docs":{},"和":{"docs":{},"研":{"docs":{},"究":{"docs":{},"的":{"docs":{},"发":{"docs":{},"展":{"docs":{},"。":{"docs":{},"据":{"docs":{},"悉":{"docs":{},"，":{"docs":{},"首":{"docs":{},"届":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"会":{"docs":{},"议":{"docs":{},"预":{"docs":{},"计":{"docs":{},"在":{"2":{"0":{"2":{"0":{"docs":{},"年":{"docs":{},"举":{"docs":{},"行":{"docs":{},"，":{"docs":{},"此":{"docs":{},"后":{"docs":{},"将":{"docs":{},"每":{"docs":{},"两":{"docs":{},"年":{"docs":{},"举":{"docs":{},"行":{"docs":{},"一":{"docs":{},"次":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}},":":{"docs":{},"表":{"docs":{},"示":{"docs":{},"层":{"docs":{},"级":{"docs":{},"的":{"docs":{},"数":{"docs":{},"量":{"docs":{},"（":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}},"m":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}},"\"":{"docs":{},"(":{"docs":{},"m":{"docs":{},"l":{"docs":{},"m":{"docs":{},")":{"docs":{},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{},"在":{"docs":{},"该":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"，":{"docs":{},"被":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"的":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"会":{"docs":{},"被":{"docs":{},"输":{"docs":{},"入":{"docs":{},"到":{"docs":{},"s":{"docs":{},"o":{"docs":{},"f":{"docs":{},"t":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"。":{"docs":{},"在":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"实":{"docs":{},"验":{"docs":{},"中":{"docs":{},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"采":{"docs":{},"用":{"1":{"5":{"docs":{},"%":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"随":{"docs":{},"机":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"输":{"docs":{},"入":{"docs":{},"句":{"docs":{},"子":{"docs":{},"中":{"docs":{},"的":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},".":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"和":{"docs":{},"n":{"docs":{},"e":{"docs":{},"x":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}},"m":{"docs":{},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{},"i":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}},"e":{"docs":{},"r":{"docs":{},"y":{"docs":{},",":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}},"n":{"docs":{},"a":{"docs":{},"g":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"，":{"docs":{},"其":{"docs":{},"关":{"docs":{},"注":{"docs":{},"信":{"docs":{},"息":{"docs":{},"管":{"docs":{},"理":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}},"k":{"docs":{},"e":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}},"s":{"docs":{},"k":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}},"e":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"d":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}}}}}},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{},"由":{"docs":{},"名":{"docs":{},"称":{"docs":{},"可":{"docs":{},"知":{"docs":{},"，":{"docs":{},"该":{"docs":{},"会":{"docs":{},"议":{"docs":{},"关":{"docs":{},"注":{"docs":{},"搜":{"docs":{},"索":{"docs":{},"和":{"docs":{},"数":{"docs":{},"据":{"docs":{},"挖":{"docs":{},"掘":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}},"(":{"docs":{},"t":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"的":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"t":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0228310502283105}}}}}},"c":{"docs":{},",":{"docs":{},"n":{"docs":{},"e":{"docs":{},"u":{"docs":{},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}},"e":{"docs":{},"u":{"docs":{},"r":{"docs":{},"a":{"docs":{},"l":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"x":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}},"i":{"docs":{},"p":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}},"l":{"docs":{},"p":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":10}},"/":{"docs":{},"c":{"docs":{},"l":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"其":{"docs":{},"他":{"docs":{},"国":{"docs":{},"际":{"docs":{},"会":{"docs":{},"议":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}}}}}},"p":{"docs":{},"a":{"docs":{},"c":{"docs":{},"i":{"docs":{},"f":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":10}}}}}},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},")":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},")":{"docs":{},",":{"docs":{},"其":{"docs":{},"他":{"docs":{},"相":{"docs":{},"关":{"docs":{},"期":{"docs":{},"刊":{"docs":{},"及":{"docs":{},"投":{"docs":{},"稿":{"docs":{},"链":{"docs":{},"接":{"docs":{},"如":{"docs":{},"下":{"docs":{},"：":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}},"举":{"docs":{},"办":{"docs":{},"的":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},",":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},"）":{"docs":{},"领":{"docs":{},"域":{"docs":{},"的":{"docs":{},"一":{"docs":{},"些":{"docs":{},"著":{"docs":{},"名":{"docs":{},"会":{"docs":{},"议":{"docs":{},"及":{"docs":{},"期":{"docs":{},"刊":{"docs":{},"，":{"docs":{},"将":{"docs":{},"按":{"docs":{},"照":{"docs":{},"c":{"docs":{},"c":{"docs":{},"f":{"docs":{},"对":{"docs":{},"会":{"docs":{},"议":{"docs":{},"的":{"docs":{},"分":{"docs":{},"级":{"docs":{},"标":{"docs":{},"准":{"docs":{},"对":{"docs":{},"相":{"docs":{},"关":{"docs":{},"会":{"docs":{},"议":{"docs":{},"进":{"docs":{},"行":{"docs":{},"整":{"docs":{},"理":{"docs":{},"，":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"每":{"docs":{},"个":{"docs":{},"会":{"docs":{},"议":{"docs":{},"的":{"docs":{},"关":{"docs":{},"注":{"docs":{},"主":{"docs":{},"题":{"docs":{},"，":{"docs":{},"召":{"docs":{},"开":{"docs":{},"周":{"docs":{},"期":{"docs":{},"、":{"docs":{},"会":{"docs":{},"议":{"docs":{},"网":{"docs":{},"站":{"docs":{},"及":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"，":{"docs":{},"方":{"docs":{},"便":{"docs":{},"后":{"docs":{},"面":{"docs":{},"查":{"docs":{},"找":{"docs":{},"最":{"docs":{},"新":{"docs":{},"的":{"docs":{},"相":{"docs":{},"关":{"docs":{},"论":{"docs":{},"文":{"docs":{},"，":{"docs":{},"从":{"docs":{},"而":{"docs":{},"对":{"docs":{},"该":{"docs":{},"领":{"docs":{},"域":{"docs":{},"进":{"docs":{},"行":{"docs":{},"比":{"docs":{},"较":{"docs":{},"深":{"docs":{},"入":{"docs":{},"的":{"docs":{},"研":{"docs":{},"究":{"docs":{},"。":{"docs":{},"每":{"docs":{},"个":{"docs":{},"会":{"docs":{},"议":{"docs":{},"及":{"docs":{},"期":{"docs":{},"刊":{"docs":{},"都":{"docs":{},"有":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"入":{"docs":{},"口":{"docs":{},"链":{"docs":{},"接":{"docs":{},"，":{"docs":{},"方":{"docs":{},"便":{"docs":{},"进":{"docs":{},"行":{"docs":{},"查":{"docs":{},"看":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},".":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}},"）":{"docs":{},")":{"docs":{},"以":{"docs":{},"及":{"docs":{},"t":{"docs":{},"a":{"docs":{},"l":{"docs":{},"i":{"docs":{},"p":{"docs":{},"(":{"docs":{},"(":{"docs":{},"a":{"docs":{},"c":{"docs":{},"m":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.09090909090909091},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.261764705882353}},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}},"o":{"docs":{},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"c":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}}}}}},"o":{"docs":{},"u":{"docs":{},"r":{"docs":{},"c":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}}}}}},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"e":{"docs":{},"v":{"docs":{},"a":{"docs":{},"l":{"docs":{},",":{"docs":{},"其":{"docs":{},"主":{"docs":{},"要":{"docs":{},"关":{"docs":{},"注":{"docs":{},"信":{"docs":{},"息":{"docs":{},"检":{"docs":{},"索":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"d":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.011764705882352941}}}}}}}}}}},"s":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"c":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"m":{"docs":{},"a":{"docs":{},"n":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"e":{"docs":{},"v":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}},"a":{"docs":{},"l":{"docs":{},",":{"docs":{},"其":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"r":{"docs":{},",":{"docs":{},"其":{"docs":{},"全":{"docs":{},"称":{"docs":{},"是":{"docs":{},"s":{"docs":{},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}},"p":{"docs":{},"e":{"docs":{},"e":{"docs":{},"c":{"docs":{},"h":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}},"y":{"docs":{},"s":{"docs":{},"t":{"docs":{},"e":{"docs":{},"m":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0091324200913242}}}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.09090909090909091},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.2558823529411764}},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"。":{"docs":{},"与":{"docs":{},"近":{"docs":{},"年":{"docs":{},"来":{"docs":{},"提":{"docs":{},"出":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"不":{"docs":{},"一":{"docs":{},"样":{"docs":{},"的":{"docs":{},"地":{"docs":{},"方":{"docs":{},"在":{"docs":{},"于":{"docs":{},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"不":{"docs":{},"再":{"docs":{},"仅":{"docs":{},"仅":{"docs":{},"是":{"docs":{},"只":{"docs":{},"关":{"docs":{},"注":{"docs":{},"一":{"docs":{},"个":{"docs":{},"词":{"docs":{},"前":{"docs":{},"文":{"docs":{},"或":{"docs":{},"后":{"docs":{},"文":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"，":{"docs":{},"而":{"docs":{},"是":{"docs":{},"整":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"所":{"docs":{},"有":{"docs":{},"层":{"docs":{},"都":{"docs":{},"去":{"docs":{},"关":{"docs":{},"注":{"docs":{},"其":{"docs":{},"整":{"docs":{},"个":{"docs":{},"上":{"docs":{},"下":{"docs":{},"文":{"docs":{},"的":{"docs":{},"语":{"docs":{},"境":{"docs":{},"信":{"docs":{},"息":{"docs":{},"。":{"docs":{},"实":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{},"证":{"docs":{},"明":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"过":{"docs":{},"的":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"仅":{"docs":{},"仅":{"docs":{},"在":{"docs":{},"后":{"docs":{},"面":{"docs":{},"再":{"docs":{},"包":{"docs":{},"一":{"docs":{},"层":{"docs":{},"输":{"docs":{},"出":{"docs":{},"层":{"docs":{},"，":{"docs":{},"并":{"docs":{},"对":{"docs":{},"其":{"docs":{},"进":{"docs":{},"行":{"docs":{},"微":{"docs":{},"调":{"docs":{},"训":{"docs":{},"练":{"docs":{},"，":{"docs":{},"就":{"docs":{},"可":{"docs":{},"以":{"docs":{},"将":{"docs":{},"其":{"docs":{},"应":{"docs":{},"用":{"docs":{},"到":{"docs":{},"其":{"docs":{},"他":{"docs":{},"多":{"docs":{},"种":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"例":{"docs":{},"如":{"docs":{},"问":{"docs":{},"答":{"docs":{},"、":{"docs":{},"语":{"docs":{},"言":{"docs":{},"推":{"docs":{},"断":{"docs":{},"等":{"docs":{},"，":{"docs":{},"并":{"docs":{},"且":{"docs":{},"在":{"docs":{},"这":{"docs":{},"些":{"docs":{},"后":{"docs":{},"端":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"并":{"docs":{},"不":{"docs":{},"需":{"docs":{},"要":{"docs":{},"根":{"docs":{},"据":{"docs":{},"特":{"docs":{},"定":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"需":{"docs":{},"要":{"docs":{},"对":{"docs":{},"模":{"docs":{},"型":{"docs":{},"结":{"docs":{},"构":{"docs":{},"进":{"docs":{},"行":{"docs":{},"修":{"docs":{},"改":{"docs":{},"。":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"是":{"docs":{},"一":{"docs":{},"种":{"docs":{},"概":{"docs":{},"念":{"docs":{},"上":{"docs":{},"简":{"docs":{},"单":{"docs":{},"，":{"docs":{},"但":{"docs":{},"根":{"docs":{},"据":{"docs":{},"经":{"docs":{},"验":{"docs":{},"推":{"docs":{},"断":{"docs":{},"其":{"docs":{},"具":{"docs":{},"有":{"docs":{},"强":{"docs":{},"大":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{},"。":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"在":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456}}}}}}}}}}}}}}}}}}}}}},"在":{"docs":{},"实":{"docs":{},"际":{"docs":{},"中":{"docs":{},"却":{"docs":{},"有":{"docs":{},"强":{"docs":{},"大":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"在":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"(":{"docs":{},"基":{"docs":{},"于":{"docs":{},"双":{"docs":{},"向":{"docs":{},"翻":{"docs":{},"译":{"docs":{},"编":{"docs":{},"码":{"docs":{},"的":{"docs":{},"表":{"docs":{},"示":{"docs":{},"模":{"docs":{},"型":{"docs":{},")":{"docs":{},"。":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"通":{"docs":{},"过":{"docs":{},"提":{"docs":{},"出":{"docs":{},"了":{"docs":{},"两":{"docs":{},"个":{"docs":{},"新":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"目":{"docs":{},"标":{"docs":{},"来":{"docs":{},"解":{"docs":{},"决":{"docs":{},"以":{"docs":{},"前":{"docs":{},"的":{"docs":{},"提":{"docs":{},"到":{"docs":{},"的":{"docs":{},"单":{"docs":{},"向":{"docs":{},"限":{"docs":{},"制":{"docs":{},"问":{"docs":{},"题":{"docs":{},"：":{"docs":{},"基":{"docs":{},"于":{"docs":{},"“":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{},"e":{"docs":{},"d":{"docs":{},"”":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"（":{"docs":{},"m":{"docs":{},"l":{"docs":{},"m":{"docs":{},"）":{"docs":{},"，":{"docs":{},"这":{"docs":{},"种":{"docs":{},"方":{"docs":{},"法":{"docs":{},"在":{"1":{"9":{"5":{"3":{"docs":{},"年":{"docs":{},"就":{"docs":{},"曾":{"docs":{},"经":{"docs":{},"被":{"docs":{},"提":{"docs":{},"出":{"docs":{},"。":{"docs":{},"这":{"docs":{},"种":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"通":{"docs":{},"过":{"docs":{},"随":{"docs":{},"机":{"docs":{},"将":{"docs":{},"输":{"docs":{},"入":{"docs":{},"句":{"docs":{},"子":{"docs":{},"中":{"docs":{},"的":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"进":{"docs":{},"行":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"，":{"docs":{},"然":{"docs":{},"后":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"目":{"docs":{},"标":{"docs":{},"就":{"docs":{},"是":{"docs":{},"基":{"docs":{},"于":{"docs":{},"上":{"docs":{},"下":{"docs":{},"文":{"docs":{},"来":{"docs":{},"预":{"docs":{},"测":{"docs":{},"出":{"docs":{},"被":{"docs":{},"覆":{"docs":{},"盖":{"docs":{},"掉":{"docs":{},"的":{"docs":{},"单":{"docs":{},"词":{"docs":{},"。":{"docs":{},"不":{"docs":{},"同":{"docs":{},"于":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"从":{"docs":{},"左":{"docs":{},"到":{"docs":{},"右":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"m":{"docs":{},"l":{"docs":{},"m":{"docs":{},"训":{"docs":{},"练":{"docs":{},"目":{"docs":{},"标":{"docs":{},"使":{"docs":{},"得":{"docs":{},"我":{"docs":{},"们":{"docs":{},"能":{"docs":{},"够":{"docs":{},"充":{"docs":{},"分":{"docs":{},"利":{"docs":{},"用":{"docs":{},"左":{"docs":{},"边":{"docs":{},"和":{"docs":{},"右":{"docs":{},"边":{"docs":{},"信":{"docs":{},"息":{"docs":{},"来":{"docs":{},"训":{"docs":{},"练":{"docs":{},"一":{"docs":{},"个":{"docs":{},"更":{"docs":{},"深":{"docs":{},"的":{"docs":{},"双":{"docs":{},"向":{"docs":{},"翻":{"docs":{},"译":{"docs":{},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{},"除":{"docs":{},"了":{"docs":{},"利":{"docs":{},"用":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{},"e":{"docs":{},"d":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"外":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"还":{"docs":{},"在":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"引":{"docs":{},"入":{"docs":{},"了":{"docs":{},"预":{"docs":{},"测":{"docs":{},"下":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"训":{"docs":{},"练":{"docs":{},"目":{"docs":{},"标":{"docs":{},"来":{"docs":{},"同":{"docs":{},"时":{"docs":{},"训":{"docs":{},"练":{"docs":{},"句":{"docs":{},"子":{"docs":{},"对":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"表":{"docs":{},"示":{"docs":{},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},")":{"docs":{},"引":{"docs":{},"入":{"docs":{},"了":{"docs":{},"最":{"docs":{},"小":{"docs":{},"任":{"docs":{},"务":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"，":{"docs":{},"在":{"docs":{},"下":{"docs":{},"游":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"其":{"docs":{},"通":{"docs":{},"过":{"docs":{},"简":{"docs":{},"单":{"docs":{},"的":{"docs":{},"微":{"docs":{},"调":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"调":{"docs":{},"整":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"。":{"docs":{},"在":{"docs":{},"以":{"docs":{},"前":{"docs":{},"的":{"docs":{},"工":{"docs":{},"作":{"docs":{},"中":{"docs":{},"，":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"方":{"docs":{},"法":{"docs":{},"都":{"docs":{},"是":{"docs":{},"通":{"docs":{},"过":{"docs":{},"使":{"docs":{},"用":{"docs":{},"同":{"docs":{},"样":{"docs":{},"的":{"docs":{},"目":{"docs":{},"标":{"docs":{},"函":{"docs":{},"数":{"docs":{},"并":{"docs":{},"通":{"docs":{},"过":{"docs":{},"双":{"docs":{},"向":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"来":{"docs":{},"学":{"docs":{},"习":{"docs":{},"更":{"docs":{},"好":{"docs":{},"的":{"docs":{},"通":{"docs":{},"用":{"docs":{},"语":{"docs":{},"言":{"docs":{},"表":{"docs":{},"示":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.09090909090909091},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.261764705882353}}}}}},"o":{"docs":{},"o":{"docs":{},"l":{"docs":{"tools/readme.html":{"ref":"tools/readme.html","tf":10}}}},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}},"u":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"o":{"docs":{},"v":{"docs":{},"a":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}},"u":{"docs":{},"n":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"）":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"。":{"docs":{},"在":{"docs":{},"基":{"docs":{},"于":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"中":{"docs":{},"，":{"docs":{},"e":{"docs":{},"l":{"docs":{},"m":{"docs":{},"o":{"docs":{},"(":{"2":{"0":{"1":{"8":{"docs":{},")":{"docs":{},"是":{"docs":{},"一":{"docs":{},"种":{"docs":{},"基":{"docs":{},"于":{"docs":{},"特":{"docs":{},"征":{"docs":{},"任":{"docs":{},"务":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"其":{"docs":{},"包":{"docs":{},"含":{"docs":{},"一":{"docs":{},"个":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"型":{"docs":{},"以":{"docs":{},"及":{"docs":{},"一":{"docs":{},"些":{"docs":{},"其":{"docs":{},"他":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"。":{"docs":{},"在":{"docs":{},"微":{"docs":{},"调":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"，":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}},"u":{"docs":{},"a":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{"paper/readme.html":{"ref":"paper/readme.html","tf":0.09090909090909091},"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":1.2558823529411764}}}}}}}}}}}},"w":{"docs":{},"e":{"docs":{},"b":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}},"i":{"docs":{},"d":{"docs":{},"e":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"o":{"docs":{},"r":{"docs":{},"k":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"p":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}},"l":{"docs":{},"d":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"s":{"docs":{},"d":{"docs":{},"m":{"docs":{},",":{"docs":{},"该":{"docs":{},"会":{"docs":{},"议":{"docs":{},"全":{"docs":{},"称":{"docs":{},"为":{"docs":{},"w":{"docs":{},"e":{"docs":{},"b":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"领":{"docs":{},"域":{"docs":{},"与":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"会":{"docs":{},"议":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}},"以":{"docs":{},"下":{"docs":{},"是":{"docs":{},"根":{"docs":{},"据":{"docs":{},"c":{"docs":{},"c":{"docs":{},"f":{"2":{"0":{"1":{"5":{"docs":{},"年":{"docs":{},"的":{"docs":{},"对":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"领":{"docs":{},"域":{"docs":{},"的":{"docs":{},"会":{"docs":{},"议":{"docs":{},"的":{"docs":{},"分":{"docs":{},"级":{"docs":{},"标":{"docs":{},"准":{"docs":{},"来":{"docs":{},"进":{"docs":{},"行":{"docs":{},"罗":{"docs":{},"列":{"docs":{},"和":{"docs":{},"整":{"docs":{},"理":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}},"会":{"docs":{},"议":{"docs":{},"名":{"docs":{},"称":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}},"网":{"docs":{},"站":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"全":{"docs":{},"称":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0136986301369863}},"是":{"docs":{},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}},"出":{"docs":{},"版":{"docs":{},"社":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}},"参":{"docs":{},"考":{"docs":{},"文":{"docs":{},"献":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"召":{"docs":{},"开":{"docs":{},"周":{"docs":{},"期":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"国":{"docs":{},"际":{"docs":{},"会":{"docs":{},"议":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"在":{"1":{"1":{"docs":{},"项":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"都":{"docs":{},"取":{"docs":{},"得":{"docs":{},"了":{"docs":{},"s":{"docs":{},"o":{"docs":{},"t":{"docs":{},"a":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"。":{"docs":{},"表":{"docs":{},"明":{"docs":{},"双":{"docs":{},"向":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"在":{"docs":{},"文":{"docs":{},"本":{"docs":{},"处":{"docs":{},"理":{"docs":{},"中":{"docs":{},"的":{"docs":{},"重":{"docs":{},"要":{"docs":{},"性":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}},"国":{"docs":{},"际":{"docs":{},"上":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"，":{"docs":{},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"，":{"docs":{},"e":{"docs":{},"n":{"docs":{},"m":{"docs":{},"l":{"docs":{},"p":{"docs":{},"和":{"docs":{},"n":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"是":{"docs":{},"默":{"docs":{},"认":{"docs":{},"的":{"docs":{},"四":{"docs":{},"大":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"顶":{"docs":{},"级":{"docs":{},"学":{"docs":{},"术":{"docs":{},"会":{"docs":{},"议":{"docs":{},"，":{"docs":{},"其":{"docs":{},"中":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"，":{"docs":{},"e":{"docs":{},"m":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},"和":{"docs":{},"n":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"都":{"docs":{},"是":{"docs":{},"由":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"及":{"docs":{},"相":{"docs":{},"应":{"docs":{},"的":{"docs":{},"子":{"docs":{},"组":{"docs":{},"织":{"docs":{},"举":{"docs":{},"办":{"docs":{},"的":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"中":{"docs":{},"，":{"docs":{},"一":{"docs":{},"般":{"docs":{},"来":{"docs":{},"将":{"docs":{},"，":{"docs":{},"大":{"docs":{},"家":{"docs":{},"更":{"docs":{},"关":{"docs":{},"注":{"docs":{},"学":{"docs":{},"术":{"docs":{},"会":{"docs":{},"议":{"docs":{},"，":{"docs":{},"其":{"docs":{},"主":{"docs":{},"要":{"docs":{},"原":{"docs":{},"因":{"docs":{},"是":{"docs":{},"发":{"docs":{},"表":{"docs":{},"周":{"docs":{},"期":{"docs":{},"短":{"docs":{},"，":{"docs":{},"通":{"docs":{},"过":{"docs":{},"会":{"docs":{},"议":{"docs":{},"也":{"docs":{},"可":{"docs":{},"以":{"docs":{},"进":{"docs":{},"行":{"docs":{},"深":{"docs":{},"入":{"docs":{},"的":{"docs":{},"交":{"docs":{},"流":{"docs":{},"。":{"docs":{},"但":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"领":{"docs":{},"域":{"docs":{},"也":{"docs":{},"有":{"docs":{},"自":{"docs":{},"己":{"docs":{},"的":{"docs":{},"学":{"docs":{},"术":{"docs":{},"期":{"docs":{},"刊":{"docs":{},"，":{"docs":{},"其":{"docs":{},"旗":{"docs":{},"舰":{"docs":{},"期":{"docs":{},"刊":{"docs":{},"有":{"docs":{},"两":{"docs":{},"个":{"docs":{},"，":{"docs":{},"分":{"docs":{},"别":{"docs":{},"是":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"u":{"docs":{},"t":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"也":{"docs":{},"是":{"docs":{},"其":{"docs":{},"主":{"docs":{},"流":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"，":{"docs":{},"在":{"docs":{},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"领":{"docs":{},"域":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"学":{"docs":{},"术":{"docs":{},"会":{"docs":{},"议":{"docs":{},"包":{"docs":{},"括":{"docs":{},"i":{"docs":{},"c":{"docs":{},"m":{"docs":{},"l":{"docs":{},",":{"docs":{},"n":{"docs":{},"i":{"docs":{},"p":{"docs":{},"s":{"docs":{},",":{"docs":{},"u":{"docs":{},"a":{"docs":{},"i":{"docs":{},"及":{"docs":{},"a":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{},"s":{"docs":{},",":{"docs":{},"下":{"docs":{},"面":{"docs":{},"将":{"docs":{},"简":{"docs":{},"单":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"中":{"docs":{},"输":{"docs":{},"入":{"docs":{},"被":{"docs":{},"表":{"docs":{},"示":{"docs":{},"为":{"docs":{},"两":{"docs":{},"种":{"docs":{},"形":{"docs":{},"式":{"docs":{},"：":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"使":{"docs":{},"用":{"docs":{},"的":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"双":{"docs":{},"向":{"docs":{},"的":{"docs":{},"自":{"docs":{},"注":{"docs":{},"意":{"docs":{},"力":{"docs":{},"机":{"docs":{},"制":{"docs":{},"的":{"docs":{},"编":{"docs":{},"码":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"而":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"的":{"docs":{},"输":{"docs":{},"入":{"docs":{},"中":{"docs":{},"，":{"docs":{},"对":{"docs":{},"于":{"docs":{},"一":{"docs":{},"个":{"docs":{},"输":{"docs":{},"入":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"序":{"docs":{},"列":{"docs":{},"，":{"docs":{},"该":{"docs":{},"输":{"docs":{},"入":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"序":{"docs":{},"列":{"docs":{},"表":{"docs":{},"示":{"docs":{},"可":{"docs":{},"以":{"docs":{},"由":{"docs":{},"几":{"docs":{},"部":{"docs":{},"分":{"docs":{},"构":{"docs":{},"成":{"docs":{},"：":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"序":{"docs":{},"列":{"docs":{},"表":{"docs":{},"示":{"docs":{},"，":{"docs":{},"s":{"docs":{},"e":{"docs":{},"g":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"和":{"docs":{},"p":{"docs":{},"o":{"docs":{},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{},"e":{"docs":{},"r":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"，":{"docs":{},"由":{"docs":{},"于":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{},"并":{"docs":{},"不":{"docs":{},"知":{"docs":{},"道":{"docs":{},"哪":{"docs":{},"些":{"docs":{},"单":{"docs":{},"词":{"docs":{},"可":{"docs":{},"能":{"docs":{},"会":{"docs":{},"被":{"docs":{},"替":{"docs":{},"换":{"docs":{},"成":{"docs":{},"随":{"docs":{},"机":{"docs":{},"的":{"docs":{},"单":{"docs":{},"词":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{},"要":{"docs":{},"求":{"docs":{},"编":{"docs":{},"码":{"docs":{},"器":{"docs":{},"能":{"docs":{},"够":{"docs":{},"学":{"docs":{},"习":{"docs":{},"到":{"docs":{},"一":{"docs":{},"种":{"docs":{},"对":{"docs":{},"所":{"docs":{},"有":{"docs":{},"单":{"docs":{},"词":{"docs":{},"都":{"docs":{},"有":{"docs":{},"表":{"docs":{},"现":{"docs":{},"的":{"docs":{},"能":{"docs":{},"力":{"docs":{},"，":{"docs":{},"此":{"docs":{},"外":{"docs":{},"随":{"docs":{},"机":{"docs":{},"替":{"docs":{},"换":{"docs":{},"只":{"docs":{},"发":{"docs":{},"在":{"docs":{},"在":{"1":{"docs":{},".":{"5":{"docs":{},"%":{"docs":{},"的":{"docs":{},"概":{"docs":{},"率":{"docs":{},"中":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{},"不":{"docs":{},"会":{"docs":{},"影":{"docs":{},"响":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"整":{"docs":{},"体":{"docs":{},"理":{"docs":{},"解":{"docs":{},"能":{"docs":{},"力":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"构":{"docs":{},"建":{"docs":{},"训":{"docs":{},"练":{"docs":{},"集":{"docs":{},"时":{"docs":{},"候":{"docs":{},"，":{"docs":{},"在":{"docs":{},"选":{"docs":{},"择":{"docs":{},"句":{"docs":{},"子":{"docs":{},"a":{"docs":{},"的":{"docs":{},"下":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"时":{"docs":{},"，":{"docs":{},"通":{"docs":{},"过":{"docs":{},"随":{"docs":{},"机":{"docs":{},"选":{"docs":{},"择":{"docs":{},"下":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"实":{"docs":{},"现":{"docs":{},"构":{"docs":{},"建":{"docs":{},"训":{"docs":{},"练":{"docs":{},"集":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"第":{"docs":{},"三":{"docs":{},"节":{"docs":{},"中":{"docs":{},"，":{"docs":{},"详":{"docs":{},"细":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"模":{"docs":{},"型":{"docs":{},"机":{"docs":{},"器":{"docs":{},"实":{"docs":{},"现":{"docs":{},"细":{"docs":{},"节":{"docs":{},"。":{"docs":{},"该":{"docs":{},"节":{"docs":{},"首":{"docs":{},"选":{"docs":{},"叙":{"docs":{},"述":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"模":{"docs":{},"块":{"docs":{},"的":{"docs":{},"整":{"docs":{},"体":{"docs":{},"框":{"docs":{},"架":{"docs":{},"及":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"的":{"docs":{},"输":{"docs":{},"入":{"docs":{},"表":{"docs":{},"示":{"docs":{},"。":{"docs":{},"接":{"docs":{},"着":{"docs":{},"在":{"3":{"docs":{},".":{"3":{"docs":{},"节":{"docs":{},"中":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"以":{"docs":{},"及":{"docs":{},"核":{"docs":{},"心":{"docs":{},"的":{"docs":{},"创":{"docs":{},"新":{"docs":{},"。":{"3":{"docs":{},".":{"4":{"docs":{},"节":{"docs":{},"中":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"流":{"docs":{},"程":{"docs":{},"，":{"docs":{},"而":{"docs":{},"在":{"3":{"docs":{},".":{"5":{"docs":{},"节":{"docs":{},"中":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"微":{"docs":{},"调":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"。":{"docs":{},"最":{"docs":{},"后":{"docs":{},"在":{"3":{"docs":{},".":{"6":{"docs":{},"节":{"docs":{},"中":{"docs":{},"讨":{"docs":{},"论":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"和":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"该":{"docs":{},"项":{"docs":{},"工":{"docs":{},"作":{"docs":{},"中":{"docs":{},"，":{"docs":{},"作":{"docs":{},"者":{"docs":{},"定":{"docs":{},"义":{"docs":{},"了":{"docs":{},"如":{"docs":{},"下":{"docs":{},"变":{"docs":{},"量":{"docs":{},"：":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}},"输":{"docs":{},"入":{"docs":{},"词":{"docs":{},"序":{"docs":{},"列":{"docs":{},"中":{"docs":{},"，":{"docs":{},"序":{"docs":{},"列":{"docs":{},"的":{"docs":{},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"词":{"docs":{},"都":{"docs":{},"是":{"docs":{},"以":{"docs":{},"[":{"docs":{},"c":{"docs":{},"l":{"docs":{},"s":{"docs":{},"]":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"开":{"docs":{},"头":{"docs":{},"，":{"docs":{},"针":{"docs":{},"对":{"docs":{},"该":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"的":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"这":{"docs":{},"篇":{"docs":{},"文":{"docs":{},"章":{"docs":{},"中":{"docs":{},"我":{"docs":{},"们":{"docs":{},"证":{"docs":{},"明":{"docs":{},"了":{"docs":{},"双":{"docs":{},"向":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"重":{"docs":{},"要":{"docs":{},"性":{"docs":{},"。":{"docs":{},"不":{"docs":{},"同":{"docs":{},"于":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"双":{"docs":{},"向":{"docs":{},"预":{"docs":{},"测":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"一":{"docs":{},"种":{"docs":{},"基":{"docs":{},"于":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{},"e":{"docs":{},"d":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"来":{"docs":{},"训":{"docs":{},"练":{"docs":{},"一":{"docs":{},"种":{"docs":{},"更":{"docs":{},"深":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{},"此":{"docs":{},"外":{"docs":{},"区":{"docs":{},"别":{"docs":{},"于":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"浅":{"docs":{},"层":{"docs":{},"双":{"docs":{},"向":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"是":{"docs":{},"一":{"docs":{},"种":{"docs":{},"使":{"docs":{},"用":{"docs":{},"深":{"docs":{},"层":{"docs":{},"次":{"docs":{},"的":{"docs":{},"双":{"docs":{},"向":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"任":{"docs":{},"务":{"docs":{},"目":{"docs":{},"前":{"docs":{},"的":{"docs":{},"技":{"docs":{},"术":{"docs":{},"严":{"docs":{},"重":{"docs":{},"限":{"docs":{},"制":{"docs":{},"了":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"在":{"docs":{},"语":{"docs":{},"言":{"docs":{},"表":{"docs":{},"达":{"docs":{},"方":{"docs":{},"面":{"docs":{},"的":{"docs":{},"能":{"docs":{},"力":{"docs":{},"，":{"docs":{},"特":{"docs":{},"别":{"docs":{},"是":{"docs":{},"针":{"docs":{},"对":{"docs":{},"微":{"docs":{},"调":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"。":{"docs":{},"最":{"docs":{},"大":{"docs":{},"的":{"docs":{},"缺":{"docs":{},"陷":{"docs":{},"是":{"docs":{},"目":{"docs":{},"前":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"型":{"docs":{},"都":{"docs":{},"是":{"docs":{},"双":{"docs":{},"向":{"docs":{},"的":{"docs":{},"，":{"docs":{},"其":{"docs":{},"严":{"docs":{},"格":{"docs":{},"限":{"docs":{},"制":{"docs":{},"了":{"docs":{},"模":{"docs":{},"型":{"docs":{},"在":{"docs":{},"进":{"docs":{},"行":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"时":{"docs":{},"的":{"docs":{},"选":{"docs":{},"择":{"docs":{},"能":{"docs":{},"力":{"docs":{},"。":{"docs":{},"例":{"docs":{},"如":{"docs":{},"在":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{},"g":{"docs":{},"p":{"docs":{},"中":{"docs":{},"，":{"docs":{},"作":{"docs":{},"者":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"从":{"docs":{},"左":{"docs":{},"到":{"docs":{},"右":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"使":{"docs":{},"得":{"docs":{},"句":{"docs":{},"子":{"docs":{},"中":{"docs":{},"每":{"docs":{},"个":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"只":{"docs":{},"能":{"docs":{},"被":{"docs":{},"以":{"docs":{},"前":{"docs":{},"的":{"docs":{},"词":{"docs":{},"所":{"docs":{},"关":{"docs":{},"注":{"docs":{},"。":{"docs":{},"这":{"docs":{},"种":{"docs":{},"方":{"docs":{},"式":{"docs":{},"对":{"docs":{},"句":{"docs":{},"子":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"是":{"docs":{},"次":{"docs":{},"优":{"docs":{},"的":{"docs":{},"(":{"docs":{},"s":{"docs":{},"u":{"docs":{},"b":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"论":{"docs":{},"文":{"docs":{},"中":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"提":{"docs":{},"出":{"docs":{},"了":{"docs":{},"一":{"docs":{},"种":{"docs":{},"基":{"docs":{},"于":{"docs":{},"微":{"docs":{},"调":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},":":{"docs":{},"b":{"docs":{},"i":{"docs":{},"d":{"docs":{},"i":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"节":{"docs":{},"中":{"docs":{},"我":{"docs":{},"们":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"基":{"docs":{},"于":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"生":{"docs":{},"成":{"docs":{},"语":{"docs":{},"言":{"docs":{},"表":{"docs":{},"示":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"相":{"docs":{},"关":{"docs":{},"方":{"docs":{},"方":{"docs":{},"法":{"docs":{},"，":{"docs":{},"同":{"docs":{},"时":{"docs":{},"简":{"docs":{},"要":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"一":{"docs":{},"下":{"docs":{},"在":{"docs":{},"这":{"docs":{},"个":{"docs":{},"领":{"docs":{},"域":{"docs":{},"目":{"docs":{},"前":{"docs":{},"最":{"docs":{},"流":{"docs":{},"行":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"处":{"docs":{},"理":{"docs":{},"和":{"docs":{},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"是":{"docs":{},"密":{"docs":{},"切":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"，":{"docs":{},"其":{"docs":{},"是":{"docs":{},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"研":{"docs":{},"究":{"docs":{},"的":{"docs":{},"重":{"docs":{},"要":{"docs":{},"内":{"docs":{},"容":{"docs":{},"，":{"docs":{},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"研":{"docs":{},"究":{"docs":{},"的":{"docs":{},"两":{"docs":{},"大":{"docs":{},"国":{"docs":{},"际":{"docs":{},"顶":{"docs":{},"会":{"docs":{},"是":{"docs":{},"a":{"docs":{},"a":{"docs":{},"a":{"docs":{},"i":{"docs":{},"和":{"docs":{},"i":{"docs":{},"j":{"docs":{},"c":{"docs":{},"a":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"期":{"docs":{},"刊":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}},"本":{"docs":{},"文":{"docs":{},"介":{"docs":{},"绍":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"（":{"docs":{},"n":{"docs":{},"a":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}},"作":{"docs":{},"者":{"docs":{},"推":{"docs":{},"出":{"docs":{},"了":{"docs":{},"一":{"docs":{},"套":{"docs":{},"新":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"表":{"docs":{},"达":{"docs":{},"模":{"docs":{},"型":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"，":{"docs":{},"全":{"docs":{},"称":{"docs":{},"为":{"docs":{},"b":{"docs":{},"i":{"docs":{},"d":{"docs":{},"i":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"机":{"docs":{},"器":{"docs":{},"学":{"docs":{},"习":{"docs":{},"领":{"docs":{},"域":{"docs":{},"中":{"docs":{},"与":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"顶":{"docs":{},"级":{"docs":{},"会":{"docs":{},"议":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}},"每":{"docs":{},"两":{"docs":{},"年":{"docs":{},"一":{"docs":{},"次":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}},"年":{"docs":{},"一":{"docs":{},"次":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0182648401826484}}}}}},"除":{"docs":{},"了":{"docs":{},"上":{"docs":{},"述":{"docs":{},"被":{"docs":{},"c":{"docs":{},"c":{"docs":{},"f":{"docs":{},"收":{"docs":{},"录":{"docs":{},"的":{"docs":{},"会":{"docs":{},"议":{"docs":{},"外":{"docs":{},"，":{"docs":{},"在":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"领":{"docs":{},"域":{"docs":{},"还":{"docs":{},"有":{"docs":{},"其":{"docs":{},"他":{"docs":{},"许":{"docs":{},"多":{"docs":{},"重":{"docs":{},"要":{"docs":{},"会":{"docs":{},"议":{"docs":{},"，":{"docs":{},"分":{"docs":{},"别":{"docs":{},"是":{"docs":{},"s":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"v":{"docs":{},"a":{"docs":{},"l":{"docs":{},",":{"docs":{},"l":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"等":{"docs":{},"。":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"直":{"docs":{},"接":{"docs":{},"与":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"相":{"docs":{},"关":{"docs":{},"外":{"docs":{},"，":{"docs":{},"还":{"docs":{},"有":{"docs":{},"其":{"docs":{},"他":{"docs":{},"许":{"docs":{},"多":{"docs":{},"与":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"学":{"docs":{},"术":{"docs":{},"会":{"docs":{},"议":{"docs":{},"，":{"docs":{},"包":{"docs":{},"括":{"docs":{},"信":{"docs":{},"息":{"docs":{},"检":{"docs":{},"索":{"docs":{},"，":{"docs":{},"数":{"docs":{},"据":{"docs":{},"挖":{"docs":{},"掘":{"docs":{},"及":{"docs":{},"人":{"docs":{},"工":{"docs":{},"智":{"docs":{},"能":{"docs":{},"领":{"docs":{},"域":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"都":{"docs":{},"是":{"docs":{},"属":{"docs":{},"于":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"的":{"docs":{},"应":{"docs":{},"用":{"docs":{},"领":{"docs":{},"域":{"docs":{},"。":{"docs":{},"其":{"docs":{},"中":{"docs":{},"信":{"docs":{},"息":{"docs":{},"检":{"docs":{},"索":{"docs":{},"和":{"docs":{},"数":{"docs":{},"据":{"docs":{},"挖":{"docs":{},"掘":{"docs":{},"与":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"是":{"docs":{},"密":{"docs":{},"切":{"docs":{},"相":{"docs":{},"关":{"docs":{},"的":{"docs":{},"，":{"docs":{},"主":{"docs":{},"要":{"docs":{},"由":{"docs":{},"美":{"docs":{},"国":{"docs":{},"计":{"docs":{},"算":{"docs":{},"机":{"docs":{},"学":{"docs":{},"会":{"docs":{},"（":{"docs":{},"a":{"docs":{},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"c":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"领":{"docs":{},"域":{"docs":{},"最":{"docs":{},"权":{"docs":{},"威":{"docs":{},"的":{"docs":{},"国":{"docs":{},"际":{"docs":{},"会":{"docs":{},"议":{"docs":{},"，":{"docs":{},"即":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"年":{"docs":{},"会":{"docs":{},"。":{"1":{"9":{"8":{"2":{"docs":{},"年":{"docs":{},"和":{"1":{"9":{"9":{"9":{"docs":{},"年":{"docs":{},"，":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"分":{"docs":{},"别":{"docs":{},"成":{"docs":{},"立":{"docs":{},"了":{"docs":{},"欧":{"docs":{},"洲":{"docs":{},"分":{"docs":{},"会":{"docs":{},"（":{"docs":{},"e":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"）":{"docs":{},"和":{"docs":{},"北":{"docs":{},"美":{"docs":{},"分":{"docs":{},"会":{"docs":{},"（":{"docs":{},"n":{"docs":{},"a":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"）":{"docs":{},"两":{"docs":{},"个":{"docs":{},"区":{"docs":{},"域":{"docs":{},"性":{"docs":{},"分":{"docs":{},"会":{"docs":{},"。":{"docs":{},"近":{"docs":{},"年":{"docs":{},"来":{"docs":{},"，":{"docs":{},"亚":{"docs":{},"太":{"docs":{},"地":{"docs":{},"区":{"docs":{},"在":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"方":{"docs":{},"面":{"docs":{},"的":{"docs":{},"研":{"docs":{},"究":{"docs":{},"进":{"docs":{},"步":{"docs":{},"显":{"docs":{},"著":{"docs":{},"，":{"2":{"0":{"1":{"8":{"docs":{},"年":{"7":{"docs":{},"月":{"1":{"5":{"docs":{},"日":{"docs":{},"，":{"docs":{},"第":{"5":{"6":{"docs":{},"届":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"年":{"docs":{},"会":{"docs":{},"在":{"docs":{},"澳":{"docs":{},"大":{"docs":{},"利":{"docs":{},"亚":{"docs":{},"墨":{"docs":{},"尔":{"docs":{},"本":{"docs":{},"举":{"docs":{},"行":{"docs":{},"。":{"docs":{},"开":{"docs":{},"幕":{"docs":{},"仪":{"docs":{},"式":{"docs":{},"上":{"docs":{},"，":{"docs":{},"a":{"docs":{},"c":{"docs":{},"l":{"docs":{},"主":{"docs":{},"席":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{"metting/summit metting.html":{"ref":"metting/summit metting.html","tf":0.0045662100456621}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}},"下":{"docs":{},"载":{"docs":{},"地":{"docs":{},"址":{"docs":{},"：":{"docs":{},"链":{"docs":{},"接":{"docs":{},"：":{"docs":{},"h":{"docs":{},"t":{"docs":{},"t":{"docs":{},"p":{"docs":{},"s":{"docs":{},":":{"docs":{},"/":{"docs":{},"/":{"docs":{},"p":{"docs":{},"a":{"docs":{},"n":{"docs":{},".":{"docs":{},"b":{"docs":{},"a":{"docs":{},"i":{"docs":{},"d":{"docs":{},"u":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"/":{"docs":{},"s":{"docs":{},"/":{"1":{"docs":{},"n":{"docs":{},"m":{"docs":{},"h":{"3":{"3":{"docs":{},"y":{"docs":{},"k":{"8":{"0":{"docs":{},"s":{"docs":{},"z":{"docs":{},"n":{"docs":{},"k":{"docs":{},"i":{"8":{"docs":{},"v":{"docs":{},"h":{"docs":{},"m":{"docs":{},"e":{"docs":{},"e":{"docs":{},"e":{"docs":{},"a":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}},"docs":{}}}}}}},"docs":{}},"docs":{}}}},"docs":{}},"docs":{}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"密":{"docs":{},"码":{"docs":{},":":{"docs":{},"t":{"docs":{},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"d":{"docs":{},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}}}}},"时":{"docs":{},"间":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}},"：":{"2":{"0":{"1":{"9":{"docs":{},"年":{"3":{"docs":{},"月":{"2":{"7":{"docs":{},"日":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}},"8":{"docs":{},"日":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}},"docs":{}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}},"概":{"docs":{},"述":{"docs":{},"：":{"docs":{},"该":{"docs":{},"p":{"docs":{},"p":{"docs":{},"t":{"docs":{},"是":{"docs":{},"做":{"docs":{},"汇":{"docs":{},"报":{"docs":{},"时":{"docs":{},"候":{"docs":{},"的":{"docs":{},"p":{"docs":{},"p":{"docs":{},"t":{"docs":{},"，":{"docs":{},"讲":{"docs":{},"了":{"docs":{},"知":{"docs":{},"识":{"docs":{},"图":{"docs":{},"谱":{"docs":{},"的":{"docs":{},"基":{"docs":{},"本":{"docs":{},"知":{"docs":{},"识":{"docs":{},"及":{"docs":{},"实":{"docs":{},"体":{"docs":{},"和":{"docs":{},"关":{"docs":{},"系":{"docs":{},"抽":{"docs":{},"取":{"docs":{},"相":{"docs":{},"关":{"docs":{},"论":{"docs":{},"文":{"docs":{},"，":{"docs":{},"有":{"docs":{},"需":{"docs":{},"要":{"docs":{},"可":{"docs":{},"以":{"docs":{},"下":{"docs":{},"载":{"docs":{},"。":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"（":{"docs":{},"永":{"docs":{},"久":{"docs":{},"有":{"docs":{},"效":{"docs":{},"）":{"docs":{},"提":{"docs":{},"取":{"docs":{},"码":{"docs":{},"：":{"docs":{},"m":{"docs":{},"d":{"docs":{},"a":{"docs":{},"n":{"docs":{"database/readme.html":{"ref":"database/readme.html","tf":0.058823529411764705}}}}}}}}}}}}}}}},"v":{"1":{"docs":{},".":{"1":{"docs":{},"的":{"docs":{},"问":{"docs":{},"答":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"其":{"docs":{},"f":{"1":{"docs":{},"值":{"docs":{},"达":{"docs":{},"到":{"docs":{},"了":{"9":{"3":{"docs":{},".":{"2":{"docs":{},"（":{"1":{"docs":{},".":{"5":{"docs":{},"%":{"docs":{},"）":{"docs":{},"的":{"docs":{},"绝":{"docs":{},"对":{"docs":{},"提":{"docs":{},"升":{"docs":{},"，":{"docs":{},"比":{"docs":{},"人":{"docs":{},"类":{"docs":{},"的":{"docs":{},"表":{"docs":{},"现":{"docs":{},"都":{"docs":{},"搞":{"docs":{},"了":{"2":{"docs":{},".":{"0":{"docs":{},"的":{"docs":{},"提":{"docs":{},"升":{"docs":{},"。":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}},"docs":{}}},"docs":{}}}}}}}}}}}}}}}}}}},"docs":{}}},"docs":{}}},"docs":{}}},"docs":{}},"docs":{}}}}}},"docs":{}}}}}}}}}},"docs":{}}},"docs":{}},"目":{"docs":{},"录":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456}}},"前":{"docs":{},"有":{"docs":{},"两":{"docs":{},"种":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"被":{"docs":{},"用":{"docs":{},"于":{"docs":{},"下":{"docs":{},"游":{"docs":{},"处":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"：":{"docs":{},"基":{"docs":{},"于":{"docs":{},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"和":{"docs":{},"基":{"docs":{},"于":{"docs":{},"微":{"docs":{},"调":{"docs":{},"（":{"docs":{},"f":{"docs":{},"i":{"docs":{},"n":{"docs":{},"e":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"研":{"docs":{},"究":{"docs":{},"性":{"docs":{},"工":{"docs":{},"作":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456}}}}}}},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"，":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{"paper/nlp/readme.html":{"ref":"paper/nlp/readme.html","tf":0.045454545454545456},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}},"任":{"docs":{},"务":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}},"#":{"docs":{},"#":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}},"*":{"docs":{},"a":{"docs":{},"*":{"docs":{},":":{"docs":{},"表":{"docs":{},"示":{"docs":{},"注":{"docs":{},"意":{"docs":{},"力":{"docs":{},"h":{"docs":{},"e":{"docs":{},"a":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{},"的":{"docs":{},"数":{"docs":{},"量":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}},".":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}},"o":{"docs":{},"p":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"a":{"docs":{},"l":{"docs":{},")":{"docs":{},"，":{"docs":{},"而":{"docs":{},"在":{"docs":{},"针":{"docs":{},"对":{"docs":{},"一":{"docs":{},"些":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"如":{"docs":{},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"d":{"docs":{},"的":{"docs":{},"问":{"docs":{},"答":{"docs":{},"问":{"docs":{},"题":{"docs":{},"中":{"docs":{},"，":{"docs":{},"则":{"docs":{},"这":{"docs":{},"种":{"docs":{},"方":{"docs":{},"法":{"docs":{},"有":{"docs":{},"可":{"docs":{},"能":{"docs":{},"是":{"docs":{},"毁":{"docs":{},"灭":{"docs":{},"性":{"docs":{},"的":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{},"在":{"docs":{},"这":{"docs":{},"些":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"纳":{"docs":{},"入":{"docs":{},"两":{"docs":{},"个":{"docs":{},"方":{"docs":{},"面":{"docs":{},"的":{"docs":{},"信":{"docs":{},"息":{"docs":{},"是":{"docs":{},"至":{"docs":{},"关":{"docs":{},"重":{"docs":{},"要":{"docs":{},"的":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"​":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.16470588235294117}}},"不":{"docs":{},"像":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"从":{"docs":{},"左":{"docs":{},"到":{"docs":{},"右":{"docs":{},"或":{"docs":{},"者":{"docs":{},"从":{"docs":{},"右":{"docs":{},"到":{"docs":{},"左":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"在":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"中":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"两":{"docs":{},"种":{"docs":{},"原":{"docs":{},"创":{"docs":{},"的":{"docs":{},"无":{"docs":{},"监":{"docs":{},"督":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"分":{"docs":{},"别":{"docs":{},"是":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"是":{"docs":{},"所":{"docs":{},"有":{"docs":{},"被":{"docs":{},"选":{"docs":{},"择":{"docs":{},"的":{"docs":{},"单":{"docs":{},"词":{"docs":{},"都":{"docs":{},"会":{"docs":{},"被":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"，":{"docs":{},"而":{"docs":{},"是":{"docs":{},"其":{"docs":{},"中":{"8":{"0":{"docs":{},"%":{"docs":{},"的":{"docs":{},"可":{"docs":{},"能":{"docs":{},"会":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"该":{"docs":{},"单":{"docs":{},"词":{"docs":{},"，":{"1":{"0":{"docs":{},"%":{"docs":{},"的":{"docs":{},"时":{"docs":{},"间":{"docs":{},"将":{"docs":{},"该":{"docs":{},"单":{"docs":{},"词":{"docs":{},"替":{"docs":{},"换":{"docs":{},"成":{"docs":{},"一":{"docs":{},"个":{"docs":{},"随":{"docs":{},"机":{"docs":{},"的":{"docs":{},"单":{"docs":{},"词":{"docs":{},"。":{"docs":{},"而":{"docs":{},"另":{"docs":{},"外":{"docs":{},"还":{"docs":{},"是":{"docs":{},"有":{"1":{"0":{"docs":{},"的":{"docs":{},"时":{"docs":{},"间":{"docs":{},"是":{"docs":{},"该":{"docs":{},"单":{"docs":{},"词":{"docs":{},"保":{"docs":{},"持":{"docs":{},"不":{"docs":{},"变":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}},"为":{"docs":{},"了":{"docs":{},"能":{"docs":{},"够":{"docs":{},"训":{"docs":{},"练":{"docs":{},"一":{"docs":{},"个":{"docs":{},"具":{"docs":{},"有":{"docs":{},"双":{"docs":{},"向":{"docs":{},"深":{"docs":{},"层":{"docs":{},"的":{"docs":{},"表":{"docs":{},"示":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"在":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"中":{"docs":{},"采":{"docs":{},"用":{"docs":{},"了":{"docs":{},"一":{"docs":{},"种":{"docs":{},"较":{"docs":{},"为":{"docs":{},"直":{"docs":{},"观":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"通":{"docs":{},"过":{"docs":{},"随":{"docs":{},"机":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"句":{"docs":{},"子":{"docs":{},"中":{"docs":{},"的":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"，":{"docs":{},"并":{"docs":{},"让":{"docs":{},"模":{"docs":{},"型":{"docs":{},"能":{"docs":{},"对":{"docs":{},"该":{"docs":{},"句":{"docs":{},"子":{"docs":{},"进":{"docs":{},"行":{"docs":{},"正":{"docs":{},"确":{"docs":{},"的":{"docs":{},"预":{"docs":{},"测":{"docs":{},"。":{"docs":{},"这":{"docs":{},"种":{"docs":{},"模":{"docs":{},"型":{"docs":{},"在":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"中":{"docs":{},"被":{"docs":{},"称":{"docs":{},"为":{"docs":{},"\"":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"从":{"docs":{},"初":{"docs":{},"始":{"docs":{},"概":{"docs":{},"念":{"docs":{},"上":{"docs":{},"来":{"docs":{},"将":{"docs":{},"，":{"docs":{},"一":{"docs":{},"个":{"docs":{},"更":{"docs":{},"深":{"docs":{},"层":{"docs":{},"次":{"docs":{},"的":{"docs":{},"双":{"docs":{},"向":{"docs":{},"模":{"docs":{},"型":{"docs":{},"比":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"从":{"docs":{},"左":{"docs":{},"到":{"docs":{},"有":{"docs":{},"或":{"docs":{},"者":{"docs":{},"从":{"docs":{},"右":{"docs":{},"到":{"docs":{},"左":{"docs":{},"基":{"docs":{},"于":{"docs":{},"简":{"docs":{},"单":{"docs":{},"讲":{"docs":{},"从":{"docs":{},"左":{"docs":{},"到":{"docs":{},"右":{"docs":{},"或":{"docs":{},"者":{"docs":{},"从":{"docs":{},"右":{"docs":{},"到":{"docs":{},"左":{"docs":{},"模":{"docs":{},"型":{"docs":{},"进":{"docs":{},"行":{"docs":{},"组":{"docs":{},"合":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"有":{"docs":{},"更":{"docs":{},"好":{"docs":{},"的":{"docs":{},"效":{"docs":{},"果":{"docs":{},"。":{"docs":{},"然":{"docs":{},"后":{"docs":{},"，":{"docs":{},"目":{"docs":{},"前":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"则":{"docs":{},"往":{"docs":{},"往":{"docs":{},"只":{"docs":{},"能":{"docs":{},"通":{"docs":{},"过":{"docs":{},"从":{"docs":{},"左":{"docs":{},"到":{"docs":{},"右":{"docs":{},"或":{"docs":{},"者":{"docs":{},"从":{"docs":{},"右":{"docs":{},"到":{"docs":{},"左":{"docs":{},"的":{"docs":{},"模":{"docs":{},"型":{"docs":{},"进":{"docs":{},"行":{"docs":{},"训":{"docs":{},"练":{"docs":{},"。":{"docs":{},"在":{"docs":{},"双":{"docs":{},"向":{"docs":{},"的":{"docs":{},"环":{"docs":{},"境":{"docs":{},"中":{"docs":{},"，":{"docs":{},"句":{"docs":{},"子":{"docs":{},"中":{"docs":{},"任":{"docs":{},"意":{"docs":{},"一":{"docs":{},"个":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"都":{"docs":{},"有":{"docs":{},"可":{"docs":{},"能":{"docs":{},"被":{"docs":{},"预":{"docs":{},"测":{"docs":{},"到":{"docs":{},"在":{"docs":{},"一":{"docs":{},"个":{"docs":{},"多":{"docs":{},"层":{"docs":{},"的":{"docs":{},"网":{"docs":{},"络":{"docs":{},"环":{"docs":{},"境":{"docs":{},"中":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"作":{"docs":{},"者":{"docs":{},"信":{"docs":{},"息":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}},"单":{"docs":{},"位":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"包":{"docs":{},"含":{"3":{"0":{"docs":{},",":{"0":{"0":{"0":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"的":{"docs":{},"词":{"docs":{},"库":{"docs":{},"的":{"docs":{},"w":{"docs":{},"o":{"docs":{},"r":{"docs":{},"d":{"docs":{},"p":{"docs":{},"i":{"docs":{},"e":{"docs":{},"c":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}}}}}},"关":{"docs":{},"键":{"docs":{},"词":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}},"其":{"docs":{},"他":{"docs":{},"解":{"docs":{},"读":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}},"内":{"docs":{},"容":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}},"再":{"docs":{},"训":{"docs":{},"练":{"docs":{},"流":{"docs":{},"程":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}},"区":{"docs":{},"别":{"docs":{},"于":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"a":{"docs":{},"u":{"docs":{},"t":{"docs":{},"o":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}},"单":{"docs":{},"词":{"docs":{},"可":{"docs":{},"能":{"docs":{},"会":{"docs":{},"被":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{},"在":{"docs":{},"训":{"docs":{},"练":{"docs":{},"过":{"docs":{},"程":{"docs":{},"中":{"docs":{},"其":{"docs":{},"会":{"docs":{},"花":{"docs":{},"费":{"docs":{},"较":{"docs":{},"多":{"docs":{},"的":{"docs":{},"时":{"docs":{},"间":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"原":{"docs":{},"文":{"docs":{},"链":{"docs":{},"接":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}},"基":{"docs":{},"于":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"词":{"docs":{},"表":{"docs":{},"示":{"docs":{},"方":{"docs":{},"法":{"docs":{},"在":{"docs":{},"多":{"docs":{},"个":{"docs":{},"领":{"docs":{},"域":{"docs":{},"都":{"docs":{},"有":{"docs":{},"广":{"docs":{},"泛":{"docs":{},"的":{"docs":{},"应":{"docs":{},"用":{"docs":{},"。":{"docs":{},"其":{"docs":{},"中":{"docs":{},"包":{"docs":{},"括":{"docs":{},"非":{"docs":{},"神":{"docs":{},"经":{"docs":{},"元":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"和":{"docs":{},"基":{"docs":{},"于":{"docs":{},"神":{"docs":{},"经":{"docs":{},"网":{"docs":{},"络":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"。":{"docs":{},"基":{"docs":{},"于":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"表":{"docs":{},"示":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"方":{"docs":{},"法":{"docs":{},"目":{"docs":{},"前":{"docs":{},"是":{"docs":{},"现":{"docs":{},"代":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"系":{"docs":{},"统":{"docs":{},"中":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"重":{"docs":{},"要":{"docs":{},"集":{"docs":{},"成":{"docs":{},"部":{"docs":{},"分":{"docs":{},"，":{"docs":{},"其":{"docs":{},"对":{"docs":{},"系":{"docs":{},"统":{"docs":{},"的":{"docs":{},"后":{"docs":{},"续":{"docs":{},"处":{"docs":{},"理":{"docs":{},"提":{"docs":{},"供":{"docs":{},"了":{"docs":{},"有":{"docs":{},"强":{"docs":{},"大":{"docs":{},"的":{"docs":{},"提":{"docs":{},"升":{"docs":{},"。":{"docs":{},"除":{"docs":{},"了":{"docs":{},"词":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"外":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"方":{"docs":{},"法":{"docs":{},"也":{"docs":{},"被":{"docs":{},"推":{"docs":{},"广":{"docs":{},"到":{"docs":{},"粗":{"docs":{},"粒":{"docs":{},"度":{"docs":{},"的":{"docs":{},"句":{"docs":{},"子":{"docs":{},"级":{"docs":{},"别":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"以":{"docs":{},"及":{"docs":{},"锻":{"docs":{},"炼":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"中":{"docs":{},"。":{"docs":{},"在":{"docs":{},"传":{"docs":{},"统":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"中":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"学":{"docs":{},"习":{"docs":{},"到":{"docs":{},"的":{"docs":{},"表":{"docs":{},"示":{"docs":{},"方":{"docs":{},"法":{"docs":{},"往":{"docs":{},"往":{"docs":{},"作":{"docs":{},"为":{"docs":{},"下":{"docs":{},"游":{"docs":{},"任":{"docs":{},"务":{"docs":{},"的":{"docs":{},"特":{"docs":{},"征":{"docs":{},"输":{"docs":{},"入":{"docs":{},"到":{"docs":{},"下":{"docs":{},"游":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"。":{"docs":{},"e":{"docs":{},"l":{"docs":{},"m":{"docs":{},"o":{"docs":{},"(":{"docs":{},"p":{"docs":{},"e":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"微":{"docs":{},"调":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}},"特":{"docs":{},"征":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}},"监":{"docs":{},"督":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{},"方":{"docs":{},"法":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}},"如":{"docs":{},"果":{"docs":{},"输":{"docs":{},"入":{"docs":{},"是":{"docs":{},"单":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"，":{"docs":{},"则":{"docs":{},"直":{"docs":{},"接":{"docs":{},"使":{"docs":{},"用":{"docs":{},"句":{"docs":{},"子":{"docs":{},"a":{"docs":{},"的":{"docs":{},"词":{"docs":{},"嵌":{"docs":{},"表":{"docs":{},"示":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}},"实":{"docs":{},"验":{"docs":{},"及":{"docs":{},"结":{"docs":{},"果":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}},"总":{"docs":{},"结":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}},"我":{"docs":{},"们":{"docs":{},"展":{"docs":{},"示":{"docs":{},"了":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"块":{"docs":{},"可":{"docs":{},"以":{"docs":{},"消":{"docs":{},"除":{"docs":{},"在":{"docs":{},"很":{"docs":{},"多":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"需":{"docs":{},"要":{"docs":{},"依":{"docs":{},"赖":{"docs":{},"严":{"docs":{},"重":{"docs":{},"特":{"docs":{},"征":{"docs":{},"工":{"docs":{},"程":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"。":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"摘":{"docs":{},"要":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}},"无":{"docs":{},"监":{"docs":{},"督":{"docs":{},"学":{"docs":{},"习":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"方":{"docs":{},"法":{"docs":{},"的":{"docs":{},"优":{"docs":{},"势":{"docs":{},"是":{"docs":{},"有":{"docs":{},"大":{"docs":{},"量":{"docs":{},"的":{"docs":{},"无":{"docs":{},"标":{"docs":{},"签":{"docs":{},"的":{"docs":{},"数":{"docs":{},"据":{"docs":{},"可":{"docs":{},"以":{"docs":{},"大":{"docs":{},"量":{"docs":{},"的":{"docs":{},"获":{"docs":{},"取":{"docs":{},"，":{"docs":{},"基":{"docs":{},"于":{"docs":{},"有":{"docs":{},"标":{"docs":{},"签":{"docs":{},"数":{"docs":{},"据":{"docs":{},"的":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{},"目":{"docs":{},"前":{"docs":{},"也":{"docs":{},"被":{"docs":{},"证":{"docs":{},"明":{"docs":{},"在":{"docs":{},"许":{"docs":{},"多":{"docs":{},"文":{"docs":{},"本":{"docs":{},"处":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"有":{"docs":{},"较":{"docs":{},"好":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"例":{"docs":{},"如":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"推":{"docs":{},"理":{"docs":{},"、":{"docs":{},"机":{"docs":{},"器":{"docs":{},"翻":{"docs":{},"译":{"docs":{},"等":{"docs":{},"任":{"docs":{},"务":{"docs":{},"。":{"docs":{},"除":{"docs":{},"了":{"docs":{},"在":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},"处":{"docs":{},"理":{"docs":{},"领":{"docs":{},"域":{"docs":{},"外":{"docs":{},"，":{"docs":{},"很":{"docs":{},"多":{"docs":{},"机":{"docs":{},"器":{"docs":{},"视":{"docs":{},"觉":{"docs":{},"领":{"docs":{},"域":{"docs":{},"的":{"docs":{},"研":{"docs":{},"究":{"docs":{},"也":{"docs":{},"说":{"docs":{},"明":{"docs":{},"了":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"并":{"docs":{},"集":{"docs":{},"合":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"重":{"docs":{},"要":{"docs":{},"方":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"方":{"docs":{},"法":{"docs":{},"通":{"docs":{},"过":{"docs":{},"在":{"docs":{},"大":{"docs":{},"量":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"数":{"docs":{},"据":{"docs":{},"上":{"docs":{},"进":{"docs":{},"行":{"docs":{},"训":{"docs":{},"练":{"docs":{},"后":{"docs":{},"再":{"docs":{},"通":{"docs":{},"过":{"docs":{},"微":{"docs":{},"调":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"可":{"docs":{},"以":{"docs":{},"取":{"docs":{},"得":{"docs":{},"较":{"docs":{},"好":{"docs":{},"的":{"docs":{},"实":{"docs":{},"验":{"docs":{},"结":{"docs":{},"果":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"是":{"docs":{},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"基":{"docs":{},"于":{"docs":{},"微":{"docs":{},"调":{"docs":{},"方":{"docs":{},"法":{"docs":{},"的":{"docs":{},"语":{"docs":{},"言":{"docs":{},"表":{"docs":{},"示":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"并":{"docs":{},"且":{"docs":{},"在":{"docs":{},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"最":{"docs":{},"后":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"状":{"docs":{},"态":{"docs":{},"被":{"docs":{},"用":{"docs":{},"在":{"docs":{},"分":{"docs":{},"类":{"docs":{},"任":{"docs":{},"务":{"docs":{},"的":{"docs":{},"聚":{"docs":{},"合":{"docs":{},"表":{"docs":{},"示":{"docs":{},"中":{"docs":{},"。":{"docs":{},"而":{"docs":{},"如":{"docs":{},"果":{"docs":{},"针":{"docs":{},"对":{"docs":{},"的":{"docs":{},"是":{"docs":{},"非":{"docs":{},"分":{"docs":{},"类":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"则":{"docs":{},"该":{"docs":{},"词":{"docs":{},"对":{"docs":{},"应":{"docs":{},"的":{"docs":{},"向":{"docs":{},"量":{"docs":{},"被":{"docs":{},"删":{"docs":{},"除":{"docs":{},"掉":{"docs":{},"。":{"docs":{},"在":{"docs":{},"句":{"docs":{},"子":{"docs":{},"对":{"docs":{},"中":{"docs":{},"，":{"docs":{},"所":{"docs":{},"有":{"docs":{},"句":{"docs":{},"子":{"docs":{},"都":{"docs":{},"被":{"docs":{},"表":{"docs":{},"示":{"docs":{},"到":{"docs":{},"一":{"docs":{},"个":{"docs":{},"序":{"docs":{},"列":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"我":{"docs":{},"们":{"docs":{},"通":{"docs":{},"过":{"docs":{},"两":{"docs":{},"种":{"docs":{},"不":{"docs":{},"同":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"区":{"docs":{},"分":{"docs":{},"。":{"docs":{},"首":{"docs":{},"先":{"docs":{},"通":{"docs":{},"过":{"docs":{},"一":{"docs":{},"个":{"docs":{},"特":{"docs":{},"殊":{"docs":{},"的":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"[":{"docs":{},"e":{"docs":{},"s":{"docs":{},"p":{"docs":{},"]":{"docs":{},"来":{"docs":{},"对":{"docs":{},"两":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"进":{"docs":{},"行":{"docs":{},"区":{"docs":{},"分":{"docs":{},"，":{"docs":{},"在":{"docs":{},"第":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"中":{"docs":{},"，":{"docs":{},"每":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"会":{"docs":{},"加":{"docs":{},"上":{"docs":{},"句":{"docs":{},"子":{"docs":{},"a":{"docs":{},"的":{"docs":{},"e":{"docs":{},"m":{"docs":{},"b":{"docs":{},"e":{"docs":{},"d":{"docs":{},"d":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"表":{"docs":{},"示":{"docs":{},"，":{"docs":{},"而":{"docs":{},"在":{"docs":{},"句":{"docs":{},"子":{"docs":{},"b":{"docs":{},"中":{"docs":{},"，":{"docs":{},"每":{"docs":{},"个":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"都":{"docs":{},"会":{"docs":{},"加":{"docs":{},"上":{"docs":{},"句":{"docs":{},"子":{"docs":{},"b":{"docs":{},"的":{"docs":{},"e":{"docs":{},"m":{"docs":{},"b":{"docs":{},"e":{"docs":{},"d":{"docs":{},"d":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"表":{"docs":{},"示":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"模":{"docs":{},"块":{"docs":{},"的":{"docs":{},"结":{"docs":{},"构":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}},"源":{"docs":{},"码":{"docs":{},"链":{"docs":{},"接":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}},"由":{"docs":{},"于":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"会":{"docs":{},"通":{"docs":{},"过":{"docs":{},"再":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"方":{"docs":{},"式":{"docs":{},"来":{"docs":{},"微":{"docs":{},"调":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"，":{"docs":{},"而":{"docs":{},"在":{"docs":{},"再":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"，":{"docs":{},"由":{"docs":{},"于":{"docs":{},"不":{"docs":{},"在":{"docs":{},"存":{"docs":{},"在":{"docs":{},"[":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{},"]":{"docs":{},"信":{"docs":{},"息":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{},"在":{"docs":{},"训":{"docs":{},"练":{"docs":{},"中":{"docs":{},"并":{"docs":{},"不":{"docs":{},"会":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"所":{"docs":{},"有":{"docs":{},"的":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{},"，":{"docs":{},"而":{"docs":{},"是":{"docs":{},"通":{"docs":{},"过":{"docs":{},"以":{"1":{"5":{"docs":{},"%":{"docs":{},"概":{"docs":{},"率":{"docs":{},"来":{"docs":{},"选":{"docs":{},"择":{"docs":{},"那":{"docs":{},"些":{"docs":{},"单":{"docs":{},"词":{"docs":{},"被":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"。":{"docs":{},"此":{"docs":{},"外":{"docs":{},"在":{"docs":{},"选":{"docs":{},"择":{"docs":{},"的":{"docs":{},"被":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"的":{"docs":{},"单":{"docs":{},"子":{"docs":{},"中":{"docs":{},"，":{"docs":{},"采":{"docs":{},"用":{"docs":{},"如":{"docs":{},"下":{"docs":{},"措":{"docs":{},"施":{"docs":{},"来":{"docs":{},"进":{"docs":{},"行":{"docs":{},"选":{"docs":{},"择":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"在":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"中":{"docs":{},"，":{"docs":{},"有":{"1":{"5":{"docs":{},"%":{"docs":{},"的":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}},"docs":{}},"docs":{}}}}}}}}}}},"的":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"版":{"docs":{},"本":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"为":{"3":{"4":{"0":{"docs":{},"m":{"docs":{},"个":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}},"词":{"docs":{},"嵌":{"docs":{},"表":{"docs":{},"示":{"docs":{},"。":{"docs":{},"图":{"2":{"docs":{},"中":{"docs":{},"表":{"docs":{},"示":{"docs":{},"的":{"docs":{},"是":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"的":{"docs":{},"输":{"docs":{},"入":{"docs":{},"表":{"docs":{},"示":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}},"训":{"docs":{},"练":{"docs":{},"任":{"docs":{},"务":{"docs":{},"只":{"docs":{},"是":{"docs":{},"预":{"docs":{},"测":{"docs":{},"被":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"掉":{"docs":{},"的":{"docs":{},"单":{"docs":{},"词":{"docs":{},"而":{"docs":{},"不":{"docs":{},"是":{"docs":{},"重":{"docs":{},"新":{"docs":{},"构":{"docs":{},"建":{"docs":{},"整":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"许":{"docs":{},"多":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"如":{"docs":{},"问":{"docs":{},"题":{"docs":{},"回":{"docs":{},"答":{"docs":{},"，":{"docs":{},"语":{"docs":{},"言":{"docs":{},"推":{"docs":{},"理":{"docs":{},"都":{"docs":{},"依":{"docs":{},"赖":{"docs":{},"于":{"docs":{},"推":{"docs":{},"断":{"docs":{},"两":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"关":{"docs":{},"系":{"docs":{},"，":{"docs":{},"而":{"docs":{},"这":{"docs":{},"种":{"docs":{},"关":{"docs":{},"系":{"docs":{},"在":{"docs":{},"部":{"docs":{},"分":{"docs":{},"的":{"docs":{},"文":{"docs":{},"本":{"docs":{},"预":{"docs":{},"处":{"docs":{},"理":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"都":{"docs":{},"没":{"docs":{},"有":{"docs":{},"捕":{"docs":{},"获":{"docs":{},"。":{"docs":{},"为":{"docs":{},"了":{"docs":{},"能":{"docs":{},"够":{"docs":{},"训":{"docs":{},"练":{"docs":{},"一":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"能":{"docs":{},"够":{"docs":{},"对":{"docs":{},"两":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"的":{"docs":{},"关":{"docs":{},"系":{"docs":{},"进":{"docs":{},"行":{"docs":{},"推":{"docs":{},"理":{"docs":{},"，":{"docs":{},"在":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"的":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"引":{"docs":{},"入":{"docs":{},"对":{"docs":{},"对":{"docs":{},"下":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"预":{"docs":{},"测":{"docs":{},"的":{"docs":{},"训":{"docs":{},"练":{"docs":{},"目":{"docs":{},"标":{"docs":{},"。":{"docs":{},"在":{"docs":{},"训":{"docs":{},"练":{"docs":{},"中":{"docs":{},"，":{"docs":{},"在":{"docs":{},"一":{"docs":{},"个":{"docs":{},"句":{"docs":{},"子":{"docs":{},"对":{"docs":{},"（":{"docs":{},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"语":{"docs":{},"言":{"docs":{},"表":{"docs":{},"示":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"新":{"docs":{},"趋":{"docs":{},"势":{"docs":{},"就":{"docs":{},"是":{"docs":{},"通":{"docs":{},"过":{"docs":{},"利":{"docs":{},"用":{"docs":{},"语":{"docs":{},"言":{"docs":{},"模":{"docs":{},"型":{"docs":{},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"一":{"docs":{},"个":{"docs":{},"模":{"docs":{},"型":{"docs":{},"，":{"docs":{},"让":{"docs":{},"后":{"docs":{},"基":{"docs":{},"于":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"方":{"docs":{},"法":{"docs":{},"将":{"docs":{},"其":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"到":{"docs":{},"其":{"docs":{},"他":{"docs":{},"的":{"docs":{},"下":{"docs":{},"游":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"，":{"docs":{},"在":{"docs":{},"下":{"docs":{},"游":{"docs":{},"任":{"docs":{},"务":{"docs":{},"中":{"docs":{},"在":{"docs":{},"对":{"docs":{},"这":{"docs":{},"些":{"docs":{},"模":{"docs":{},"型":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"进":{"docs":{},"行":{"docs":{},"微":{"docs":{},"调":{"docs":{},"。":{"docs":{},"这":{"docs":{},"些":{"docs":{},"方":{"docs":{},"法":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"优":{"docs":{},"势":{"docs":{},"就":{"docs":{},"是":{"docs":{},"只":{"docs":{},"有":{"docs":{},"较":{"docs":{},"少":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"需":{"docs":{},"要":{"docs":{},"进":{"docs":{},"行":{"docs":{},"重":{"docs":{},"新":{"docs":{},"学":{"docs":{},"习":{"docs":{},"。":{"docs":{},"在":{"docs":{},"这":{"docs":{},"方":{"docs":{},"面":{"docs":{},"的":{"docs":{},"工":{"docs":{},"作":{"docs":{},"中":{"docs":{},"，":{"docs":{},"最":{"docs":{},"新":{"docs":{},"的":{"docs":{},"一":{"docs":{},"个":{"docs":{},"工":{"docs":{},"作":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"预":{"docs":{},"训":{"docs":{},"练":{"docs":{},"模":{"docs":{},"型":{"docs":{},"已":{"docs":{},"经":{"docs":{},"被":{"docs":{},"证":{"docs":{},"明":{"docs":{},"能":{"docs":{},"有":{"docs":{},"效":{"docs":{},"提":{"docs":{},"升":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"处":{"docs":{},"理":{"docs":{},"任":{"docs":{},"务":{"docs":{},"的":{"docs":{},"性":{"docs":{},"能":{"docs":{},"。":{"docs":{},"这":{"docs":{},"些":{"docs":{},"任":{"docs":{},"务":{"docs":{},"包":{"docs":{},"括":{"docs":{},"句":{"docs":{},"子":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"如":{"docs":{},"语":{"docs":{},"言":{"docs":{},"推":{"docs":{},"理":{"docs":{},"及":{"docs":{},"解":{"docs":{},"析":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"任":{"docs":{},"务":{"docs":{},"其":{"docs":{},"目":{"docs":{},"的":{"docs":{},"是":{"docs":{},"从":{"docs":{},"句":{"docs":{},"子":{"docs":{},"级":{"docs":{},"别":{"docs":{},"推":{"docs":{},"断":{"docs":{},"句":{"docs":{},"子":{"docs":{},"间":{"docs":{},"相":{"docs":{},"互":{"docs":{},"关":{"docs":{},"系":{"docs":{},"。":{"docs":{},"此":{"docs":{},"外":{"docs":{},"还":{"docs":{},"包":{"docs":{},"括":{"docs":{},"一":{"docs":{},"些":{"docs":{},"序":{"docs":{},"列":{"docs":{},"级":{"docs":{},"别":{"docs":{},"的":{"docs":{},"任":{"docs":{},"务":{"docs":{},"，":{"docs":{},"如":{"docs":{},"命":{"docs":{},"名":{"docs":{},"实":{"docs":{},"体":{"docs":{},"识":{"docs":{},"别":{"docs":{},"，":{"docs":{},"文":{"docs":{},"本":{"docs":{},"理":{"docs":{},"解":{"docs":{},"挑":{"docs":{},"战":{"docs":{},"任":{"docs":{},"务":{"docs":{},"（":{"docs":{},"s":{"docs":{},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"d":{"docs":{},"）":{"docs":{},"，":{"docs":{},"这":{"docs":{},"些":{"docs":{},"任":{"docs":{},"务":{"docs":{},"模":{"docs":{},"型":{"docs":{},"中":{"docs":{},"需":{"docs":{},"要":{"docs":{},"在":{"docs":{},"序":{"docs":{},"列":{"docs":{},"级":{"docs":{},"别":{"docs":{},"产":{"docs":{},"生":{"docs":{},"经":{"docs":{},"过":{"docs":{},"微":{"docs":{},"调":{"docs":{},"后":{"docs":{},"的":{"docs":{},"更":{"docs":{},"好":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"。":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"这":{"docs":{},"篇":{"docs":{},"文":{"docs":{},"章":{"docs":{},"的":{"docs":{},"贡":{"docs":{},"献":{"docs":{},"如":{"docs":{},"下":{"docs":{},"：":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}}},"项":{"docs":{},"目":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}},"：":{"docs":{},"表":{"docs":{},"示":{"docs":{},"隐":{"docs":{},"藏":{"docs":{},"层":{"docs":{},"数":{"docs":{},"量":{"docs":{"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"ref":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","tf":0.0058823529411764705}}}}}}}}}}},"length":350},"corpusTokens":["##","*a*:表示注意力header的数量。",".","1.简介","11项自然语言处理任务中都取得了目前最好的性能，具体包括将glue的基准值提升到了80.4%(7.6%的绝对提升)，在multnli中准确率有了86.7的提升（5.6%的绝对提升），在squad","11项自然语言处理任务中都取得了目前最好的性能，具体包括将glue的基准值提升到了80.4%(7.6%的绝对提升)，在multnli中准确率达到86.7的（5.6%的绝对提升），在squad","2.1","2.2","2.3","2.相关工作","2018年","2019","2019年","3","3.1","3.2","3.3","3.3.1","3.3.2","3.4","a,sent","aaai,全称是associ","abstract：","abstract：本文作者推出了一套新的语言表达模型bert，全称为bidirect","ac(lth","acl","acl的全称是th","acl（tacl），此外还有一些期刊与自然语言处理相关。如tslp(（acm","acm","acm）主办，包括如下几个会议。","advanc","ai","ai,artifici","aistat","al.,2017)从另外一个重要角度来对词嵌进行处理。在该方法中，作者提出了从语言模型中提取出与上下文敏感特征的方法。通过将基于上下文敏感的词嵌和特定的任务结构相结合后，elmo在很多自然语言处理任务中都取得了soat(st","american","approach","ariv","art)的实验效果，包括基于squad的问题，语义分析以及命名识别识别等众多的任务。","artifici","asia","asian","associ","association)组织","a类","base","base版本的参数有110m个，而bert","bert","bert:","bert是一个多层的基于双向转移编码器的模型，其根据tensor2tensor的方式来实践。bert使用的“transform","bert有两个版本：bertbase和bertlarge,其中bertbase的参数如下：l=12,h=768,a=12,而bertlarge的数据l=24,h=1024,a=16.因此bert","bert模型介绍","bert的输入表示","bidirect","blocks）","b类","b）中，一个句子b有50%是句子a的下一句，而50%的可能是随机另外一个句子。","ccf","chang,","chapter","cikm,该会议名称缩写和上一个相同，但是其全称为intern","cikmm,其全称是confer","classif","cole","coling(intern","committe","comput","confer","conference，也是有关信息检索及数据挖掘。","conll","conll(confer","c类","data","data&corpu","databas","decis","deep","embedding:","embedding:使用了支持长度为512个token的词嵌表示方法。","embedding方法来实现词嵌表示。","emnlp","emnlp(confer","emnlp是有acl下面的比较著名的兴趣小组（speci","empir","encod","encoder”编码器近来得到广泛的应用，因此本文中没有进行详细的介绍。","encoder模型，bert的","et","evaluation,其由欧洲语言资源组织进行elra(european","evaluation,该会议由acl的特殊兴趣小组siglex进行组织，每年都会举办，国内也有很多研究机构及公司参与，如哈工大科大讯飞等,其最新的链接网址如下：http://alt.qcri.org/semeval2019/index.php?id=task","googl","gpt(gener","gpt2018基于迁移学习的方法在许多句子级别的任务中取得了sota的实验结果。","gpt中则是使用的一种从左到右的编码模型，引起bert可以被看做迁移学习编码器，而openai","gpt则是一种迁移学习的解码器。bert和openai","gpt的区别可以如图所示","gpt等结构的差别。","group","groups,sigs）之一的sigdata(speci","h","hearst正式宣布成立国际计算语言学学会亚太地区分会（aacl，th","http://coling2018.org","http://emnlp2018.org","http://naacl.org","http://www.conll.org/","https://arxiv.org/abs/1810.04805","https://blog.csdn.net/jilrvrtrc/article/details/83829470","https://blog.csdn.net/lyb3b3b/article/details/83548964","https://v.youku.com/v_show/id_xnda4mdyznzgwoa==.html?spm=a2h3j.8428770.3416059.1","https://www.aclweb.org/port","https://www.cnblogs.com/niuxichuan/p/7602012.html","icml,全称是","ijcai,全称是intern","inform","intellig","intelligence，主要关注人工智能领域的最新研究进展，其中也有大量的人工智能相关的知识。","intelligence，也是关注人工智能领域的另外一个重要学术会议。","interest","intern","introduct","jacobdevlin,m","jair,journ","jmlr,","joint","journal","jumper:learn","kenton","keywords:","knowledg","knowledgebas","kristina","l:表示层级的数量（transform","languag","learn","learnin)","learning)举办的。其也是每年举办一次，由于naacl是acl在北美的分会，因此当acl在北美举办的时候，naacl就会停办一年。","lee,","level级别的任务中都取得了较好的效果。在很多有特定需要的任务中也取得了较好的效果。","levle和token","linguist","linguistics)","linguistics)组织的。该会议每两年举办一次。","linguistics,其由1965年创办，是由老牌的nlp学术会议组织iccl(th","linguistics,翻译过来是计算机语言协会，自然语言处理与计算语言学领域（以下简称nlp/cl）最权威的国际专业学会，acl成立于1962年，是自然语言处理(nlp)领域影响力最大、最具活力的顶级国际学术组织，每年举办一次。这个学会主办了","linguistics,该会议是acl在北美的分会，也是有acl主办。其是有acl下面的兴趣小组signal(speci","linguistics和transact","linguistics）。此次成立acl亚太分会，将进一步促进亚太地区nlp相关技术和研究的发展。据悉，首届aacl会议预计在2020年举行，此后将每两年举行一次。","lm","lm\"(mlm)模型。在该模型中，被隐藏掉的token会被输入到softmax模型中。在所有的预训练实验中，bert采用15%的概率随机隐藏掉输入句子中的token.","lm和next","lrec","lrec全称是intern","machin","machinery,","make","management，其关注信息管理","mask","method","mining,由名称可知，该会议关注搜索和数据挖掘。","naacl","naacl(th","naacl的全称是th","natur","nc,neurocompt","neural","next","nip","nlp","nlp/cl","nlp相关的其他国际会议","north","optimal)，而在针对一些token级别的任务中，如squad的问答问题中，则这种方法有可能是毁灭性的，因此在这些任务中，纳入两个方面的信息是至关重要的。","pacif","paper","posit","pre","predictioni","prediction模型。","process","processing)","processing)),其他相关期刊及投稿链接如下：","processing)举办的。","processing,nlp）领域的一些著名会议及期刊，将按照ccf对会议的分级标准对相关会议进行整理，介绍每个会议的关注主题，召开周期、会议网站及相关的信息，方便后面查找最新的相关论文，从而对该领域进行比较深入的研究。每个会议及期刊都有相关的入口链接，方便进行查看。","processing.","processing）)以及talip((acm","read","represent","research","resourc","retrieval,其主要关注信息检索。","search","semant","semev","semeval,其全称是intern","sentenc","sigir,其全称是speci","speech","statist","system","token","tool","toutanova","train","transact","transform","transform)引入了最小任务的参数，在下游任务中，其通过简单的微调的方式来调整模型的参数。在以前的工作中，所有的预训练方法都是通过使用同样的目标函数并通过双向语言模型来学习更好的通用语言表示。","transformers(基于双向翻译编码的表示模型)。bert通过提出了两个新的预训练目标来解决以前的提到的单向限制问题：基于“masked”的语言模型（mlm），这种方法在1953年就曾经被提出。这种语言模型通过随机将输入句子中的token进行隐藏，然后预训练的目标就是基于上下文来预测出被覆盖掉的单词。不同于传统的从左到右的预训练语言模型，mlm训练目标使得我们能够充分利用左边和右边信息来训练一个更深的双向翻译模型。除了利用masked语言模型外，我们还在模型中引入了预测下一个句子的训练目标来同时训练句子对级别的表示模型。","transformers。与近年来提出的语言模型不一样的地方在于，bert不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的bert模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改。bert是一种概念上简单，但在实际中却有强大的性能的模型。bert在","transformers。与近年来提出的语言模型不一样的地方在于，bert不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的bert模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改。bert是一种概念上简单，但根据经验推断其具有强大的性能。bert在","tuning）的方法。在基于特征的方法中，elmo(2018)是一种基于特征任务的模型，其包含一个预训练模型以及一些其他的特征。在微调模型中，openai","uai","uncertainti","understand","v1.1的问答任务中其f1值达到了93.2（1.5%）的绝对提升，比人类的表现都搞了2.0的提升。","web","wei","wide","workshop","world","wsdm,该会议全称为web","​","下载地址：链接：https://pan.baidu.com/s/1nmh33yk80sznki8vhmeeea","不像传统的从左到右或者从右到左的预训练语言模型任务，在bert中使用了两种原创的无监督学习的预训练任务，分别是mask","不是所有被选择的单词都会被隐藏掉，而是其中80%的可能会隐藏掉该单词，10%的时间将该单词替换成一个随机的单词。而另外还是有10的时间是该单词保持不变。","为了能够训练一个具有双向深层的表示模型，在bert中采用了一种较为直观的模型，通过随机隐藏掉句子中的token，并让模型能对该句子进行正确的预测。这种模型在bert中被称为\"mask","人工智能领域与自然语言处理相关的会议","从初始概念上来将，一个更深层次的双向模型比传统的从左到有或者从右到左基于简单讲从左到右或者从右到左模型进行组合的方法有更好的效果。然后，目前传统的语言模型则往往只能通过从左到右或者从右到左的模型进行训练。在双向的环境中，句子中任意一个token都有可能被预测到在一个多层的网络环境中。","以下是根据ccf2015年的对自然语言处理领域的会议的分级标准来进行罗列和整理","会议名称","会议网站","作者信息","作者单位","使用了包含30,000token的词库的wordpiec","全称","全称是confer","全称是intern","关键词","其他解读","内容","再训练流程","出版社","分享者：刘露平","分析解读最新的自然语言处理方面的论文，供大家一起进行交流和学习。","分类等级","区别于传统的auto","单词可能会被隐藏掉，因此在训练过程中其会花费较多的时间。","原文链接","参考文献","召开周期","国际会议","在","在11项自然语言处理任务中都取得了sota的结果。表明双向语言模型在文本处理中的重要性。","在bert中输入被表示为两种形式：","在bert中，bert使用的是一个双向的自注意力机制的编码模型，而openai","在bert的输入中，对于一个输入token序列，该输入句子的序列表示可以由几部分构成：token序列表示，segment和posit","在transformer模型中，由于encoder并不知道哪些单词可能会被替换成随机的单词，因此要求编码器能够学习到一种对所有单词都有表现的能力，此外随机替换只发在在1.5%的概率中，因此不会影响模型的整体理解能力。","在国际上acl，coling，enmlp和naacl是默认的四大自然语言处理顶级学术会议，其中acl，emnlp和naacl都是由acl及相应的子组织举办的。","在构建训练集时候，在选择句子a的下一个句子时，通过随机选择下一个句子的方式来实现构建训练集。","在第三节中，详细介绍bert模型机器实现细节。该节首选叙述bert模块的整体框架及bert的输入表示。接着在3.3节中介绍预训练的任务以及核心的创新。3.4节中介绍预训练的流程，而在3.5节中介绍微调的方法。最后在3.6节中讨论bert和openai","在自然语言处理中，一般来将，大家更关注学术会议，其主要原因是发表周期短，通过会议也可以进行深入的交流。但自然语言处理领域也有自己的学术期刊，其旗舰期刊有两个，分别是comput","在自然语言处理中，机器学习也是其主流的方法，在机器学习领域相关的学术会议包括icml,nips,uai及aistats,下面将简单介绍。","在该项工作中，作者定义了如下变量：","在输入词序列中，序列的第一个词都是以[cls]的方式开头，针对该token的","在这篇文章中我们证明了双向预训练模型的重要性。不同于传统的双向预测模型，bert使用了一种基于masked的语言模型来训练一种更深的语言模型。此外区别于传统的浅层双向语言模型，bert是一种使用深层次的双向语言模型。","在这篇文章中，我们任务目前的技术严重限制了预训练在语言表达方面的能力，特别是针对微调的方法。最大的缺陷是目前的预训练模型都是双向的，其严格限制了模型在进行预训练时的选择能力。例如在openaigp中，作者使用了从左到右的模型，使得句子中每个token只能被以前的词所关注。这种方式对句子级别的任务是次优的(sub","在这篇论文中，我们提出了一种基于微调的方法bert:bidirect","在这节中我们介绍基于预训练的方式来生成语言表示模型的相关方方法，同时简要介绍一下在这个领域目前最流行的方法。","基于学习的词表示方法在多个领域都有广泛的应用。其中包括非神经元的方法和基于神经网络的方法。基于词嵌表示的预训练方法目前是现代语言处理系统中的一个重要集成部分，其对系统的后续处理提供了有强大的提升。除了词级别的词嵌外，这些方法也被推广到粗粒度的句子级别词嵌以及锻炼词嵌中。在传统的方法中，这些学习到的表示方法往往作为下游任务的特征输入到下游模型中。elmo(pet","基于微调的方法","基于特征的方法","基于监督数据的迁移学习方法","处理和人工智能是密切相关的，其是人工智能研究的重要内容，人工智能研究的两大国际顶会是aaai和ijcai","如果输入是单个句子，则直接使用句子a的词嵌表示。","实验及结果","密码:trsadmin","将相关的知识进行整理后分享，方便后面的人学习","将自然语言处理方面的工具进行整理汇总","工具整理","总结","我们展示了预训练模块可以消除在很多任务中需要依赖严重特征工程的任务。bert","摘要","收集整理相关成员上传的资料及相应的数据集，方便有需要的同学进行下载。","收集整理自然语言处理领域的会议及期刊，方便大家进行相关的查阅。","数据集","无监督学习预训练方法的优势是有大量的无标签的数据可以大量的获取，基于有标签数据的迁移学习目前也被证明在许多文本处理任务中有较好的结果，例如自然语言推理、机器翻译等任务。除了在nlp处理领域外，很多机器视觉领域的研究也说明了预训练并集合迁移学习的重要方，这些方法通过在大量的预训练数据上进行训练后再通过微调的方式可以取得较好的实验结果。","时间","时间：2019年3月27日","时间：2019年3月28日","是第一个基于微调方法的语言表示模型，并且在sentenc","最后隐藏状态被用在分类任务的聚合表示中。而如果针对的是非分类任务，则该词对应的向量被删除掉。在句子对中，所有句子都被表示到一个序列任务中，我们通过两种不同的方式来区分。首先通过一个特殊的token[esp]来对两个句子进行区分，在第一个句子中，每个句子的token会加上句子a的embedding表示，而在句子b中，每个token都会加上句子b的embedding表示。","期刊","本文介绍自然语言处理（natur","本文作者推出了一套新的语言表达模型bert，全称为bidirect","机器学习领域中与自然语言处理相关的顶级会议","概述：该ppt是做汇报时候的ppt，讲了知识图谱的基本知识及实体和关系抽取相关论文，有需要可以下载。","模块的结构","每两年一次","每年一次","源码链接","由于bert会通过再训练的方式来微调模型的参数，而在再训练模型中，由于不在存在[mask]信息，因此在训练中并不会隐藏掉所有的mask，而是通过以15%概率来选择那些单词被隐藏掉。此外在选择的被隐藏的单子中，采用如下措施来进行选择。","由于在bert中，有15%的","的large版本的参数为340m个。","的词嵌表示。图2中表示的是bert的输入表示。","目前有两种预训练的方法被用于下游处理任务中：基于特征的和基于微调（fine","目录","知识分享","知识图谱概述ppt","研究性工作","自然语言","自然语言处理会议及期刊","训练任务只是预测被隐藏掉的单词而不是重新构建整个句子。","许多自然语言处理任务中，如问题回答，语言推理都依赖于推断两个句子的关系，而这种关系在部分的文本预处理模型中都没有捕获。为了能够训练一个模型能够对两个句子的关系进行推理，在bert的预训练任务中，引入对对下一个句子预测的训练目标。在训练中，在一个句子对（sent","论文解读","该博客主要收集整理与自然语言处理相关工具，论文以及代码等资源信息，对目前自然语言处理最前沿的技术、论文及相关相关代码进行整理后进行分享，主要关注点包括自然语言处理基础方法，知识图谱构建，智能问答等几个方面的知识。通过不断地积累让大家在这条路上越走越远。","该材料是一个视频，讲述的是基于强化学习的文本分类模型，其用于阅读理解中，感谢图像所王大东同学提供的资料。有兴趣的可以观看。","该部分主要是分享目前自然语言及知识图谱处理方法的基础知识，目前包含如下三个方面：自然语言处理基础技术，知识图谱构建，智能问答,论文整理将按照年份和板块来进行整理。","该页面收集整理日常工作中相关的数据集和资料，大家可以通过将相关资料放到网盘或者其他github仓库中，然后这里放上相应的链接。","语言表示模型的一个新趋势就是通过利用语言模型预训练一个模型，让后基于迁移学习的方法将其迁移到其他的下游任务中，在下游任务中在对这些模型的参数进行微调。这些方法的一个优势就是只有较少的参数需要进行重新学习。在这方面的工作中，最新的一个工作openai","语言预训练模型已经被证明能有效提升自然语言处理任务的性能。这些任务包括句子级别的任务如语言推理及解析，这些任务其目的是从句子级别推断句子间相互关系。此外还包括一些序列级别的任务，如命名实体识别，文本理解挑战任务（squad），这些任务模型中需要在序列级别产生经过微调后的更好的结果。","这篇文章的贡献如下：","除了上述被ccf收录的会议外，在自然语言处理领域还有其他许多重要会议，分别是semeval,lrec等。","除了直接与自然语言处理相关外，还有其他许多与自然语言处理相关的学术会议，包括信息检索，数据挖掘及人工智能领域，这些都是属于自然语言处理的应用领域。其中信息检索和数据挖掘与自然语言处理是密切相关的，主要由美国计算机学会（associ","项目","预训练任务","预训练，迁移学习","领域最权威的国际会议，即acl年会。1982年和1999年，acl分别成立了欧洲分会（eacl）和北美分会（naacl）两个区域性分会。近年来，亚太地区在自然语言处理方面的研究进步显著，2018年7月15日，第56届acl年会在澳大利亚墨尔本举行。开幕仪式上，acl主席marti","（永久有效）提取码：mdan","：表示隐藏层数量"],"pipeline":["stopWordFilter","stemmer"]},"store":{"./":{"url":"./","title":"Introduction","keywords":"","body":"该博客主要收集整理与自然语言处理相关工具，论文以及代码等资源信息，对目前自然语言处理最前沿的技术、论文及相关相关代码进行整理后进行分享，主要关注点包括自然语言处理基础方法，知识图谱构建，智能问答等几个方面的知识。通过不断地积累让大家在这条路上越走越远。\n自然语言处理会议及期刊\n收集整理自然语言处理领域的会议及期刊，方便大家进行相关的查阅。\n工具整理\n将自然语言处理方面的工具进行整理汇总\n数据集\n收集整理相关成员上传的资料及相应的数据集，方便有需要的同学进行下载。\n论文解读\n分析解读最新的自然语言处理方面的论文，供大家一起进行交流和学习。\n知识分享\n将相关的知识进行整理后分享，方便后面的人学习\n"},"metting/summit metting.html":{"url":"metting/summit metting.html","title":"Conferences and journals","keywords":"","body":"本文介绍自然语言处理（Natural Language Processing,NLP）领域的一些著名会议及期刊，将按照CCF对会议的分级标准对相关会议进行整理，介绍每个会议的关注主题，召开周期、会议网站及相关的信息，方便后面查找最新的相关论文，从而对该领域进行比较深入的研究。每个会议及期刊都有相关的入口链接，方便进行查看。\n国际会议\n以下是根据CCF2015年的对自然语言处理领域的会议的分级标准来进行罗列和整理\n\n   \n      分类等级\n      会议名称\n      召开周期\n      出版社\n      会议网站\n   \n   \n      CCF A类\n      AC(LThe Association for Computational Linguistics)\n      每年一次\n      ACL\n      https://www.aclweb.org/portal\n   \n   \n      \n   \n   \n      CCF B类\n      COLING(International Conference on Computational Linguistics)\n      每年一次\n      ACL\n      http://coling2018.org\n   \n   \n      \n   \n   \n      \n      EMNLP(Conference on Empirical Methods in Natural Language Processing)\n      每年一次\n      ACM\n      http://emnlp2018.org\n   \n   \n      \n   \n   \n      CCF C类\n      NAACL(The North American Chapter of the Association for Computational Linguistics)\n      每年一次\n      NAACL\n      http://naacl.org\n   \n   \n      \n   \n   \n      \n      CoNLL(Conference on Computational Natural Language Learnin)\n      每两年一次\n      CoNLL\n      http://www.conll.org/\n   \n   \n      \n   \n\n\n\n在国际上ACL，COLING，ENMLP和NAACL是默认的四大自然语言处理顶级学术会议，其中ACL，EMNLP和NAACL都是由ACL及相应的子组织举办的。\nACL\nACL的全称是The Association for Computational Linguistics,翻译过来是计算机语言协会，自然语言处理与计算语言学领域（以下简称NLP/CL）最权威的国际专业学会，ACL成立于1962年，是自然语言处理(NLP)领域影响力最大、最具活力的顶级国际学术组织，每年举办一次。这个学会主办了 NLP/CL 领域最权威的国际会议，即ACL年会。1982年和1999年，ACL分别成立了欧洲分会（EACL）和北美分会（NAACL）两个区域性分会。近年来，亚太地区在自然语言处理方面的研究进步显著，2018年7月15日，第56届ACL年会在澳大利亚墨尔本举行。开幕仪式上，ACL主席Marti Hearst正式宣布成立国际计算语言学学会亚太地区分会（AACL，The Asia-Pacific Chapter of Association for Computational Linguistics）。此次成立ACL亚太分会，将进一步促进亚太地区NLP相关技术和研究的发展。据悉，首届AACL会议预计在2020年举行，此后将每两年举行一次。\nCOLING\nCOLING 全称是International Conference on Computational Linguistics,其由1965年创办，是由老牌的NLP学术会议组织ICCL(The International Committee on Computational Linguistics)组织的。该会议每两年举办一次。\nEMNLP\nEMNLP 全称是Conference on Empirical Methods in Natural Language Processing. EMNLP是有ACL下面的比较著名的兴趣小组（Special Interest Groups,SIGS）之一的SIGDATA(Special Interest Group on Linguistic Data&Corpus-based Approaches to Natural Language Processing)举办的。\nNAACL\nNAACL的全称是The North American Chapter of the Association for Computational Linguistics,该会议是ACL在北美的分会，也是有ACL主办。其是有ACL下面的兴趣小组SIGNAL(special Interest Group on Natural Language Learning)举办的。其也是每年举办一次，由于NAACL是ACL在北美的分会，因此当ACL在北美举办的时候，NAACL就会停办一年。\n除了上述被CCF收录的会议外，在自然语言处理领域还有其他许多重要会议，分别是SemEval,LREC等。\nSemEval\nSemEval,其全称是International Workshop on Semantic Evaluation,该会议由ACL的特殊兴趣小组SIGLEX进行组织，每年都会举办，国内也有很多研究机构及公司参与，如哈工大科大讯飞等,其最新的链接网址如下：http://alt.qcri.org/semeval2019/index.php?id=tasks\nLREC\nLREC全称是International Conference on Language Resource and Evaluation,其由欧洲语言资源组织进行ELRA(European Language Resources Association)组织\nNLP相关的其他国际会议\n除了直接与自然语言处理相关外，还有其他许多与自然语言处理相关的学术会议，包括信息检索，数据挖掘及人工智能领域，这些都是属于自然语言处理的应用领域。其中信息检索和数据挖掘与自然语言处理是密切相关的，主要由美国计算机学会（Association for Computing Machinery, ACM）主办，包括如下几个会议。\nSIGIR,其全称是Special Interest Group on Informational Retrieval,其主要关注信息检索。\nCIKMM,其全称是Conference on Information and Knowledge Management，其关注信息管理\nCIKM,该会议名称缩写和上一个相同，但是其全称为International World Wide Web Conference，也是有关信息检索及数据挖掘。\nWSDM,该会议全称为Web Search and Data Mining,由名称可知，该会议关注搜索和数据挖掘。\n人工智能领域与自然语言处理相关的会议\n自然语言 处理和人工智能是密切相关的，其是人工智能研究的重要内容，人工智能研究的两大国际顶会是AAAI和IJCAI\nAAAI,全称是Association for the Advancement of Artificial Intelligence，主要关注人工智能领域的最新研究进展，其中也有大量的人工智能相关的知识。\nIJCAI,全称是International Joint Conferences on Artificial Intelligence，也是关注人工智能领域的另外一个重要学术会议。\n机器学习领域中与自然语言处理相关的顶级会议\n在自然语言处理中，机器学习也是其主流的方法，在机器学习领域相关的学术会议包括ICML,NIPS,UAI及AISTATS,下面将简单介绍。\nICML,全称是 International Conference on Machine Learning\nNIPS 全称 Conference on Neural Information Processing Systems\nUAI 全称 Conference on Uncertainty in Artificial Intelligence\nAISTATS 全称 International Conference on Artificial Intelligence and Statistics\n期刊\n在自然语言处理中，一般来将，大家更关注学术会议，其主要原因是发表周期短，通过会议也可以进行深入的交流。但自然语言处理领域也有自己的学术期刊，其旗舰期刊有两个，分别是Computational Linguistics和Transactions of ACL（TACL），此外还有一些期刊与自然语言处理相关。如TSLP(（ACM Transactions on Speech and Language Processing）)以及TALIP((ACM Transactions on Asian Language Information Processing)),其他相关期刊及投稿链接如下：\nAI,Artificial Intelligence\nJAIR,Journal of AI Research\nJMLR, Journal of Machine Learning Research\nNC,Neurocompting\n参考文献\n\nhttps://blog.csdn.net/lyb3b3b/article/details/83548964\nhttps://www.cnblogs.com/niuxichuan/p/7602012.html\n\n"},"tools/readme.html":{"url":"tools/readme.html","title":"tools","keywords":"","body":""},"database/readme.html":{"url":"database/readme.html","title":"database","keywords":"","body":"该页面收集整理日常工作中相关的数据集和资料，大家可以通过将相关资料放到网盘或者其他github仓库中，然后这里放上相应的链接。\n\n知识图谱概述PPT\n\n时间：2019年3月27日\n分享者：刘露平\n下载地址：链接：https://pan.baidu.com/s/1nmH33YK80SZNKI8VHmEeEA （永久有效）提取码：mdan \n概述：该PPT是做汇报时候的PPT，讲了知识图谱的基本知识及实体和关系抽取相关论文，有需要可以下载。\n\n\nJumper:Learning When to Make Classification Decision in Reading\n\n时间：2019年3月28日\n分享者：刘露平\nhttps://v.youku.com/v_show/id_XNDA4MDYzNzgwOA==.html?spm=a2h3j.8428770.3416059.1  密码:trsadmin\n该材料是一个视频，讲述的是基于强化学习的文本分类模型，其用于阅读理解中，感谢图像所王大东同学提供的资料。有兴趣的可以观看。\n\n\n\n"},"paper/readme.html":{"url":"paper/readme.html","title":"paper","keywords":"","body":"该部分主要是分享目前自然语言及知识图谱处理方法的基础知识，目前包含如下三个方面：自然语言处理基础技术，知识图谱构建，智能问答,论文整理将按照年份和板块来进行整理。\n\n2019\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding--ariv\n\n"},"paper/nlp/readme.html":{"url":"paper/nlp/readme.html","title":"NLP","keywords":"","body":"目录\n\n2019年\n\n2018年\n研究性工作\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding--ariv\nAbstract： 本文作者推出了一套新的语言表达模型BERT，全称为Bidirectional Encoder Representations from Transformers。与近年来提出的语言模型不一样的地方在于，BERT不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的BERT模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改。Bert是一种概念上简单，但根据经验推断其具有强大的性能。BERT在 11项自然语言处理任务中都取得了目前最好的性能，具体包括将GLUE的基准值提升到了80.4%(7.6%的绝对提升)，在MultNLI中准确率有了86.7的提升（5.6%的绝对提升），在SQuAD v1.1的问答任务中其F1值达到了93.2（1.5%）的绝对提升，比人类的表现都搞了2.0的提升。       \nkeywords:  预训练，迁移学习\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html":{"url":"paper/nlp/papers/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html","title":"BERT Pre-training of Deep Bidirectional Transformers for Language Understanding","keywords":"","body":"\nBERT Pre-training of Deep Bidirectional Transformers for Language Understanding\n\n\n\n\n项目\n内容\n\n\n\n\n时间\n2018年\n\n\n作者单位\nGoogle\n\n\n作者信息\nJacobDevlin,Ming-Wei Chang, Kenton Lee, Kristina Toutanova\n\n\n源码链接\n\n\n\n原文链接\nhttps://arxiv.org/abs/1810.04805\n\n\n其他解读\nhttps://blog.csdn.net/jilrvrtrc/article/details/83829470\n\n\n关键词\n预训练，迁移学习\n\n\n\n摘要\nAbstract：本文作者推出了一套新的语言表达模型BERT，全称为Bidirectional Encoder Representations from Transformers。与近年来提出的语言模型不一样的地方在于，BERT不再仅仅是只关注一个词前文或后文的信息，而是整个模型的所有层都去关注其整个上下文的语境信息。实验结果证明，使用预训练过的BERT模型，仅仅在后面再包一层输出层，并对其进行微调训练，就可以将其应用到其他多种任务中，例如问答、语言推断等，并且在这些后端任务中并不需要根据特定的任务需要对模型结构进行修改。Bert是一种概念上简单，但在实际中却有强大的性能的模型。BERT在 11项自然语言处理任务中都取得了目前最好的性能，具体包括将GLUE的基准值提升到了80.4%(7.6%的绝对提升)，在MultNLI中准确率达到86.7的（5.6%的绝对提升），在SQuAD v1.1的问答任务中其F1值达到了93.2（1.5%）的绝对提升，比人类的表现都搞了2.0的提升。   \n1.简介\n​    语言预训练模型已经被证明能有效提升自然语言处理任务的性能。这些任务包括句子级别的任务如语言推理及解析，这些任务其目的是从句子级别推断句子间相互关系。此外还包括一些序列级别的任务，如命名实体识别，文本理解挑战任务（SQuAD），这些任务模型中需要在序列级别产生经过微调后的更好的结果。\n​    目前有两种预训练的方法被用于下游处理任务中：基于特征的和基于微调（fine-tuning）的方法。在基于特征的方法中，ELMO(2018)是一种基于特征任务的模型，其包含一个预训练模型以及一些其他的特征。在微调模型中，OpenAI GPT(Generative Pre-trained Transform)引入了最小任务的参数，在下游任务中，其通过简单的微调的方式来调整模型的参数。在以前的工作中，所有的预训练方法都是通过使用同样的目标函数并通过双向语言模型来学习更好的通用语言表示。\n​    在这篇文章中，我们任务目前的技术严重限制了预训练在语言表达方面的能力，特别是针对微调的方法。最大的缺陷是目前的预训练模型都是双向的，其严格限制了模型在进行预训练时的选择能力。例如在OpenAIGP中，作者使用了从左到右的模型，使得句子中每个token只能被以前的词所关注。这种方式对句子级别的任务是次优的(sub-optimal)，而在针对一些token级别的任务中，如SQuAD的问答问题中，则这种方法有可能是毁灭性的，因此在这些任务中，纳入两个方面的信息是至关重要的。\n​    在这篇论文中，我们提出了一种基于微调的方法BERT:Bidirectional Encoder Representations from Transformers(基于双向翻译编码的表示模型)。BERT通过提出了两个新的预训练目标来解决以前的提到的单向限制问题：基于“masked”的语言模型（MLM），这种方法在1953年就曾经被提出。这种语言模型通过随机将输入句子中的token进行隐藏，然后预训练的目标就是基于上下文来预测出被覆盖掉的单词。不同于传统的从左到右的预训练语言模型，MLM训练目标使得我们能够充分利用左边和右边信息来训练一个更深的双向翻译模型。除了利用masked语言模型外，我们还在模型中引入了预测下一个句子的训练目标来同时训练句子对级别的表示模型。\n​    这篇文章的贡献如下：\n\n在这篇文章中我们证明了双向预训练模型的重要性。不同于传统的双向预测模型，BERT使用了一种基于masked的语言模型来训练一种更深的语言模型。此外区别于传统的浅层双向语言模型，BERT是一种使用深层次的双向语言模型。\n我们展示了预训练模块可以消除在很多任务中需要依赖严重特征工程的任务。BERT 是第一个基于微调方法的语言表示模型，并且在sentence-levle和token-level级别的任务中都取得了较好的效果。在很多有特定需要的任务中也取得了较好的效果。\nBERT 在11项自然语言处理任务中都取得了SOTA的结果。表明双向语言模型在文本处理中的重要性。\n\n2.相关工作\n​    在这节中我们介绍基于预训练的方式来生成语言表示模型的相关方方法，同时简要介绍一下在这个领域目前最流行的方法。\n2.1 基于特征的方法\n​    基于学习的词表示方法在多个领域都有广泛的应用。其中包括非神经元的方法和基于神经网络的方法。基于词嵌表示的预训练方法目前是现代语言处理系统中的一个重要集成部分，其对系统的后续处理提供了有强大的提升。除了词级别的词嵌外，这些方法也被推广到粗粒度的句子级别词嵌以及锻炼词嵌中。在传统的方法中，这些学习到的表示方法往往作为下游任务的特征输入到下游模型中。ELMo(Peters et al.,2017)从另外一个重要角度来对词嵌进行处理。在该方法中，作者提出了从语言模型中提取出与上下文敏感特征的方法。通过将基于上下文敏感的词嵌和特定的任务结构相结合后，ELMo在很多自然语言处理任务中都取得了SOAT(State of the ART)的实验效果，包括基于SQuAD的问题，语义分析以及命名识别识别等众多的任务。\n2.2 基于微调的方法\n​    语言表示模型的一个新趋势就是通过利用语言模型预训练一个模型，让后基于迁移学习的方法将其迁移到其他的下游任务中，在下游任务中在对这些模型的参数进行微调。这些方法的一个优势就是只有较少的参数需要进行重新学习。在这方面的工作中，最新的一个工作OpenAI GPT2018基于迁移学习的方法在许多句子级别的任务中取得了SOTA的实验结果。\n2.3 基于监督数据的迁移学习方法\n​    无监督学习预训练方法的优势是有大量的无标签的数据可以大量的获取，基于有标签数据的迁移学习目前也被证明在许多文本处理任务中有较好的结果，例如自然语言推理、机器翻译等任务。除了在NLP处理领域外，很多机器视觉领域的研究也说明了预训练并集合迁移学习的重要方，这些方法通过在大量的预训练数据上进行训练后再通过微调的方式可以取得较好的实验结果。\n3 . Bert模型介绍\n​    在第三节中，详细介绍BERT模型机器实现细节。该节首选叙述Bert模块的整体框架及Bert的输入表示。接着在3.3节中介绍预训练的任务以及核心的创新。3.4节中介绍预训练的流程，而在3.5节中介绍微调的方法。最后在3.6节中讨论Bert和OpenAI GPT等结构的差别。\n3.1 模块的结构\n​    BERT是一个多层的基于双向转移编码器的模型，其根据tensor2tensor的方式来实践。BERT使用的“Transformer Encoder”编码器近来得到广泛的应用，因此本文中没有进行详细的介绍。\n​    在该项工作中，作者定义了如下变量：\n​    L:表示层级的数量（Transformer blocks）\n​    H ：表示隐藏层数量\n *A*:表示注意力header的数量。\n​    bert有两个版本：BERTbase和BERTlarge,其中BERTbase的参数如下：L=12,H=768,A=12,而BERTlarge的数据L=24,H=1024,A=16.因此BERT base版本的参数有110M个，而BERT 的large版本的参数为340M个。\n​    在BERT中，BERT使用的是一个双向的自注意力机制的编码模型，而OpenAI GPT中则是使用的一种从左到右的编码模型，引起BERT可以被看做迁移学习编码器，而OpenAI GPT则是一种迁移学习的解码器。BERT和OpenAI GPT的区别可以如图所示\n## 3.2 BERT的输入表示\n​    在bert的输入中，对于一个输入token序列，该输入句子的序列表示可以由几部分构成：token序列表示，segment和position 的词嵌表示。图2中表示的是BERT的输入表示。\n在BERT中输入被表示为两种形式：\n\nToken Embedding: 使用了包含30,000token的词库的WordPiece embedding方法来实现词嵌表示。\nPositional Embedding:使用了支持长度为512个token的词嵌表示方法。\n\n​    在输入词序列中，序列的第一个词都是以[CLS]的方式开头，针对该token的 最后隐藏状态被用在分类任务的聚合表示中。而如果针对的是非分类任务，则该词对应的向量被删除掉。在句子对中，所有句子都被表示到一个序列任务中，我们通过两种不同的方式来区分。首先通过一个特殊的token[ESP]来对两个句子进行区分，在第一个句子中，每个句子的token会加上句子A的embedding表示，而在句子B中，每个token都会加上句子B的embedding表示。\n​    如果输入是单个句子，则直接使用句子A的词嵌表示。\n3.3 预训练任务\n​    不像传统的从左到右或者从右到左的预训练语言模型任务，在BERT中使用了两种原创的无监督学习的预训练任务，分别是Masked LM和Next Prediction模型。\n3.3.1  Masked LM\n​    从初始概念上来将，一个更深层次的双向模型比传统的从左到有或者从右到左基于简单讲从左到右或者从右到左模型进行组合的方法有更好的效果。然后，目前传统的语言模型则往往只能通过从左到右或者从右到左的模型进行训练。在双向的环境中，句子中任意一个token都有可能被预测到在一个多层的网络环境中。\n​    为了能够训练一个具有双向深层的表示模型，在bert中采用了一种较为直观的模型，通过随机隐藏掉句子中的token，并让模型能对该句子进行正确的预测。这种模型在BERT中被称为\"masked LM\"(MLM)模型。在该模型中，被隐藏掉的token会被输入到softmax模型中。在所有的预训练实验中，BERT采用15%的概率随机隐藏掉输入句子中的token. 区别于传统的auto-encoder模型，BERT的 训练任务只是预测被隐藏掉的单词而不是重新构建整个句子。\n​    由于BERT会通过再训练的方式来微调模型的参数，而在再训练模型中，由于不在存在[MASK]信息，因此在训练中并不会隐藏掉所有的mask，而是通过以15%概率来选择那些单词被隐藏掉。此外在选择的被隐藏的单子中，采用如下措施来进行选择。\n\n不是所有被选择的单词都会被隐藏掉，而是其中80%的可能会隐藏掉该单词，10%的时间将该单词替换成一个随机的单词。而另外还是有10的时间是该单词保持不变。\n\n  ​    在Transformer模型中，由于encoder并不知道哪些单词可能会被替换成随机的单词，因此要求编码器能够学习到一种对所有单词都有表现的能力，此外随机替换只发在在1.5%的概率中，因此不会影响模型的整体理解能力。\n  ​    由于在BERT中，有15%的 单词可能会被隐藏掉，因此在训练过程中其会花费较多的时间。\n3.3.2 Next Sentence Predictioni\n  ​    在 许多自然语言处理任务中，如问题回答，语言推理都依赖于推断两个句子的关系，而这种关系在部分的文本预处理模型中都没有捕获。为了能够训练一个模型能够对两个句子的关系进行推理，在BERT的预训练任务中，引入对对下一个句子预测的训练目标。在训练中，在一个句子对（sentence A,sentence B）中，一个句子B有50%是句子A的下一句，而50%的可能是随机另外一个句子。\n  \n  ​    在构建训练集时候，在选择句子A的下一个句子时，通过随机选择下一个句子的方式来实现构建训练集。\n3.4 再训练流程\n实验及结果\n总结\n​      \n"},"paper/KnowledgeBase/readme.html":{"url":"paper/KnowledgeBase/readme.html","title":"KnowledgeBase","keywords":"","body":""}}}